# SSTå†…éƒ¨ç‰¹å¾æå– - å¿«é€Ÿå¼€å§‹

## ğŸ“Œ æ ¸å¿ƒé—®é¢˜è§£ç­”

**Q: é€šè¿‡SSTçš„å¸¸è§„æ¨ç†ï¼ˆforwardï¼‰ï¼Œæˆ‘åªèƒ½å¾—åˆ°Tæ—¥å’ŒT+1æ—¥çš„é¢„æµ‹è¾“å‡ºï¼Œå¦‚ä½•è·å–å†…éƒ¨ç‰¹å¾ï¼ˆattention weightsã€encoder outputï¼‰ï¼Ÿ**

**A: éœ€è¦ä½¿ç”¨ä¸“é—¨çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œä¸æ˜¯å¸¸è§„çš„forwardæ¨ç†ï¼**

---

## ğŸ”‘ å…³é”®ç†è§£

### å¸¸è§„æ¨ç† âŒ
```python
predictions = model(boundary_conditions)
# åªè¿”å›é¢„æµ‹ï¼Œæ— æ³•è·å–ä¸­é—´ç‰¹å¾
```

### ç‰¹å¾æå–æ¨ç† âœ…
```python
predictions, features = model.forward_with_features(
    boundary_conditions,
    return_attention=True,
    return_encoder_output=True
)
# è¿”å›é¢„æµ‹ + æ‰€æœ‰ä¸­é—´ç‰¹å¾
```

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `FEATURE_EXTRACTION_GUIDE.md` | å®Œæ•´æŠ€æœ¯æ–‡æ¡£ï¼ˆç†è®º+å®ç°ï¼‰ |
| `../examples/extract_sst_internals_demo.py` | å¯è¿è¡Œçš„å®Œæ•´ç¤ºä¾‹ |
| `../models/spatial_feature_extractor.py` | æ ¸å¿ƒæ¨¡å‹å®ç° |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Step 1: ç†è§£ä¸¤ç§æ¨¡å¼

```python
from models.spatial_feature_extractor import SpatialFeatureExtractor

# åˆ›å»ºæ¨¡å‹
model = SpatialFeatureExtractor(
    num_boundary_sensors=23,
    num_target_sensors=1,
    d_model=128,
    nhead=8,
    num_layers=3,
    enable_feature_extraction=True  # â† å¯ç”¨ç‰¹å¾æå–
)

# åŠ è½½æƒé‡
model.load_state_dict(torch.load('sst_model.pth'))
model.eval()

# å‡†å¤‡è¾“å…¥
boundary_conditions = torch.randn(1, 23)  # [batch=1, num_sensors=23]
```

### Step 2: æå–å®Œæ•´ç‰¹å¾

```python
with torch.no_grad():
    predictions, features = model.forward_with_features(
        boundary_conditions,
        return_attention=True,
        return_encoder_output=True
    )

# æ£€æŸ¥è¾“å‡º
print(f"é¢„æµ‹: {predictions.shape}")  # [1, 1]
print(f"\nä¸­é—´ç‰¹å¾:")
for key, value in features.items():
    print(f"  {key}: {value.shape}")

# è¾“å‡ºï¼š
#   embeddings: torch.Size([1, 23, 128])
#   encoder_output: torch.Size([1, 23, 128])
#   attention_weights: torch.Size([1, 3, 8, 23, 23])
#   pooled_features: torch.Size([1, 128])
```

### Step 3: å¿«é€Ÿæå–å•ä¸€ç‰¹å¾

```python
# åªæå–encoder outputï¼ˆæ›´å¿«ï¼‰
encoder_output = model.get_encoder_output(boundary_conditions)
# shape: [1, 23, 128]

# åªæå–attention weights
attention_weights = model.get_attention_weights(boundary_conditions)
# shape: [1, 3, 8, 23, 23]
```

---

## ğŸ¯ åŒè¾“å‡ºSSTï¼ˆTæ—¥ + T+1æ—¥ï¼‰

å½“å‰çš„`SpatialFeatureExtractor`åªæœ‰å•è¾“å‡ºã€‚å¦‚éœ€åŒè¾“å‡ºï¼Œå‚è€ƒ `examples/extract_sst_internals_demo.py` ä¸­çš„ `DualOutputSST` å®ç°ï¼š

```python
class DualOutputSST(SpatialFeatureExtractor):
    def __init__(self, num_boundary_sensors, num_target_sensors, **kwargs):
        super().__init__(num_boundary_sensors, num_target_sensors, **kwargs)

        # åŒè¾“å‡ºå¤´
        self.output_projection_T = nn.Linear(self.d_model, num_target_sensors)
        self.output_projection_T1 = nn.Linear(self.d_model, num_target_sensors)

    def forward(self, boundary_conditions):
        # ... (çœç•¥embeddingå’Œtransformer)

        # åŒè¾“å‡º
        pred_T = self.output_projection_T(x_pooled)    # Tæ—¥é¢„æµ‹
        pred_T1 = self.output_projection_T1(x_pooled)  # T+1æ—¥é¢„æµ‹

        return pred_T, pred_T1
```

ä½¿ç”¨ï¼š
```python
# è®­ç»ƒ
pred_T, pred_T1 = model(boundary_conditions)
loss = criterion(pred_T, target_T) + criterion(pred_T1, target_T1)

# ç‰¹å¾æå–
(pred_T, pred_T1), features = model.forward_with_features(
    boundary_conditions,
    return_attention=True,
    return_encoder_output=True
)

# è®¡ç®—æ®‹å·®
residual_T = target_T - pred_T      # ç©ºé—´æ®‹å·®
residual_T1 = target_T1 - pred_T1   # æ—¶ç©ºæ®‹å·®
```

---

## ğŸ“Š æ„å»º60å¤©å†å²åºåˆ—

```python
# å­˜å‚¨60å¤©çš„ç‰¹å¾
all_features = {
    'attention_weights': [],
    'encoder_output': [],
    'residual_T': [],
    'residual_T1': []
}

# é€å¤©æå–
for day in range(60):
    bc = historical_data[day:day+1]

    (pred_T, pred_T1), features = model.forward_with_features(
        bc, return_attention=True, return_encoder_output=True
    )

    # è®¡ç®—æ®‹å·®
    residual_T = true_values_T[day:day+1] - pred_T
    residual_T1 = true_values_T1[day:day+1] - pred_T1

    # ä¿å­˜
    all_features['attention_weights'].append(features['attention_weights'])
    all_features['encoder_output'].append(features['encoder_output'])
    all_features['residual_T'].append(residual_T)
    all_features['residual_T1'].append(residual_T1)

# åˆå¹¶
attention_seq = torch.cat(all_features['attention_weights'], dim=0)
# shape: [60, num_layers, num_heads, num_sensors, num_sensors]

encoder_seq = torch.cat(all_features['encoder_output'], dim=0)
# shape: [60, num_sensors, d_model]

residual_T_seq = torch.cat(all_features['residual_T'], dim=0)
residual_T1_seq = torch.cat(all_features['residual_T1'], dim=0)
# shape: [60, 1]
```

---

## ğŸ§® é™ç»´å¹¶æ„å»ºLSTMè¾“å…¥

```python
# åˆ›å»ºé™ç»´å™¨ï¼ˆå‚è€ƒdemoä¸­çš„å®ç°ï¼‰
attention_extractor = SimpleAttentionExtractor(output_dim=10)
encoder_extractor = SimpleEncoderExtractor(d_model=128, output_dim=32)

# é€å¤©é™ç»´
lstm_input_list = []

for day in range(60):
    # æå–attentionç‰¹å¾ï¼ˆ10ç»´ï¼‰
    attn_feat = attention_extractor(
        all_features['attention_weights'][day],
        target_stock_idx=5
    )

    # æå–encoderç‰¹å¾ï¼ˆ32ç»´ï¼‰
    enc_feat = encoder_extractor(
        all_features['encoder_output'][day],
        target_stock_idx=5
    )

    # æ®‹å·®ç‰¹å¾ï¼ˆ2ç»´ï¼‰
    res_feat = torch.cat([
        all_features['residual_T'][day],
        all_features['residual_T1'][day]
    ], dim=-1)

    # åˆå¹¶ï¼ˆ10+32+2=44ç»´ï¼‰
    day_feat = torch.cat([attn_feat, enc_feat, res_feat], dim=-1)
    lstm_input_list.append(day_feat)

# æ„å»ºLSTMè¾“å…¥
lstm_input = torch.cat(lstm_input_list, dim=0)  # [60, 44]
lstm_input = lstm_input.unsqueeze(0)  # [1, 60, 44]

# ç°åœ¨å¯ä»¥è¾“å…¥LSTMäº†ï¼
lstm = nn.LSTM(input_size=44, hidden_size=64, num_layers=2)
output, (h_n, c_n) = lstm(lstm_input)
```

---

## ğŸ”¬ æŠ€æœ¯åŸç†ï¼ˆç®€è¦ï¼‰

### ä¸ºä»€ä¹ˆå¸¸è§„forwardæ— æ³•è¿”å›ä¸­é—´ç‰¹å¾ï¼Ÿ

PyTorchçš„`nn.TransformerEncoder`åªè¿”å›æœ€ç»ˆè¾“å‡ºï¼Œä¸è¿”å›attentionæƒé‡ï¼š

```python
# PyTorchæºç ï¼ˆç®€åŒ–ï¼‰
def forward(self, src):
    output = src
    for layer in self.layers:
        output = layer(output)  # â† å†…éƒ¨è®¡ç®—äº†attentionï¼Œä½†æ²¡è¿”å›
    return output  # â† åªè¿”å›è¾“å‡º
```

### SpatialFeatureExtractorçš„è§£å†³æ–¹æ¡ˆ

æ‰‹åŠ¨é€å±‚æ‰§è¡Œï¼Œæ˜¾å¼è¯·æ±‚attentionæƒé‡ï¼š

```python
def _forward_transformer_with_attention(self, x):
    attention_weights_list = []

    for layer in self.transformer.layers:
        # æ‰‹åŠ¨è°ƒç”¨attentionï¼Œå¹¶è¦æ±‚è¿”å›æƒé‡
        attn_output, attn_weights = layer.self_attn(
            x, x, x,
            need_weights=True,              # â† å…³é”®ï¼
            average_attn_weights=False      # â† è¿”å›æ¯ä¸ªhead
        )

        attention_weights_list.append(attn_weights)

        # æ‰‹åŠ¨æ‰§è¡Œresidualã€normã€FFN...
        x = residual + layer.dropout1(attn_output)
        x = layer.norm1(x)
        # ... (çœç•¥FFNéƒ¨åˆ†)

    return x, torch.stack(attention_weights_list, dim=1)
```

è¯¦ç»†æŠ€æœ¯è¯´æ˜è¯·å‚è€ƒ `FEATURE_EXTRACTION_GUIDE.md`ã€‚

---

## ğŸƒ è¿è¡Œæ¼”ç¤º

```bash
# ç¡®ä¿å®‰è£…äº†PyTorch
pip install torch

# è¿è¡Œå®Œæ•´æ¼”ç¤º
python examples/extract_sst_internals_demo.py
```

æ¼”ç¤ºè¾“å‡ºï¼š
```
================================================================================
SSTå†…éƒ¨ç‰¹å¾æå–å®Œæ•´æ¼”ç¤º
================================================================================

Step 1: åˆ›å»ºåŒè¾“å‡ºSSTæ¨¡å‹
--------------------------------------------------------------------------------
æ¨¡å‹å‚æ•°é‡: 339,073

Step 2: å‡†å¤‡å†å²æ•°æ®ï¼ˆ60å¤©ï¼‰
--------------------------------------------------------------------------------
å†å²æ•°æ®å½¢çŠ¶: torch.Size([60, 23])
çœŸå®å€¼Tå½¢çŠ¶: torch.Size([60, 1])
çœŸå®å€¼T+1å½¢çŠ¶: torch.Size([60, 1])

Step 3: å¯¹æ¯”ä¸¤ç§æ¨ç†æ¨¡å¼
--------------------------------------------------------------------------------
ã€æ¨¡å¼1ã€‘å¸¸è§„æ¨ç†ï¼ˆä»…ç”¨äºé¢„æµ‹ï¼‰
  è¾“å…¥: torch.Size([1, 23])
  è¾“å‡º pred_T: torch.Size([1, 1])
  è¾“å‡º pred_T1: torch.Size([1, 1])
  âœ— æ— æ³•è·å–ä¸­é—´ç‰¹å¾

ã€æ¨¡å¼2ã€‘ç‰¹å¾æå–æ¨ç†ï¼ˆç”¨äºåˆ†æå’Œå¢å¼ºï¼‰
  è¾“å…¥: torch.Size([1, 23])
  è¾“å‡º pred_T: torch.Size([1, 1])
  è¾“å‡º pred_T1: torch.Size([1, 1])
  âœ“ ä¸­é—´ç‰¹å¾:
    - embeddings: torch.Size([1, 23, 128])
    - attention_weights: torch.Size([1, 3, 8, 23, 23])
    - encoder_output: torch.Size([1, 23, 128])
    - pooled_features: torch.Size([1, 128])

  éªŒè¯: ä¸¤ç§æ¨¡å¼çš„é¢„æµ‹æ˜¯å¦ä¸€è‡´?
    pred_Tå·®å¼‚: 3.91e-06
    pred_T1å·®å¼‚: 3.91e-06

... (æ›´å¤šè¾“å‡º)

ã€LSTMè¾“å…¥ã€‘
  - å½¢çŠ¶: torch.Size([1, 60, 44])
  - è¯´æ˜: 60ä¸ªæ—¶é—´æ­¥ï¼Œæ¯æ­¥44ç»´å‹ç¼©ç‰¹å¾

âœ“ æ¼”ç¤ºå®Œæˆï¼
```

---

## âœ… æ ¸å¿ƒè¦ç‚¹

| ç»´åº¦ | å¸¸è§„æ¨ç† | ç‰¹å¾æå–æ¨ç† |
|------|----------|--------------|
| **æ–¹æ³•** | `model(x)` | `model.forward_with_features(x)` |
| **è¿”å›** | ä»…é¢„æµ‹ | é¢„æµ‹ + ä¸­é—´ç‰¹å¾ |
| **Attention** | âŒ | âœ… |
| **Encoder Output** | âŒ | âœ… |
| **ç”¨é€”** | è®­ç»ƒ/ç”Ÿäº§ | åˆ†æ/å¢å¼º |

---

## ğŸ“š å‚è€ƒ

- **å®Œæ•´æŠ€æœ¯æ–‡æ¡£**: `docs/FEATURE_EXTRACTION_GUIDE.md`
- **å¯è¿è¡Œç¤ºä¾‹**: `examples/extract_sst_internals_demo.py`
- **æ ¸å¿ƒæ¨¡å‹**: `models/spatial_feature_extractor.py`
- **ç‰¹å¾æå–å™¨**: `models/relationship_extractors.py`

---

## ğŸ’¬ å¸¸è§é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆéœ€è¦åŒè¾“å‡ºï¼ˆTæ—¥å’ŒT+1æ—¥ï¼‰ï¼Ÿ**

A: ç†è®ºæ¡†æ¶è¦æ±‚ï¼š
- Tæ—¥é¢„æµ‹ = çº¯ç©ºé—´å“åº”ï¼ˆåªä¾èµ–ç©ºé—´å…³ç³»ï¼‰
- T+1æ—¥é¢„æµ‹ = ç©ºé—´å“åº” + æ—¶åºæ¼”åŒ–
- å·®å€¼ = çº¯æ—¶åºæˆåˆ†

è¿™æ ·å¯ä»¥åˆ†ç¦»ç©ºé—´å’Œæ—¶åºæ•ˆåº”ã€‚

**Q2: é™ç»´ä¸€å®šè¦ç”¨è¿™äº›æ–¹æ³•å—ï¼Ÿ**

A: ä¸ä¸€å®šã€‚ç¤ºä¾‹ä¸­çš„é™ç»´æ–¹æ³•ï¼ˆSimpleAttentionExtractorç­‰ï¼‰åªæ˜¯å‚è€ƒå®ç°ã€‚ä½ å¯ä»¥ï¼š
- ä½¿ç”¨PCAé™ç»´
- ä½¿ç”¨Autoencoder
- ä½¿ç”¨å› å­æ¨¡å‹
- æˆ–ç›´æ¥ç”¨åŸå§‹ç‰¹å¾ï¼ˆå¦‚æœLSTMèƒ½å¤„ç†ï¼‰

**Q3: LSTMçš„è¾“å…¥ä¸€å®šè¦44ç»´å—ï¼Ÿ**

A: ä¸ä¸€å®šã€‚44ç»´æ˜¯ç¤ºä¾‹ä¸­çš„é…ç½®ï¼ˆ10æ³¨æ„åŠ›+32ç¼–ç å™¨+2æ®‹å·®ï¼‰ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¯éƒ¨åˆ†çš„ç»´åº¦ã€‚

**Q4: èƒ½ä¸èƒ½åªç”¨æ®‹å·®ï¼Œä¸ç”¨attentionå’Œencoderï¼Ÿ**

A: å¯ä»¥ï¼Œä½†ä¼šä¸¢å¤±å…³ç³»ä¿¡æ¯ã€‚ç†è®ºæ¡†æ¶å¼ºè°ƒï¼š
- Attentionæ•æ‰"è°å½±å“è°"
- Encoderæ•æ‰"ä¸Šä¸‹æ–‡åµŒå…¥"
- æ®‹å·®æ•æ‰"ç³»ç»Ÿæ€§åå·®"

ä¸‰è€…ç»“åˆæ•ˆæœæ›´å¥½ã€‚

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. **ç†è§£ç†è®º**: é˜…è¯» `FEATURE_EXTRACTION_GUIDE.md`
2. **è¿è¡Œç¤ºä¾‹**: æ‰§è¡Œ `extract_sst_internals_demo.py`
3. **å®ç°Stage3**: æ„å»ºLSTMå¢å¼ºæ¨¡å‹
4. **å®éªŒéªŒè¯**: å¯¹æ¯”SST vs SST+LSTMçš„æ€§èƒ½

ç¥ä½ æˆåŠŸï¼ğŸ‰
