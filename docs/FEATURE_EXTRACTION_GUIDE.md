# SSTå†…éƒ¨ç‰¹å¾æå–å®Œå…¨æŒ‡å—

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

**é—®é¢˜**ï¼šé€šè¿‡SSTæ¨¡å‹çš„å¸¸è§„æ¨ç†ï¼ˆ`forward()`ï¼‰ï¼Œåªèƒ½å¾—åˆ°Tæ—¥å’ŒT+1æ—¥çš„é¢„æµ‹è¾“å‡ºï¼Œå¦‚ä½•æå–å†…éƒ¨ç‰¹å¾ï¼ˆattention weightsã€encoder outputï¼‰ï¼Ÿ

**ç­”æ¡ˆ**ï¼šéœ€è¦ä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•ï¼Œä¸æ˜¯å¸¸è§„æ¨ç†ï¼

---

## ğŸ“Š ä¸¤ç§æ¨ç†æ¨¡å¼å¯¹æ¯”

### æ¨¡å¼1ï¸âƒ£ï¼šå¸¸è§„æ¨ç†ï¼ˆä»…ç”¨äºé¢„æµ‹ï¼‰

```python
# æ ‡å‡†çš„forwardè°ƒç”¨
predictions = model(boundary_conditions)

# è¾“å‡ºï¼š
# predictions: [batch, num_target_sensors]
#
# é—®é¢˜ï¼šæ— æ³•è·å–ä¸­é—´ç‰¹å¾ï¼
```

**ç‰¹ç‚¹**ï¼š
- âœ… å¿«é€Ÿã€é«˜æ•ˆ
- âœ… ç”¨äºè®­ç»ƒå’Œç”Ÿäº§æ¨ç†
- âŒ åªè¿”å›æœ€ç»ˆé¢„æµ‹
- âŒ æ— æ³•è·å–attention weights
- âŒ æ— æ³•è·å–encoder output

---

### æ¨¡å¼2ï¸âƒ£ï¼šç‰¹å¾æå–æ¨ç†ï¼ˆç”¨äºåˆ†æå’Œå¢å¼ºï¼‰

```python
# ä½¿ç”¨SpatialFeatureExtractorçš„ä¸“é—¨æ–¹æ³•
predictions, features = model.forward_with_features(
    boundary_conditions,
    return_attention=True,
    return_encoder_output=True
)

# è¾“å‡ºï¼š
# predictions: [batch, num_target_sensors] - é¢„æµ‹ç»“æœ
# features: dict - åŒ…å«æ‰€æœ‰ä¸­é—´ç‰¹å¾
#   {
#       'embeddings': [batch, num_sensors, d_model],
#       'encoder_output': [batch, num_sensors, d_model],
#       'attention_weights': [batch, num_layers, num_heads, num_sensors, num_sensors],
#       'pooled_features': [batch, d_model]
#   }
```

**ç‰¹ç‚¹**ï¼š
- âœ… è¿”å›æ‰€æœ‰ä¸­é—´ç‰¹å¾
- âœ… å¯ä»¥åˆ†ææ¨¡å‹å†…éƒ¨æœºåˆ¶
- âœ… ç”¨äºæ„å»ºå¢å¼ºæ¨¡å‹ï¼ˆå¦‚Stage3 LSTMï¼‰
- âš ï¸ è®¡ç®—ç¨æ…¢ï¼ˆéœ€è¦é¢å¤–çš„å¼ é‡æ“ä½œï¼‰

---

## ğŸ”¬ æŠ€æœ¯å®ç°åŸç†

### ä¸ºä»€ä¹ˆå¸¸è§„`forward()`æ— æ³•è¿”å›ä¸­é—´ç‰¹å¾ï¼Ÿ

PyTorchçš„æ ‡å‡†`nn.TransformerEncoder`è®¾è®¡å¦‚ä¸‹ï¼š

```python
# PyTorchæºç ï¼ˆç®€åŒ–ï¼‰
class TransformerEncoder(nn.Module):
    def forward(self, src):
        output = src
        for layer in self.layers:
            output = layer(output)  # â† è¿™é‡Œè°ƒç”¨äº†attentionï¼Œä½†æ²¡æœ‰è¿”å›æƒé‡
        return output  # â† åªè¿”å›æœ€ç»ˆè¾“å‡º
```

**æ ¸å¿ƒé—®é¢˜**ï¼š
- `TransformerEncoderLayer.forward()` å†…éƒ¨è°ƒç”¨äº† `self.self_attn()`
- ä½†å®ƒ**ä¸è¿”å›**attention weightsï¼ˆå°½ç®¡å†…éƒ¨è®¡ç®—äº†ï¼‰
- æ ‡å‡†forwardåªè¿”å›transformerçš„è¾“å‡ºå¼ é‡

---

### `SpatialFeatureExtractor`çš„è§£å†³æ–¹æ¡ˆ

é€šè¿‡**æ‰‹åŠ¨é€å±‚æ‰§è¡Œ**transformerï¼Œåœ¨æ¯ä¸€å±‚æ‰‹åŠ¨è°ƒç”¨`self_attn()`å¹¶è®¾ç½®`need_weights=True`ï¼š

```python
def _forward_transformer_with_attention(self, x):
    attention_weights_list = []

    # é€å±‚æ‰§è¡Œï¼ˆè€Œä¸æ˜¯è°ƒç”¨self.transformer(x)ï¼‰
    for layer in self.transformer.layers:
        residual = x

        # æ‰‹åŠ¨è°ƒç”¨MultiheadAttentionï¼Œå¹¶è¦æ±‚è¿”å›æƒé‡
        attn_output, attn_weights = layer.self_attn(
            x, x, x,
            need_weights=True,              # â† å…³é”®ï¼
            average_attn_weights=False      # â† è¿”å›æ¯ä¸ªheadçš„æƒé‡
        )

        attention_weights_list.append(attn_weights)

        # æ‰‹åŠ¨æ‰§è¡Œå‰©ä½™çš„æ“ä½œï¼ˆdropoutã€residualã€normã€FFNï¼‰
        x = residual + layer.dropout1(attn_output)
        x = layer.norm1(x)
        residual = x
        ff_output = layer.linear2(layer.dropout(layer.activation(layer.linear1(x))))
        x = residual + layer.dropout2(ff_output)
        x = layer.norm2(x)

    # åˆå¹¶æ‰€æœ‰å±‚çš„attention
    attention_weights = torch.stack(attention_weights_list, dim=1)

    return x, attention_weights
```

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š
1. **ç»•è¿‡æ ‡å‡†forward**ï¼šä¸è°ƒç”¨`self.transformer(x)`
2. **æ‰‹åŠ¨é€å±‚æ‰§è¡Œ**ï¼šç›´æ¥è®¿é—®`self.transformer.layers`
3. **æ˜¾å¼è¯·æ±‚æƒé‡**ï¼š`need_weights=True, average_attn_weights=False`
4. **æ‰‹åŠ¨å®ç°residual connectionå’Œnormalization**ï¼šå¤ç°TransformerEncoderLayerçš„å®Œæ•´é€»è¾‘

---

## ğŸ”§ å®Œæ•´ä½¿ç”¨æµç¨‹

### Step 1: åˆ›å»ºæ”¯æŒç‰¹å¾æå–çš„æ¨¡å‹

```python
from models.spatial_feature_extractor import SpatialFeatureExtractor

# åˆ›å»ºæ¨¡å‹
model = SpatialFeatureExtractor(
    num_boundary_sensors=23,  # 20ä¸ªè‚¡ç¥¨ + 3ä¸ªæŒ‡æ•°
    num_target_sensors=1,     # é¢„æµ‹1ä¸ªç›®æ ‡
    d_model=128,
    nhead=8,
    num_layers=3,
    enable_feature_extraction=True  # â† å¯ç”¨ç‰¹å¾æå–
)

# åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
model.load_state_dict(torch.load('sst_model.pth'))
model.eval()
```

### Step 2: å‡†å¤‡è¾“å…¥æ•°æ®

```python
import torch

# è¾¹ç•Œæ¡ä»¶ï¼šå¤§ç›˜ã€æ¿å—ã€é¾™å¤´
boundary_conditions = torch.tensor([
    # [æŒ‡æ•°1, æŒ‡æ•°2, æŒ‡æ•°3, è‚¡ç¥¨1, è‚¡ç¥¨2, ..., è‚¡ç¥¨20]
    [0.01, 0.02, 0.015, 0.005, -0.003, ...]  # Tæ—¥çš„æ•°æ®
], dtype=torch.float32)

# shape: [1, 23]
```

### Step 3: æå–å®Œæ•´ç‰¹å¾

```python
with torch.no_grad():
    predictions, features = model.forward_with_features(
        boundary_conditions,
        return_attention=True,
        return_encoder_output=True
    )

# æ£€æŸ¥è¾“å‡º
print(f"é¢„æµ‹: {predictions.shape}")  # [1, 1]
print(f"Embeddings: {features['embeddings'].shape}")  # [1, 23, 128]
print(f"Encoder Output: {features['encoder_output'].shape}")  # [1, 23, 128]
print(f"Attention: {features['attention_weights'].shape}")  # [1, 3, 8, 23, 23]
print(f"Pooled: {features['pooled_features'].shape}")  # [1, 128]
```

### Step 4: å¿«é€Ÿæå–å•ä¸€ç‰¹å¾

å¦‚æœåªéœ€è¦æŸä¸€ç±»ç‰¹å¾ï¼ˆæ›´é«˜æ•ˆï¼‰ï¼š

```python
# åªæå–encoder output
encoder_output = model.get_encoder_output(boundary_conditions)
# shape: [1, 23, 128]

# åªæå–attention weights
attention_weights = model.get_attention_weights(boundary_conditions)
# shape: [1, 3, 8, 23, 23]
```

---

## ğŸ¯ åŒè¾“å‡ºé—®é¢˜ï¼šTæ—¥ vs T+1æ—¥

### é—®é¢˜æè¿°

ç†è®ºæ¡†æ¶è¦æ±‚ï¼š
- **è¾“å‡º1ï¼ˆTæ—¥ï¼‰**ï¼šåŒæ—¶åˆ»é¢„æµ‹ï¼ˆçº¯ç©ºé—´å“åº”ï¼‰
- **è¾“å‡º2ï¼ˆT+1æ—¥ï¼‰**ï¼šæ¬¡æ—¥é¢„æµ‹ï¼ˆç©ºé—´+æ—¶åºï¼‰

ä½†å½“å‰`StaticSensorTransformer`åªæœ‰**å•è¾“å‡ºå¤´**ï¼š

```python
# å½“å‰å®ç°ï¼ˆå•è¾“å‡ºï¼‰
self.output_projection = nn.Linear(d_model, num_target_sensors)
```

### è§£å†³æ–¹æ¡ˆï¼šæ‰©å±•ä¸ºåŒè¾“å‡ºå¤´

éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼š

```python
class DualOutputSST(SpatialFeatureExtractor):
    """åŒè¾“å‡ºSSTï¼šåŒæ—¶é¢„æµ‹Tæ—¥å’ŒT+1æ—¥"""

    def __init__(self, num_boundary_sensors, num_target_sensors, **kwargs):
        super().__init__(num_boundary_sensors, num_target_sensors, **kwargs)

        # æ›¿æ¢å•ä¸€è¾“å‡ºå±‚ä¸ºåŒè¾“å‡ºå¤´
        self.output_projection_T = nn.Linear(self.d_model, num_target_sensors)
        self.output_projection_T1 = nn.Linear(self.d_model, num_target_sensors)

    def forward(self, boundary_conditions):
        """æ ‡å‡†forwardï¼šè¿”å›åŒè¾“å‡º"""
        batch_size = boundary_conditions.shape[0]

        # 1. Embed
        x = boundary_conditions.unsqueeze(-1)
        x = self.boundary_embedding(x) + self.boundary_position_encoding.unsqueeze(0)

        # 2. Transform
        x = self.transformer(x)

        # 3. Pool
        x_pooled = x.permute(0, 2, 1)
        x_pooled = self.global_pool(x_pooled).squeeze(-1)

        # 4. åŒè¾“å‡º
        pred_T = self.output_projection_T(x_pooled)    # Tæ—¥é¢„æµ‹
        pred_T1 = self.output_projection_T1(x_pooled)  # T+1æ—¥é¢„æµ‹

        return pred_T, pred_T1

    def forward_with_features(self, boundary_conditions, **kwargs):
        """å¸¦ç‰¹å¾çš„åŒè¾“å‡ºforward"""
        batch_size = boundary_conditions.shape[0]
        features = {}

        # 1. Embed
        x = boundary_conditions.unsqueeze(-1)
        x = self.boundary_embedding(x) + self.boundary_position_encoding.unsqueeze(0)
        features['embeddings'] = x.clone()

        # 2. Transform (with attention)
        if kwargs.get('return_attention', True):
            encoder_output, attention_weights = self._forward_transformer_with_attention(x)
            features['attention_weights'] = attention_weights
        else:
            encoder_output = self.transformer(x)

        features['encoder_output'] = encoder_output

        # 3. Pool
        x_pooled = encoder_output.permute(0, 2, 1)
        x_pooled = self.global_pool(x_pooled).squeeze(-1)
        features['pooled_features'] = x_pooled

        # 4. åŒè¾“å‡º
        pred_T = self.output_projection_T(x_pooled)
        pred_T1 = self.output_projection_T1(x_pooled)

        return (pred_T, pred_T1), features
```

### ä½¿ç”¨åŒè¾“å‡ºæ¨¡å‹

```python
# è®­ç»ƒæ—¶
pred_T, pred_T1 = model(boundary_conditions)
loss = criterion(pred_T, target_T) + criterion(pred_T1, target_T1)

# ç‰¹å¾æå–æ—¶
(pred_T, pred_T1), features = model.forward_with_features(
    boundary_conditions,
    return_attention=True,
    return_encoder_output=True
)

# è®¡ç®—æ®‹å·®
residual_T = target_T - pred_T      # ç©ºé—´æ®‹å·®
residual_T1 = target_T1 - pred_T1   # æ—¶ç©ºæ®‹å·®
```

---

## ğŸ“¦ å®Œæ•´æ•°æ®æµï¼ˆ60å¤©å†å²ï¼‰

```python
import torch
import numpy as np

# å‡è®¾æœ‰60å¤©å†å²æ•°æ®
num_days = 60
batch_size = 1
num_sensors = 23
d_model = 128

# å­˜å‚¨æ‰€æœ‰å¤©çš„ç‰¹å¾
all_features = {
    'attention': [],
    'encoder_output': [],
    'residual_T': [],
    'residual_T1': []
}

# é€å¤©æå–
for day in range(num_days):
    # å½“å¤©çš„è¾¹ç•Œæ¡ä»¶
    boundary_conditions = historical_data[day]  # shape: [1, 23]

    # è·å–çœŸå®å€¼
    target_T = true_values_T[day]    # shape: [1, 1]
    target_T1 = true_values_T1[day]  # shape: [1, 1]

    # æå–ç‰¹å¾
    with torch.no_grad():
        (pred_T, pred_T1), features = model.forward_with_features(
            boundary_conditions,
            return_attention=True,
            return_encoder_output=True
        )

    # è®¡ç®—æ®‹å·®
    residual_T = target_T - pred_T
    residual_T1 = target_T1 - pred_T1

    # ä¿å­˜
    all_features['attention'].append(features['attention_weights'])
    all_features['encoder_output'].append(features['encoder_output'])
    all_features['residual_T'].append(residual_T)
    all_features['residual_T1'].append(residual_T1)

# åˆå¹¶æˆåºåˆ—
attention_sequence = torch.cat(all_features['attention'], dim=0)
# shape: [60, num_layers, num_heads, num_sensors, num_sensors]

encoder_sequence = torch.cat(all_features['encoder_output'], dim=0)
# shape: [60, num_sensors, d_model]

residual_T_sequence = torch.cat(all_features['residual_T'], dim=0)
# shape: [60, 1]

residual_T1_sequence = torch.cat(all_features['residual_T1'], dim=0)
# shape: [60, 1]
```

---

## ğŸ§® é™ç»´åæ„å»ºLSTMè¾“å…¥

```python
from models.relationship_extractors import AttentionBasedExtractor, EmbeddingBasedExtractor

# Step 1: åˆ›å»ºç‰¹å¾æå–å™¨ï¼ˆé™ç»´ï¼‰
attention_extractor = AttentionBasedExtractor(
    num_sensors=23,
    output_dim=10,  # 9600ç»´ â†’ 10ç»´
    method='graph_features'
)

embedding_extractor = EmbeddingBasedExtractor(
    d_model=128,
    output_dim=32,  # 2560ç»´ â†’ 32ç»´
    pooling_method='autoencoder'
)

# Step 2: é€å¤©é™ç»´
lstm_input_sequence = []

for day in range(60):
    # æå–attentionç‰¹å¾ï¼ˆ10ç»´ï¼‰
    attn_features = attention_extractor(
        all_features['attention'][day],
        target_stock_idx=0
    )  # shape: [1, 10]

    # æå–encoderç‰¹å¾ï¼ˆ32ç»´ï¼‰
    enc_features = embedding_extractor(
        encoder_output=all_features['encoder_output'][day],
        target_stock_idx=0
    )  # shape: [1, 32]

    # æ®‹å·®ç‰¹å¾ï¼ˆ2ç»´ï¼‰
    res_features = torch.cat([
        all_features['residual_T'][day],
        all_features['residual_T1'][day]
    ], dim=-1)  # shape: [1, 2]

    # åˆå¹¶ï¼ˆ10+32+2=44ç»´ï¼‰
    day_features = torch.cat([attn_features, enc_features, res_features], dim=-1)
    lstm_input_sequence.append(day_features)

# Step 3: æ„å»ºLSTMè¾“å…¥
lstm_input = torch.cat(lstm_input_sequence, dim=0)
# shape: [60, 44]

# æ·»åŠ batchç»´åº¦
lstm_input = lstm_input.unsqueeze(0)
# shape: [1, 60, 44]
```

---

## âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“

| ç»´åº¦ | å¸¸è§„æ¨ç† | ç‰¹å¾æå–æ¨ç† |
|------|----------|--------------|
| **æ–¹æ³•** | `model(x)` | `model.forward_with_features(x)` |
| **è¿”å›** | ä»…é¢„æµ‹ | é¢„æµ‹ + ä¸­é—´ç‰¹å¾ |
| **Attention** | âŒ | âœ… |
| **Encoder Output** | âŒ | âœ… |
| **ç”¨é€”** | è®­ç»ƒ/ç”Ÿäº§ | åˆ†æ/å¢å¼º |
| **é€Ÿåº¦** | å¿« | ç¨æ…¢ |

**å…³é”®ç†è§£**ï¼š
1. å¸¸è§„`forward()`è°ƒç”¨`nn.TransformerEncoder`ï¼Œå®ƒä¸è¿”å›attentionæƒé‡
2. éœ€è¦æ‰‹åŠ¨é€å±‚æ‰§è¡Œï¼Œå¹¶åœ¨`self_attn()`è°ƒç”¨æ—¶è®¾ç½®`need_weights=True`
3. `SpatialFeatureExtractor`å·²ç»å®ç°äº†è¿™ä¸ªé€»è¾‘
4. åŒè¾“å‡ºéœ€è¦æ‰©å±•æ¨¡å‹ï¼ˆæ·»åŠ ä¸¤ä¸ªè¾“å‡ºå¤´ï¼‰

---

## ğŸ“š å‚è€ƒä»£ç ä½ç½®

- **SpatialFeatureExtractor**: `/home/user/Quant-Stock-Transformer/models/spatial_feature_extractor.py`
- **å…³é”®æ–¹æ³•**:
  - `forward_with_features()`: ç¬¬79-130è¡Œ
  - `_forward_transformer_with_attention()`: ç¬¬132-184è¡Œ
  - `get_encoder_output()`: ç¬¬236-248è¡Œ
  - `get_attention_weights()`: ç¬¬250-262è¡Œ
