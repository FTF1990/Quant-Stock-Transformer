{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ è‚¡ç¥¨é¢„æµ‹æ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹\n",
    "\n",
    "**æµç¨‹æ¦‚è§ˆ**ï¼š\n",
    "1. ğŸ“¥ åŠ è½½æ™ºèƒ½ä½“ç”Ÿæˆçš„æ•°æ®\n",
    "2. ğŸ”„ æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹\n",
    "3. ğŸ§  Stage 1: åŒè¾“å‡ºSSTæ¨¡å‹è®­ç»ƒï¼ˆTæ—¥ + T+1æ—¥é¢„æµ‹ï¼‰\n",
    "4. ğŸ” Stage 2: ç‰¹å¾æå–ï¼ˆAttention + Encoder + æ®‹å·®ï¼‰\n",
    "5. â° Stage 3: æ—¶åºæ¨¡å‹è®­ç»ƒï¼ˆiTransformer + LSTM + GRUï¼‰\n",
    "6. ğŸ“Š Stage 4: æ¨¡å‹æ•ˆæœéªŒè¯å’Œå¯¹æ¯”\n",
    "\n",
    "**ç›®æ ‡**ï¼šé¢„æµ‹æŒ‡å®šè‚¡ç¥¨çš„T+1æ—¥æ¶¨è·Œå¹…\n",
    "\n",
    "---\n",
    "\n",
    "**çŠ¶æ€**: ğŸš§ å¼€å‘æµ‹è¯•ä¸­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…‹éš†é¡¹ç›®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰\n",
    "!git clone https://github.com/FTF1990/Quant-Stock-Transformer.git /content/Quant-Stock-Transformer 2>/dev/null || echo \"é¡¹ç›®å·²å­˜åœ¨\"\n",
    "%cd /content/Quant-Stock-Transformer\n",
    "\n",
    "# å®‰è£…ä¾èµ–\n",
    "!pip install -q torch pandas numpy scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "print(\"âœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "sys.path.append('/content/Quant-Stock-Transformer')\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
    "from models.spatial_feature_extractor import SpatialFeatureExtractor\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾æ ·å¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Step 1: åŠ è½½æ•°æ®\n",
    "\n",
    "åŠ è½½æ™ºèƒ½ä½“ç”Ÿæˆçš„è‚¡ç¥¨åˆ—è¡¨å’Œå†å²æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# é…ç½®ï¼šæŒ‡å®šæ•°æ®è·¯å¾„å’Œç›®æ ‡è‚¡ç¥¨\n",
    "# ============================================================================\n",
    "\n",
    "STOCKS_JSON_PATH = \"selected_stocks.json\"  # æ™ºèƒ½ä½“ç”Ÿæˆçš„è‚¡ç¥¨åˆ—è¡¨\n",
    "HISTORICAL_DATA_PATH = \"historical_data.pkl\"  # æŠ“å–çš„å†å²æ•°æ®\n",
    "\n",
    "TARGET_MARKET = \"CN\"  # ç›®æ ‡å¸‚åœº\n",
    "TARGET_STOCK_SYMBOL = \"600519\"  # è¦é¢„æµ‹çš„è‚¡ç¥¨ï¼ˆè´µå·èŒ…å°ç¤ºä¾‹ï¼‰\n",
    "\n",
    "# ============================================================================\n",
    "# åŠ è½½æ•°æ®\n",
    "# ============================================================================\n",
    "\n",
    "# åŠ è½½è‚¡ç¥¨åˆ—è¡¨\n",
    "with open(STOCKS_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    selected_stocks = json.load(f)\n",
    "\n",
    "print(f\"âœ“ åŠ è½½è‚¡ç¥¨åˆ—è¡¨: {sum(len(v) for v in selected_stocks.values())}åªè‚¡ç¥¨\")\n",
    "for market, stocks in selected_stocks.items():\n",
    "    print(f\"  {market}: {len(stocks)}åª\")\n",
    "\n",
    "# åŠ è½½å†å²æ•°æ®\n",
    "with open(HISTORICAL_DATA_PATH, 'rb') as f:\n",
    "    historical_data = pickle.load(f)\n",
    "\n",
    "print(f\"\\nâœ“ åŠ è½½å†å²æ•°æ®å®Œæˆ\")\n",
    "\n",
    "# æ£€æŸ¥ç›®æ ‡è‚¡ç¥¨\n",
    "if TARGET_MARKET not in historical_data:\n",
    "    raise ValueError(f\"å¸‚åœº{TARGET_MARKET}æ•°æ®ä¸å­˜åœ¨\")\n",
    "\n",
    "if TARGET_STOCK_SYMBOL not in historical_data[TARGET_MARKET]:\n",
    "    raise ValueError(f\"è‚¡ç¥¨{TARGET_STOCK_SYMBOL}æ•°æ®ä¸å­˜åœ¨\")\n",
    "\n",
    "print(f\"\\nâœ“ ç›®æ ‡è‚¡ç¥¨: {TARGET_MARKET} - {TARGET_STOCK_SYMBOL}\")\n",
    "print(f\"  æ•°æ®æ¡æ•°: {len(historical_data[TARGET_MARKET][TARGET_STOCK_SYMBOL])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 2: æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataProcessor:\n",
    "    \"\"\"è‚¡ç¥¨æ•°æ®é¢„å¤„ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, historical_data: Dict, target_market: str, target_stock: str):\n",
    "        self.historical_data = historical_data\n",
    "        self.target_market = target_market\n",
    "        self.target_stock = target_stock\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_training_data(self, sequence_length: int = 60):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "        \n",
    "        Returns:\n",
    "            boundary_conditions: [N, num_boundary_sensors] - è¾¹ç•Œæ¡ä»¶ï¼ˆæŒ‡æ•°+è‚¡ç¥¨ï¼‰\n",
    "            targets_T: [N, 1] - Tæ—¥ç›®æ ‡ï¼ˆå½“æ—¥æ”¶ç›Šç‡ï¼‰\n",
    "            targets_T1: [N, 1] - T+1æ—¥ç›®æ ‡ï¼ˆæ¬¡æ—¥æ”¶ç›Šç‡ï¼‰\n",
    "            dates: [N] - æ—¥æœŸç´¢å¼•\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...\")\n",
    "        \n",
    "        # è·å–å¸‚åœºæ•°æ®\n",
    "        market_data = self.historical_data[self.target_market]\n",
    "        \n",
    "        # è·å–å¤§ç›˜æŒ‡æ•°\n",
    "        index_df = market_data.get('_INDEX_')\n",
    "        if index_df is None:\n",
    "            print(\"  è­¦å‘Š: æ— å¤§ç›˜æŒ‡æ•°æ•°æ®\")\n",
    "        \n",
    "        # è·å–ç›®æ ‡è‚¡ç¥¨æ•°æ®\n",
    "        target_df = market_data[self.target_stock].copy()\n",
    "        \n",
    "        # è®¡ç®—æ”¶ç›Šç‡\n",
    "        target_df['return'] = target_df['Close'].pct_change()\n",
    "        target_df['return_next'] = target_df['return'].shift(-1)  # T+1æ—¥æ”¶ç›Šç‡\n",
    "        \n",
    "        # ç§»é™¤NaN\n",
    "        target_df = target_df.dropna()\n",
    "        \n",
    "        print(f\"  âœ“ ç›®æ ‡è‚¡ç¥¨æ•°æ®: {len(target_df)}æ¡\")\n",
    "        \n",
    "        # æ„å»ºè¾¹ç•Œæ¡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨ç›®æ ‡è‚¡ç¥¨çš„å†å²æ•°æ®ä½œä¸ºç‰¹å¾ï¼‰\n",
    "        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŒ…å«å¤§ç›˜ã€æ¿å—ã€é¾™å¤´è‚¡ç­‰\n",
    "        boundary_features = []\n",
    "        targets_T = []\n",
    "        targets_T1 = []\n",
    "        dates = []\n",
    "        \n",
    "        # ä½¿ç”¨æ»‘åŠ¨çª—å£\n",
    "        for i in range(len(target_df) - 1):\n",
    "            # è¾¹ç•Œæ¡ä»¶ï¼šå½“æ—¥çš„OHLCV\n",
    "            boundary = [\n",
    "                target_df['Open'].iloc[i],\n",
    "                target_df['High'].iloc[i],\n",
    "                target_df['Low'].iloc[i],\n",
    "                target_df['Close'].iloc[i],\n",
    "                target_df['Volume'].iloc[i]\n",
    "            ]\n",
    "            \n",
    "            # æ·»åŠ å¤§ç›˜æŒ‡æ•°ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "            if index_df is not None and len(index_df) > i:\n",
    "                boundary.extend([\n",
    "                    index_df['Close'].iloc[i]\n",
    "                ])\n",
    "            \n",
    "            boundary_features.append(boundary)\n",
    "            targets_T.append(target_df['return'].iloc[i])\n",
    "            targets_T1.append(target_df['return_next'].iloc[i])\n",
    "            dates.append(target_df.index[i])\n",
    "        \n",
    "        # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
    "        boundary_features = np.array(boundary_features, dtype=np.float32)\n",
    "        targets_T = np.array(targets_T, dtype=np.float32).reshape(-1, 1)\n",
    "        targets_T1 = np.array(targets_T1, dtype=np.float32).reshape(-1, 1)\n",
    "        \n",
    "        # æ ‡å‡†åŒ–è¾¹ç•Œæ¡ä»¶\n",
    "        boundary_features = self.scaler.fit_transform(boundary_features)\n",
    "        \n",
    "        print(f\"  âœ“ è¾¹ç•Œæ¡ä»¶å½¢çŠ¶: {boundary_features.shape}\")\n",
    "        print(f\"  âœ“ Tæ—¥ç›®æ ‡å½¢çŠ¶: {targets_T.shape}\")\n",
    "        print(f\"  âœ“ T+1æ—¥ç›®æ ‡å½¢çŠ¶: {targets_T1.shape}\")\n",
    "        \n",
    "        return boundary_features, targets_T, targets_T1, dates\n",
    "\n",
    "\n",
    "# å®ä¾‹åŒ–å¤„ç†å™¨\n",
    "processor = StockDataProcessor(\n",
    "    historical_data=historical_data,\n",
    "    target_market=TARGET_MARKET,\n",
    "    target_stock=TARGET_STOCK_SYMBOL\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "X, y_T, y_T1, dates = processor.prepare_training_data()\n",
    "\n",
    "print(f\"\\nâœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
    "print(f\"  æ€»æ ·æœ¬æ•°: {len(X)}\")\n",
    "print(f\"  ç‰¹å¾ç»´åº¦: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 3: æ•°æ®é›†åˆ’åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¶é—´åºåˆ—æ•°æ®ï¼šæŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_T_train = y_T[:train_size]\n",
    "y_T1_train = y_T1[:train_size]\n",
    "\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_T_val = y_T[train_size:train_size+val_size]\n",
    "y_T1_val = y_T1[train_size:train_size+val_size]\n",
    "\n",
    "X_test = X[train_size+val_size:]\n",
    "y_T_test = y_T[train_size+val_size:]\n",
    "y_T1_test = y_T1[train_size+val_size:]\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬\")\n",
    "print(f\"éªŒè¯é›†: {len(X_val)} æ ·æœ¬\")\n",
    "print(f\"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Stage 1: åŒè¾“å‡ºSSTæ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "è®­ç»ƒåŒæ—¶é¢„æµ‹Tæ—¥å’ŒT+1æ—¥æ”¶ç›Šç‡çš„SSTæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»ç¤ºä¾‹ä»£ç å¯¼å…¥åŒè¾“å‡ºSST\n",
    "# ï¼ˆè¿™é‡Œç®€åŒ–å®ç°ï¼Œå®Œæ•´ç‰ˆå‚è€ƒ examples/extract_sst_internals_demo.pyï¼‰\n",
    "\n",
    "class DualOutputSST(SpatialFeatureExtractor):\n",
    "    \"\"\"åŒè¾“å‡ºSST\"\"\"\n",
    "    \n",
    "    def __init__(self, num_boundary_sensors, num_target_sensors, **kwargs):\n",
    "        super().__init__(num_boundary_sensors, num_target_sensors, **kwargs)\n",
    "        \n",
    "        # åŒè¾“å‡ºå¤´\n",
    "        self.output_projection_T = nn.Linear(self.d_model, num_target_sensors)\n",
    "        self.output_projection_T1 = nn.Linear(self.d_model, num_target_sensors)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.output_projection_T.weight)\n",
    "        nn.init.xavier_uniform_(self.output_projection_T1.weight)\n",
    "    \n",
    "    def forward(self, boundary_conditions):\n",
    "        batch_size = boundary_conditions.shape[0]\n",
    "        \n",
    "        x = boundary_conditions.unsqueeze(-1)\n",
    "        x = self.boundary_embedding(x) + self.boundary_position_encoding.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        x_pooled = x.permute(0, 2, 1)\n",
    "        x_pooled = self.global_pool(x_pooled).squeeze(-1)\n",
    "        \n",
    "        pred_T = self.output_projection_T(x_pooled)\n",
    "        pred_T1 = self.output_projection_T1(x_pooled)\n",
    "        \n",
    "        return pred_T, pred_T1\n",
    "    \n",
    "    def forward_with_features(self, boundary_conditions, **kwargs):\n",
    "        batch_size = boundary_conditions.shape[0]\n",
    "        features = {}\n",
    "        \n",
    "        x = boundary_conditions.unsqueeze(-1)\n",
    "        x = self.boundary_embedding(x) + self.boundary_position_encoding.unsqueeze(0)\n",
    "        features['embeddings'] = x.clone()\n",
    "        \n",
    "        if kwargs.get('return_attention', True):\n",
    "            encoder_output, attention_weights = self._forward_transformer_with_attention(x)\n",
    "            features['attention_weights'] = attention_weights\n",
    "        else:\n",
    "            encoder_output = self.transformer(x)\n",
    "        \n",
    "        features['encoder_output'] = encoder_output\n",
    "        \n",
    "        x_pooled = encoder_output.permute(0, 2, 1)\n",
    "        x_pooled = self.global_pool(x_pooled).squeeze(-1)\n",
    "        features['pooled_features'] = x_pooled\n",
    "        \n",
    "        pred_T = self.output_projection_T(x_pooled)\n",
    "        pred_T1 = self.output_projection_T1(x_pooled)\n",
    "        \n",
    "        return (pred_T, pred_T1), features\n",
    "\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "num_features = X.shape[1]\n",
    "sst_model = DualOutputSST(\n",
    "    num_boundary_sensors=num_features,\n",
    "    num_target_sensors=1,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    "    enable_feature_extraction=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"âœ“ SSTæ¨¡å‹åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"  å‚æ•°é‡: {sum(p.numel() for p in sst_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒSSTæ¨¡å‹\n",
    "def train_sst(\n",
    "    model, \n",
    "    X_train, y_T_train, y_T1_train,\n",
    "    X_val, y_T_val, y_T1_val,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    lr=0.001\n",
    "):\n",
    "    \"\"\"è®­ç»ƒåŒè¾“å‡ºSSTæ¨¡å‹\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # è½¬æ¢ä¸ºtensor\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_T_train_t = torch.FloatTensor(y_T_train).to(device)\n",
    "    y_T1_train_t = torch.FloatTensor(y_T1_train).to(device)\n",
    "    \n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_T_val_t = torch.FloatTensor(y_T_val).to(device)\n",
    "    y_T1_val_t = torch.FloatTensor(y_T1_val).to(device)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒSSTæ¨¡å‹...\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # è®­ç»ƒ\n",
    "        epoch_loss = 0\n",
    "        num_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(X_train))\n",
    "            \n",
    "            batch_X = X_train_t[start_idx:end_idx]\n",
    "            batch_y_T = y_T_train_t[start_idx:end_idx]\n",
    "            batch_y_T1 = y_T1_train_t[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred_T, pred_T1 = model(batch_X)\n",
    "            \n",
    "            loss_T = criterion(pred_T, batch_y_T)\n",
    "            loss_T1 = criterion(pred_T1, batch_y_T1)\n",
    "            loss = loss_T + loss_T1\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        \n",
    "        # éªŒè¯\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred_T, val_pred_T1 = model(X_val_t)\n",
    "            val_loss_T = criterion(val_pred_T, y_T_val_t)\n",
    "            val_loss_T1 = criterion(val_pred_T1, y_T1_val_t)\n",
    "            val_loss = (val_loss_T + val_loss_T1).item()\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {epoch_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    print(\"\\nâœ“ SSTè®­ç»ƒå®Œæˆ\")\n",
    "    return history\n",
    "\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "sst_history = train_sst(\n",
    "    sst_model,\n",
    "    X_train, y_T_train, y_T1_train,\n",
    "    X_val, y_T_val, y_T1_val,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sst_history['train_loss'], label='Train Loss')\n",
    "plt.plot(sst_history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('SST Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Stage 2: ç‰¹å¾æå–\n",
    "\n",
    "ä»è®­ç»ƒå¥½çš„SSTæ¨¡å‹ä¸­æå–å†…éƒ¨ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¤cellç»§ç»­å®Œæ•´å®ç°...\n",
    "# ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæˆ‘ä¼šåœ¨åç»­cellä¸­ç»§ç»­è¡¥å……\n",
    "\n",
    "print(\"â­ï¸ ç»§ç»­ä¸‹ä¸€æ­¥...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
