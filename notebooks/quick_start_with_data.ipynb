{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Quick Start: Data Loading & Gradio Interface\n",
    "\n",
    "This notebook provides a streamlined workflow:\n",
    "1. **Load data** from Kaggle, Google Drive, or local files\n",
    "2. **Launch Gradio interface** with pre-loaded data\n",
    "3. **Start training** immediately\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "- [Cell 1: Environment Setup](#cell-1)\n",
    "- [Cell 2: Data Loading Options](#cell-2)\n",
    "- [Cell 3: Verify Data](#cell-3)\n",
    "- [Cell 4: Launch Gradio with Pre-loaded Data](#cell-4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell-1\"></a>\n",
    "## Cell 1: Environment Setup\n",
    "\n",
    "Install required packages and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch>=2.0.0 gradio>=4.0.0 pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0 matplotlib>=3.7.0 seaborn>=0.12.0 -q\n",
    "\n",
    "# Clone repository if not already present\n",
    "import os\n",
    "if not os.path.exists('Industrial-digital-twin-by-transformer'):\n",
    "    !git clone https://github.com/FTF1990/Industrial-digital-twin-by-transformer.git\n",
    "    os.chdir('Industrial-digital-twin-by-transformer')\n",
    "else:\n",
    "    os.chdir('Industrial-digital-twin-by-transformer')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell-2\"></a>\n",
    "## Cell 2: Data Loading Options\n",
    "\n",
    "Choose **ONE** of the following methods to load your data:\n",
    "\n",
    "### Option A: Load from Kaggle Dataset\n",
    "### Option B: Load from Google Drive\n",
    "### Option C: Upload Local File\n",
    "### Option D: Create Example Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION A: Load from Kaggle Dataset\n",
    "# ============================================================================\n",
    "def load_from_kaggle(dataset_name, file_name):\n",
    "    \"\"\"\n",
    "    Load dataset from Kaggle\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Kaggle dataset identifier (e.g., 'username/dataset-name')\n",
    "        file_name: CSV file name in the dataset\n",
    "    \n",
    "    Example:\n",
    "        df = load_from_kaggle('username/industrial-sensors', 'sensor_data.csv')\n",
    "    \"\"\"\n",
    "    print(\"üì¶ Setting up Kaggle API...\")\n",
    "    \n",
    "    # Upload kaggle.json if not present\n",
    "    if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "        print(\"‚ö†Ô∏è  Please upload your kaggle.json file:\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        !mkdir -p /root/.kaggle\n",
    "        with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "            f.write(list(uploaded.values())[0].decode('utf-8'))\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "    \n",
    "    # Install kaggle package\n",
    "    !pip install kaggle -q\n",
    "    \n",
    "    # Download dataset\n",
    "    print(f\"üì• Downloading {dataset_name}...\")\n",
    "    !kaggle datasets download -d {dataset_name} --unzip\n",
    "    \n",
    "    # Load CSV\n",
    "    print(f\"üìä Loading {file_name}...\")\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(f\"‚úÖ Loaded data: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION B: Load from Google Drive\n",
    "# ============================================================================\n",
    "def load_from_google_drive(file_path):\n",
    "    \"\"\"\n",
    "    Load dataset from Google Drive\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file in Google Drive (e.g., '/content/drive/MyDrive/data.csv')\n",
    "    \n",
    "    Example:\n",
    "        df = load_from_google_drive('/content/drive/MyDrive/sensor_data.csv')\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Mounting Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    print(f\"üìä Loading {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"‚úÖ Loaded data: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION C: Upload Local File\n",
    "# ============================================================================\n",
    "def load_from_upload():\n",
    "    \"\"\"\n",
    "    Upload and load CSV file from local computer\n",
    "    \n",
    "    Example:\n",
    "        df = load_from_upload()\n",
    "    \"\"\"\n",
    "    print(\"üì§ Please select your CSV file to upload...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Get the first uploaded file\n",
    "    file_name = list(uploaded.keys())[0]\n",
    "    print(f\"üìä Loading {file_name}...\")\n",
    "    \n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "    print(f\"‚úÖ Loaded data: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION D: Create Example Dataset\n",
    "# ============================================================================\n",
    "def create_example_data(n_samples=10000, n_boundary=10, n_target=5, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Create synthetic industrial sensor dataset\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of data points\n",
    "        n_boundary: Number of boundary (input) sensors\n",
    "        n_target: Number of target (output) sensors\n",
    "        noise_level: Noise standard deviation\n",
    "    \n",
    "    Example:\n",
    "        df = create_example_data(n_samples=10000, n_boundary=10, n_target=5)\n",
    "    \"\"\"\n",
    "    print(\"üîß Generating synthetic industrial sensor data...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate boundary sensor readings (inputs)\n",
    "    boundary_data = np.random.randn(n_samples, n_boundary) * 10 + 100\n",
    "    \n",
    "    # Generate target sensors with complex relationships to boundary sensors\n",
    "    target_data = np.zeros((n_samples, n_target))\n",
    "    \n",
    "    for i in range(n_target):\n",
    "        # Complex non-linear relationships\n",
    "        target_data[:, i] = (\n",
    "            0.3 * boundary_data[:, i % n_boundary] +\n",
    "            0.2 * boundary_data[:, (i+1) % n_boundary] ** 2 / 100 +\n",
    "            0.15 * boundary_data[:, (i+2) % n_boundary] * boundary_data[:, (i+3) % n_boundary] / 100 +\n",
    "            0.1 * np.sin(boundary_data[:, (i+4) % n_boundary] / 10) +\n",
    "            np.random.randn(n_samples) * noise_level\n",
    "        )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    boundary_cols = [f'boundary_{i+1}' for i in range(n_boundary)]\n",
    "    target_cols = [f'target_{i+1}' for i in range(n_target)]\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        np.hstack([boundary_data, target_data]),\n",
    "        columns=boundary_cols + target_cols\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created dataset: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"   - Boundary sensors: {n_boundary}\")\n",
    "    print(f\"   - Target sensors: {n_target}\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT YOUR DATA LOADING METHOD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ SELECT YOUR DATA LOADING METHOD\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUncomment ONE of the following options:\\n\")\n",
    "\n",
    "# OPTION A: Kaggle\n",
    "# df = load_from_kaggle('your-username/your-dataset', 'your_file.csv')\n",
    "\n",
    "# OPTION B: Google Drive\n",
    "# df = load_from_google_drive('/content/drive/MyDrive/your_data.csv')\n",
    "\n",
    "# OPTION C: Upload\n",
    "# df = load_from_upload()\n",
    "\n",
    "# OPTION D: Example Data (DEFAULT)\n",
    "df = create_example_data(n_samples=10000, n_boundary=10, n_target=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DATA LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell-3\"></a>\n",
    "## Cell 3: Verify Data\n",
    "\n",
    "Quick data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Data Overview:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìà Basic Statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Visualize first few columns\n",
    "print(\"\\nüìä Data Visualization (first 1000 samples):\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# Plot first 4 columns\n",
    "df.iloc[:1000, :4].plot(ax=axes[0], alpha=0.7)\n",
    "axes[0].set_title('First 4 Columns (1000 samples)')\n",
    "axes[0].set_xlabel('Sample Index')\n",
    "axes[0].set_ylabel('Sensor Value')\n",
    "axes[0].legend(loc='best', fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = df.iloc[:, :min(10, df.shape[1])].corr()\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm', center=0, ax=axes[1])\n",
    "axes[1].set_title('Correlation Matrix (first 10 columns)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete! Ready to launch Gradio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell-4\"></a>\n",
    "## Cell 4: Launch Gradio Interface with Pre-loaded Data\n",
    "\n",
    "This will:\n",
    "1. Save your loaded data to a temporary CSV file\n",
    "2. Launch the enhanced Gradio interface\n",
    "3. **Automatically load the data in Tab 1**\n",
    "4. You can immediately start training in Tab 2\n",
    "\n",
    "**Note**: The Gradio interface will open in Tab 1 showing your pre-loaded data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gradio as gr\n",
    "\n",
    "# Save data to temporary file\n",
    "temp_data_path = '/tmp/preloaded_data.csv'\n",
    "df.to_csv(temp_data_path, index=False)\n",
    "print(f\"üíæ Data saved to: {temp_data_path}\")\n",
    "print(f\"üìä Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Add project to path\n",
    "if os.path.exists('gradio_residual_tft_app.py'):\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "    print(\"‚úÖ Found gradio_residual_tft_app.py\")\n",
    "else:\n",
    "    print(\"‚ùå Error: gradio_residual_tft_app.py not found!\")\n",
    "    print(\"   Please make sure you're in the project directory.\")\n",
    "\n",
    "# Import and modify the Gradio app to auto-load data\n",
    "print(\"\\nüöÄ Launching Enhanced Gradio Interface...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìå Your data is PRE-LOADED in Tab 1!\")\n",
    "print(\"üìå You can immediately:\")\n",
    "print(\"   1. View your data in Tab 1\")\n",
    "print(\"   2. Select boundary and target signals\")\n",
    "print(\"   3. Start SST training in Tab 2\")\n",
    "print(\"   4. Continue with Stage2 Boost training\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚è≥ Loading interface...\\n\")\n",
    "\n",
    "# Launch the app with pre-loaded data\n",
    "# Note: We'll create a wrapper that auto-loads the data\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"gradio_app\", \"gradio_residual_tft_app.py\")\n",
    "gradio_module = importlib.util.module_from_spec(spec)\n",
    "\n",
    "# Inject pre-loaded data into global state\n",
    "import pandas as pd\n",
    "preloaded_df = pd.read_csv(temp_data_path)\n",
    "\n",
    "# Execute the module (this will create the interface)\n",
    "print(\"üì± Launching interface (this may take a moment)...\\n\")\n",
    "spec.loader.exec_module(gradio_module)\n",
    "\n",
    "# Inject preloaded data into global_state\n",
    "if hasattr(gradio_module, 'global_state'):\n",
    "    gradio_module.global_state['df'] = preloaded_df\n",
    "    gradio_module.global_state['all_signals'] = list(preloaded_df.columns)\n",
    "    print(\"‚úÖ Data injected into Gradio app!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ GRADIO INTERFACE IS READY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"üëâ Check Tab 1 - your data is already loaded!\")\n",
    "print(\"üëâ Select signals and start training immediately!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Quick Workflow Guide\n",
    "\n",
    "Once Gradio launches, follow this workflow:\n",
    "\n",
    "### Tab 1: Data Management\n",
    "‚úÖ Your data is already loaded!\n",
    "- View data statistics and preview\n",
    "- Select **boundary signals** (inputs)\n",
    "- Select **target signals** (outputs to predict)\n",
    "\n",
    "### Tab 2: SST Model Training\n",
    "- Configure model parameters (d_model, nhead, num_layers)\n",
    "- Set training hyperparameters (epochs, batch_size, learning rate)\n",
    "- Click \"Train SST Model\" and monitor progress\n",
    "- Model automatically saves after training\n",
    "\n",
    "### Tab 3: Residual Extraction\n",
    "- Select your trained SST model\n",
    "- Extract residuals (prediction errors)\n",
    "- Analyze residual patterns\n",
    "\n",
    "### Tab 4: Stage2 Boost Training\n",
    "- Select extracted residuals\n",
    "- Train Stage2 model to learn residual corrections\n",
    "- Further improve prediction accuracy\n",
    "\n",
    "### Tab 5: Ensemble Model Generation\n",
    "- Select base SST + Stage2 models\n",
    "- Set R¬≤ threshold (default: 0.4)\n",
    "- Generate intelligent ensemble model\n",
    "- View per-signal improvement metrics\n",
    "\n",
    "### Tab 6: Inference Comparison\n",
    "- Compare SST vs. Ensemble model performance\n",
    "- Visualize improvements\n",
    "- Analyze prediction quality\n",
    "\n",
    "### Tab 7: Sundial Forecasting (Optional)\n",
    "- Predict future residual trends\n",
    "- Long-term forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Tips\n",
    "\n",
    "1. **Data Requirements**:\n",
    "   - At least 1,000 samples recommended (10,000+ ideal)\n",
    "   - No missing values\n",
    "   - Numerical data only\n",
    "\n",
    "2. **Signal Selection**:\n",
    "   - Choose 5-20 boundary sensors\n",
    "   - Choose 3-10 target sensors\n",
    "   - More sensors = longer training time\n",
    "\n",
    "3. **Training Time**:\n",
    "   - SST: ~10-30 minutes (depends on data size and epochs)\n",
    "   - Stage2: ~10-20 minutes\n",
    "   - Use GPU for faster training\n",
    "\n",
    "4. **Performance**:\n",
    "   - Expected R¬≤: 0.8-0.95 for SST\n",
    "   - Stage2 boost: +15-25% accuracy improvement\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Documentation\n",
    "\n",
    "- **Quick Start**: `docs/QUICKSTART.md`\n",
    "- **Detailed Features**: `docs/ENHANCED_VERSION_README.md`\n",
    "- **Update Notes**: `docs/UPDATE_NOTES.md`\n",
    "- **Main README**: `README.md`\n",
    "\n",
    "---\n",
    "\n",
    "## üÜò Troubleshooting\n",
    "\n",
    "**Q: Gradio doesn't load my data**\n",
    "- Make sure Cell 2 ran successfully\n",
    "- Check that `df` variable exists: `print(df.shape)`\n",
    "\n",
    "**Q: Training is too slow**\n",
    "- Reduce batch_size or num_layers\n",
    "- Use fewer epochs for initial testing\n",
    "- Ensure GPU is available\n",
    "\n",
    "**Q: Out of memory error**\n",
    "- Reduce batch_size\n",
    "- Reduce d_model or num_layers\n",
    "- Use smaller data subsets for testing\n",
    "\n",
    "**Q: Model performance is poor**\n",
    "- Check data quality (no outliers, proper scaling)\n",
    "- Increase epochs (try 100-200)\n",
    "- Adjust learning rate (try 0.0001 - 0.01)\n",
    "- Ensure boundary signals are causally related to targets\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
