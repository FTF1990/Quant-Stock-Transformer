#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Gradio Interface for Industrial Digital Twin with Residual Boost Training
Complete Residual Boost Training System - Enhanced Gradio Interface
New features:
1. Stage2 residual model training (based on residuals generated by SST)
2. Intelligent R2 threshold selection to generate ensemble inference model
3. Secondary inference comparison (Ensemble model vs Pure SST model)
4. Sundial time series model predicting future residuals
"""
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from typing import Dict, List, Tuple, Any, Optional
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
import sys
import warnings
import traceback
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import matplotlib
import platform
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ============================================================================
# TFT model save function (added at top of file)

def save_tft_model_with_config(
        model_name: str,
        tft_model: nn.Module,
        config: Dict[str, Any],
        scalers: Dict[str, StandardScaler],
        residual_data_key: str,
        residual_info: Dict[str, Any],
        history: Dict[str, List[float]]
) -> Tuple[str, str, str]:
    """
    Save TFT model, config and scalers

    Args:
        model_name: TFTModel name
        tft_model: Trained TFT model
        config: Training config
        scalers: Data scalers
        residual_data_key: Residual data key
        residual_info: Residual data info
        history: Training history

    Returns:
        model_path: Model weight file path
        scaler_path: ScalerFile path
        inference_config_path: æ¨ç†é…ç½®File path
    """
    model_dir = "saved_models/tft_models"
    os.makedirs(model_dir, exist_ok=True)

    # 1. Save model weights
    model_path = os.path.join(model_dir, f"{model_name}.pth")
    torch.save({
        'model_state_dict': tft_model.state_dict(),
        'model_config': {
            'num_targets': tft_model.num_targets,
            'num_external_factors': tft_model.num_external_factors,
            'd_model': tft_model.d_model,
            'use_grouping': tft_model.use_grouping,
            'signal_groups': tft_model.signal_groups if hasattr(tft_model, 'signal_groups') else None
        },
        'training_config': config,
        'training_history': history,
        'residual_data_key': residual_data_key,
        'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }, model_path)

    # 2. Save scalers
    scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
    with open(scaler_path, 'wb') as f:
        pickle.dump(scalers, f)

    # 3. Save inference config JSON (most important)
    inference_config_path = os.path.join(model_dir, f"{model_name}_inference.json")
    inference_config = {
        'model_name': model_name,
        'model_type': 'ResidualTFT',
        'model_path': model_path,
        'scaler_path': scaler_path,

        # TFT model architecture
        'architecture': {
            'd_model': config['d_model'],
            'nhead': config['nhead'],
            'num_encoder_layers': config['num_encoder_layers'],
            'num_decoder_layers': config['num_decoder_layers'],
            'dropout': config['dropout'],
            'use_grouping': config.get('use_grouping', False),
            'signal_groups': config.get('signal_groups', None)
        },

        # Data config
        'data_config': {
            'encoder_length': config['encoder_length'],
            'future_horizon': residual_info['future_horizon'],
            'residual_data_key': residual_data_key,
            'base_model_name': residual_info['base_model_name'],
            'num_targets': len(residual_info['target_signals']),
            'num_external_factors': len(residual_info['boundary_signals'])
        },

        # Signal info
        'signals': {
            'boundary_signals': residual_info['boundary_signals'],
            'target_signals': residual_info['target_signals'],
            'residual_signals': residual_info['residual_signals']
        },

        # Training info
        'training_info': {
            'epochs_trained': len(history['train_losses']),
            'best_val_loss': min(history['val_losses']),
            'batch_size': config['batch_size'],
            'learning_rate': config['lr']
        },

        'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    with open(inference_config_path, 'w', encoding='utf-8') as f:
        json.dump(inference_config, f, indent=2, ensure_ascii=False)

    print(f"âœ… TFTæ¨¡å‹å·²ä¿å­˜:")
    print(f"   ğŸ“¦ æ¨¡å‹æƒé‡: {model_path}")
    print(f"   ğŸ“Š Scalers: {scaler_path}")
    print(f"   ğŸ“„ æ¨ç†é…ç½®: {inference_config_path}")

    return model_path, scaler_path, inference_config_path


# ============================================================================
# TFT model load function

def load_tft_model_from_config(config_file_path: str, device: torch.device) -> Tuple[str, str]:
    """
    Load TFT model from inference config file

    Args:
        config_file_path: æ¨ç†é…ç½®JSONFile path
        device: PyTorch device

    Returns:
        model_name: Model name
        status_msg: åŠ è½½Status message
    """
    try:
        # Read config
        with open(config_file_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        model_name = config['model_name']
        model_path = config['model_path']
        scaler_path = config['scaler_path']

        # Check if files exist
        if not os.path.exists(model_path):
            return None, f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
        if not os.path.exists(scaler_path):
            return None, f"âŒ Scaleræ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}"

        # Load model
        checkpoint = torch.load(model_path, map_location=device)
        model_config = checkpoint['model_config']

        # Rebuild TFT model
        tft_model = GroupedMultiTargetTFT(
            num_targets=model_config['num_targets'],
            num_external_factors=model_config['num_external_factors'],
            d_model=config['architecture']['d_model'],
            nhead=config['architecture']['nhead'],
            num_encoder_layers=config['architecture']['num_encoder_layers'],
            num_decoder_layers=config['architecture']['num_decoder_layers'],
            dropout=config['architecture']['dropout'],
            use_grouping=config['architecture'].get('use_grouping', False),
            signal_groups=config['architecture'].get('signal_groups', None)
        )

        tft_model.load_state_dict(checkpoint['model_state_dict'])
        tft_model.to(device)
        tft_model.eval()

        # Load scalers
        with open(scaler_path, 'rb') as f:
            scalers = pickle.load(f)

        # Save to global state
        global_state['residual_models'][model_name] = {
            'model': tft_model,
            'config': config['architecture'],
            'history': checkpoint.get('training_history', {'train_losses': [], 'val_losses': []}),
            'residual_data_key': config['data_config']['residual_data_key'],
            'info': {
                'base_model_name': config['data_config']['base_model_name'],
                'target_signals': config['signals']['target_signals'],
                'boundary_signals': config['signals']['boundary_signals'],
                'residual_signals': config['signals']['residual_signals'],
                'model_type': 'StaticSensorTransformer',  # ä»base modelç»§æ‰¿
                'future_horizon': config['data_config']['future_horizon']
            },
            'encoder_length': config['data_config']['encoder_length'],
            'future_horizon': config['data_config']['future_horizon']
        }

        global_state['residual_scalers'][model_name] = scalers

        # æ„å»ºStatus message
        status_msg = f"âœ… TFTæ¨¡å‹åŠ è½½æˆåŠŸ!\n\n"
        status_msg += f"ğŸ“Œ Model name: {model_name}\n"
        status_msg += f"ğŸ“Š åŸºç¡€æ¨¡å‹: {config['data_config']['base_model_name']}\n"
        status_msg += f"ğŸ¯ ç›®æ ‡ä¿¡å·æ•°: {config['data_config']['num_targets']}\n"
        status_msg += f"ğŸ“ˆ è¾¹ç•Œä¿¡å·æ•°: {config['data_config']['num_external_factors']}\n"
        status_msg += f"ğŸ“ å†å²çª—å£é•¿åº¦: {config['data_config']['encoder_length']}\n"
        status_msg += f"ğŸ”® æœªæ¥é¢„æµ‹é•¿åº¦: {config['data_config']['future_horizon']}\n"
        status_msg += f"âš™ï¸ æ¨¡å‹ç»´åº¦: {config['architecture']['d_model']}\n"
        status_msg += f"ğŸ•’ åˆ›å»ºæ—¶é—´: {config['created_time']}\n"

        if 'training_info' in config:
            ti = config['training_info']
            status_msg += f"\nğŸ“š Training info:\n"
            status_msg += f"   - Trainingè½®æ•°: {ti['epochs_trained']}\n"
            status_msg += f"   - æœ€ä½³ValidationæŸå¤±: {ti['best_val_loss']:.6f}\n"
            status_msg += f"   - æ‰¹å¤§å°: {ti['batch_size']}\n"
            status_msg += f"   - å­¦ä¹ ç‡: {ti['learning_rate']}\n"

        print(status_msg)
        return model_name, status_msg

    except Exception as e:
        error_msg = f"âŒ TFTæ¨¡å‹åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return None, error_msg


# ============================================================================
# Configure Chinese font

def configure_chinese_font():
    """Configure matplotlib for Chinese font display"""
    import matplotlib
    import platform

    system = platform.system()
    if system == 'Darwin':  # macOS
        matplotlib.rc('font', family='Arial Unicode MS')
    elif system == 'Windows':
        matplotlib.rc('font', family='SimHei')
    else:  # Linux
        matplotlib.rc('font', family='DejaVu Sans')

    matplotlib.rcParams['axes.unicode_minus'] = False
    sns.set_style("whitegrid")


# ============================================================================
# Import modules

try:
    import gradio as gr

    print("âœ… Gradioå¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âŒ è¯·å®‰è£…gradio: pip install gradio")
    sys.exit(1)

# å°è¯•å¯¼å…¥æœ¬åœ°æ¨¡å—
try:
    from models.static_transformer import StaticSensorTransformer
    from models.residual_tft import (
        GroupedMultiTargetTFT,
        ResidualExtractor,
        train_residual_tft,
        prepare_residual_sequence_data,
        compute_r2_safe,
        compute_residuals_correctly,
        batch_inference,
        inference_with_boosting,
        compute_per_signal_metrics,
        clear_gpu_memory,
        print_gpu_memory
    )
    from models.utils import apply_ifd_smoothing

    print("âœ… æœ¬åœ°æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ æœ¬åœ°æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("å°è¯•ä½¿ç”¨ç›¸å¯¹å¯¼å…¥...")

    try:
        from static_transformer import StaticSensorTransformer
        from residual_tft import (
            GroupedMultiTargetTFT,
            ResidualExtractor,
            train_residual_tft,
            prepare_residual_sequence_data,
            compute_r2_safe,
            compute_residuals_correctly,
            batch_inference,
            inference_with_boosting,
            compute_per_signal_metrics,
            clear_gpu_memory,
            print_gpu_memory
        )
        from utils import apply_ifd_smoothing

        print("âœ… ç›¸å¯¹å¯¼å…¥æˆåŠŸ")
    except ImportError as e2:
        print(f"âŒ ç›¸å¯¹å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}")
        print("å°†ä½¿ç”¨å†…è”å®šä¹‰...")


# Setup device with enhanced GPU detection
def setup_device():
    """Setup computing device with GPU detection and configuration"""
    configure_chinese_font()
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"GPUæ£€æµ‹æˆåŠŸ: {torch.cuda.get_device_name(0)}")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        return device
    else:
        device = torch.device('cpu')
        print("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
        return device


device = setup_device()


def load_saved_models():
    """Load saved models from file system"""
    model_dir = "saved_models"
    if not os.path.exists(model_dir):
        return

    print(f"æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ä» {model_dir}...")

    for filename in os.listdir(model_dir):
        if filename.endswith('.pth') and not filename.endswith('_scalers.pkl'):
            model_name = filename[:-4]
            model_path = os.path.join(model_dir, filename)
            scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")

            try:
                checkpoint = torch.load(model_path, map_location=device)
                model_config = checkpoint['model_config']

                if model_config['type'] == 'StaticSensorTransformer':
                    model = StaticSensorTransformer(
                        num_boundary_sensors=len(model_config['boundary_signals']),
                        num_target_sensors=len(model_config['target_signals']),
                        d_model=model_config['config']['d_model'],
                        nhead=model_config['config']['nhead'],
                        num_layers=model_config['config']['num_layers'],
                        dropout=model_config['config']['dropout']
                    )
                else:
                    continue

                model.load_state_dict(checkpoint['model_state_dict'])
                model.to(device)
                model.eval()

                scalers = {}
                if os.path.exists(scaler_path):
                    with open(scaler_path, 'rb') as f:
                        scalers = pickle.load(f)

                global_state['trained_models'][model_name] = {
                    'model': model,
                    'type': model_config['type'],
                    'boundary_signals': model_config['boundary_signals'],
                    'target_signals': model_config['target_signals'],
                    'config': model_config['config'],
                    'model_path': model_path,
                    'scaler_path': scaler_path
                }

                global_state['scalers'][model_name] = scalers
                print(f"  åŠ è½½æ¨¡å‹: {model_name}")

            except Exception as e:
                print(f"  åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {e}")
                continue

    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(global_state['trained_models'])} ä¸ªæ¨¡å‹")


# Global state management
global_state = {
    'df': None,
    'trained_models': {},
    'scalers': {},
    'residual_data': {},
    'residual_models': {},
    'residual_scalers': {},
    'final_predictions': {},
    'training_logs': {},
    'model_configs': {},
    'all_signals': [],
    # New: Stage2 Boost model storage
    'stage2_models': {},  # Stage2 residual model
    'stage2_scalers': {},  # Stage2 Scalers
    'ensemble_models': {},  # Ensemble inference model (SST + Stage2)
    'sundial_models': {},  # Sundial time series prediction model
}


# ============================================================================
# Colab Auto-load Support
# ============================================================================
def autoload_colab_data():
    """
    Automatically load pre-defined data from Colab environment

    This function checks for pre-saved CSV files and automatically loads them
    into global_state, making them immediately available in Tab1.

    Supports:
    - Standard predefined paths
    - Environment variable: COLAB_DATA_PATH
    - Wildcard matching in data/ folder
    - Google Drive mounted paths
    """
    import glob

    # Priority 1: Environment variable
    env_path = os.environ.get('COLAB_DATA_PATH')
    if env_path and os.path.exists(env_path):
        preload_paths = [env_path]
    else:
        # Priority 2: Standard predefined paths
        preload_paths = [
            'data/colab_preloaded_data.csv',
            'data/test_data.csv',
            '/content/colab_data.csv',
            # Add more common names
            'data/leap_data.csv',
            'data/sensor_data.csv',
            'data/training_data.csv',
            # Google Drive paths
            '/content/drive/MyDrive/data.csv',
            '/content/drive/MyDrive/colab_data.csv',
        ]

        # Priority 3: Wildcard search in data/ folder
        if os.path.exists('data'):
            csv_files = glob.glob('data/*.csv')
            if csv_files:
                # Add all CSV files in data/ folder
                preload_paths.extend(csv_files)

    for preload_path in preload_paths:
        if os.path.exists(preload_path):
            try:
                df_auto = pd.read_csv(preload_path)

                # Validate: must have at least 2 columns
                if df_auto.shape[1] < 2:
                    print(f"âš ï¸ [Colab Auto-load] Skipping {preload_path}: too few columns")
                    continue

                # Validate: must have at least 10 rows
                if df_auto.shape[0] < 10:
                    print(f"âš ï¸ [Colab Auto-load] Skipping {preload_path}: too few rows")
                    continue

                global_state['df'] = df_auto
                global_state['data_loaded'] = True

                print("=" * 80)
                print("âœ…âœ…âœ… [Colab Auto-load] Data successfully loaded into Tab1! âœ…âœ…âœ…")
                print(f"ğŸ“Š Data shape: {df_auto.shape}")
                print(f"ğŸ“‹ Columns: {list(df_auto.columns)[:10]}")  # Show first 10 columns
                if df_auto.shape[1] > 10:
                    print(f"    ... and {df_auto.shape[1] - 10} more columns")
                print(f"ğŸ“ Source: {preload_path}")
                print("=" * 80)

                return df_auto
            except Exception as e:
                print(f"âš ï¸ [Colab Auto-load] Failed to load {preload_path}: {e}")
                continue

    return None

# Auto-load disabled - user can manually select files in Tab1
# To enable auto-load, uncomment the line below:
# autoload_colab_data()


load_saved_models()

plt.style.use('default')
sns.set_palette("husl")

print("=" * 80)
print("Industrial Digital Twin with Residual Boost - Enhanced Interface")
print("=" * 80)
print(f"Using device: {device}")
print(f"PyTorch version: {torch.__version__}")
print("=" * 80)


# ============================================================================
# Model loading and inference config management

def save_inference_config(model_name, model_type, model_path, scaler_path,
                          boundary_signals, target_signals, config_dict):
    """
    Save inference config file - for direct model loading for inference later

    Args:
        model_name: Model name
        model_type: Model type
        model_path: Model weight file path
        scaler_path: ScalerFile path
        boundary_signals: Boundary signal list
        target_signals: Target signal list
        config_dict: Model architecture config
    """
    inference_config = {
        'model_name': model_name,
        'model_type': model_type,
        'model_path': model_path,
        'scaler_path': scaler_path,
        'boundary_signals': boundary_signals,
        'target_signals': target_signals,
        'architecture': config_dict,
        'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    model_dir = os.path.dirname(model_path)
    config_path = os.path.join(model_dir, f"{model_name}_inference.json")

    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(inference_config, f, indent=2, ensure_ascii=False)

    print(f"âœ… æ¨ç†é…ç½®å·²ä¿å­˜: {config_path}")
    return config_path


def load_model_from_inference_config(config_file_path, device):
    """
    ä»æ¨ç†é…ç½®æ–‡ä»¶Load model

    Args:
        config_file_path: æ¨ç†é…ç½®JSONFile path
        device: PyTorch device

    Returns:
        model_name: Model name
        success_msg: Success message
    """
    try:
        with open(config_file_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        model_name = config['model_name']
        model_type = config['model_type']
        model_path = config['model_path']
        scaler_path = config['scaler_path']

        # Check if files exist
        if not os.path.exists(model_path):
            return None, f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
        if not os.path.exists(scaler_path):
            return None, f"âŒ Scaleræ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}"

        # Load model
        checkpoint = torch.load(model_path, map_location=device)
        arch = config['architecture']

        if model_type == 'StaticSensorTransformer':
            model = StaticSensorTransformer(
                num_boundary_sensors=len(config['boundary_signals']),
                num_target_sensors=len(config['target_signals']),
                d_model=arch['d_model'],
                nhead=arch['nhead'],
                num_layers=arch['num_layers'],
                dropout=arch['dropout']
            )
        else:
            return None, f"âŒ ä¸æ”¯æŒçš„Model type: {model_type}"

        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()

        # Load scalers
        with open(scaler_path, 'rb') as f:
            scalers = pickle.load(f)

        # Save to global state
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': model_type,
            'boundary_signals': config['boundary_signals'],
            'target_signals': config['target_signals'],
            'config': arch,
            'model_path': model_path,
            'scaler_path': scaler_path
        }

        global_state['scalers'][model_name] = scalers

        success_msg = f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\n\n"
        success_msg += f"ğŸ“Œ Model name: {model_name}\n"
        success_msg += f"ğŸ“Š Model type: {model_type}\n"
        success_msg += f"ğŸ¯ è¾¹ç•Œä¿¡å·æ•°: {len(config['boundary_signals'])}\n"
        success_msg += f"ğŸ“ˆ ç›®æ ‡ä¿¡å·æ•°: {len(config['target_signals'])}\n"
        success_msg += f"âš™ï¸ æ¨¡å‹å‚æ•°: d_model={arch['d_model']}, nhead={arch['nhead']}, layers={arch['num_layers']}\n"
        success_msg += f"ğŸ•’ åˆ›å»ºæ—¶é—´: {config['created_time']}\n"

        print(success_msg)
        return model_name, success_msg

    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return None, error_msg


# ============================================================================
# Stage2 Boost model definition and training functions

def train_stage2_boost_model(
        residual_data_key: str,
        config: Dict[str, Any],
        progress_callback=None
) -> Tuple[str, Dict[str, Any]]:
    """
    Train Stage2 Boost residual model

    Args:
        residual_data_key: Residual data key
        config: Training config
        progress_callback: Progress callback function

    Returns:
        status_msg: Training status message
        results: Training result dictionary
    """
    try:
        if residual_data_key not in global_state['residual_data']:
            return "âŒ æ®‹å·®æ•°æ®ä¸å­˜åœ¨ï¼", {}

        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸš€ å¼€å§‹è®­ç»ƒ Stage2 Boost æ®‹å·®æ¨¡å‹")
        log_msg.append("=" * 80)

        # Get residual data
        residuals_df = global_state['residual_data'][residual_data_key]['data']
        residual_info = global_state['residual_data'][residual_data_key]['info']

        boundary_signals = residual_info['boundary_signals']
        target_signals = residual_info['target_signals']
        residual_signals = residual_info['residual_signals']

        log_msg.append(f"\nğŸ“Š æ•°æ®ä¿¡æ¯:")
        log_msg.append(f"  æ®‹å·®æ•°æ®: {residual_data_key}")
        log_msg.append(f"  è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}")
        log_msg.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")
        log_msg.append(f"  æ•°æ®é•¿åº¦: {len(residuals_df)}")

        # Prepare training data
        X = residuals_df[boundary_signals].values
        y_residual = residuals_df[residual_signals].values

        # Data split
        train_size = int(len(X) * (1 - config['test_size'] - config['val_size']))
        val_size = int(len(X) * config['val_size'])

        X_train = X[:train_size]
        X_val = X[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]

        y_train = y_residual[:train_size]
        y_val = y_residual[train_size:train_size + val_size]
        y_test = y_residual[train_size + val_size:]

        log_msg.append(f"\nğŸ”€ æ•°æ®åˆ†å‰²:")
        log_msg.append(f"  è®­ç»ƒé›†: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
        log_msg.append(f"  éªŒè¯é›†: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
        log_msg.append(f"  æµ‹è¯•é›†: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

        # Data standardization
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()

        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)

        y_train_scaled = scaler_y.fit_transform(y_train)
        y_val_scaled = scaler_y.transform(y_val)
        y_test_scaled = scaler_y.transform(y_test)

        # Create DataLoader
        train_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_train_scaled),
            torch.FloatTensor(y_train_scaled)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val_scaled),
            torch.FloatTensor(y_val_scaled)
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config['batch_size'],
            shuffle=True
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            shuffle=False
        )

        # Initialize Stage2 model (using SST architecture)
        log_msg.append(f"\nğŸ—ï¸ åˆå§‹åŒ–Stage2æ®‹å·®æ¨¡å‹:")
        log_msg.append(f"  æ¶æ„: StaticSensorTransformer")
        log_msg.append(f"  d_model: {config['d_model']}")
        log_msg.append(f"  nhead: {config['nhead']}")
        log_msg.append(f"  num_layers: {config['num_layers']}")

        stage2_model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            dropout=config['dropout']
        ).to(device)

        # Optimizer and scheduler
        optimizer = torch.optim.AdamW(
            stage2_model.parameters(),
            lr=config['lr'],
            weight_decay=config.get('weight_decay', 1e-5)
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=config.get('scheduler_patience', 10),
            verbose=True
        )

        criterion = nn.MSELoss()

        # Mixed precision training
        scaler = GradScaler()

        # Training loop
        log_msg.append(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (æ··åˆç²¾åº¦, æ€»è½®æ•°: {config['epochs']})")

        history = {
            'train_losses': [],
            'val_losses': [],
            'train_r2': [],
            'val_r2': []
        }

        best_val_loss = float('inf')
        patience_counter = 0
        early_stop_patience = config.get('early_stop_patience', 25)

        for epoch in range(config['epochs']):
            # Training phase with mixed precision
            stage2_model.train()
            train_loss = 0.0
            train_preds = []
            train_targets = []

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)

                optimizer.zero_grad()

                # Mixed precision forward pass
                with autocast():
                    outputs = stage2_model(batch_X)
                    loss = criterion(outputs, batch_y)

                # Mixed precision backward pass
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)

                # Gradient clipping
                if config.get('grad_clip', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(stage2_model.parameters(), config['grad_clip'])

                scaler.step(optimizer)
                scaler.update()

                train_loss += loss.item()
                train_preds.append(outputs.detach().cpu().numpy())
                train_targets.append(batch_y.detach().cpu().numpy())

            train_loss /= len(train_loader)
            train_preds = np.vstack(train_preds)
            train_targets = np.vstack(train_targets)
            train_r2, _ = compute_r2_safe(train_targets, train_preds, method='per_output_mean')

            # Validation phase with mixed precision
            stage2_model.eval()
            val_loss = 0.0
            val_preds = []
            val_targets = []

            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    with autocast():
                        outputs = stage2_model(batch_X)
                        loss = criterion(outputs, batch_y)

                    val_loss += loss.item()
                    val_preds.append(outputs.cpu().numpy())
                    val_targets.append(batch_y.cpu().numpy())

            val_loss /= len(val_loader)
            val_preds = np.vstack(val_preds)
            val_targets = np.vstack(val_targets)
            val_r2, _ = compute_r2_safe(val_targets, val_preds, method='per_output_mean')

            # Record history
            history['train_losses'].append(train_loss)
            history['val_losses'].append(val_loss)
            history['train_r2'].append(train_r2)
            history['val_r2'].append(val_r2)

            # Learning rate scheduling
            scheduler.step(val_loss)

            # Early stopping check
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0

                # Save best model
                best_model_state = stage2_model.state_dict().copy()
            else:
                patience_counter += 1

            # Progress output
            if (epoch + 1) % max(1, config['epochs'] // 20) == 0 or epoch == 0 or epoch == config['epochs'] - 1:
                msg = f"Epoch {epoch + 1}/{config['epochs']} - "
                msg += f"Train Loss: {train_loss:.6f}, Train RÂ²: {train_r2:.4f} | "
                msg += f"Val Loss: {val_loss:.6f}, Val RÂ²: {val_r2:.4f}"
                log_msg.append(msg)

                if progress_callback:
                    progress_callback("\n".join(log_msg))

            # Early stopping
            if patience_counter >= early_stop_patience:
                log_msg.append(f"\nâ¸ï¸ æ—©åœè§¦å‘ (Epoch {epoch + 1})")
                break

        # Load best model
        stage2_model.load_state_dict(best_model_state)

        # Test set evaluation with batch inference
        y_test_pred = batch_inference(
            stage2_model, X_test, scaler_X, scaler_y, device,
            batch_size=config['batch_size'], model_name="Stage2"
        )

        test_mae = mean_absolute_error(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        test_r2, _ = compute_r2_safe(y_test, y_test_pred, method='per_output_mean')

        log_msg.append(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        log_msg.append(f"  MAE: {test_mae:.6f}")
        log_msg.append(f"  RMSE: {test_rmse:.6f}")
        log_msg.append(f"  RÂ²: {test_r2:.4f}")

        # Save model
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f"Stage2_Boost_{residual_data_key}_{timestamp}"

        model_dir = "saved_models/stage2_boost"
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"{model_name}.pth")
        torch.save({
            'model_state_dict': stage2_model.state_dict(),
            'config': config,
            'history': history,
            'residual_data_key': residual_data_key,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_signals,
            'test_metrics': {
                'mae': test_mae,
                'rmse': test_rmse,
                'r2': test_r2
            }
        }, model_path)

        # Save scalers
        scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
        with open(scaler_path, 'wb') as f:
            pickle.dump({'X': scaler_X, 'y': scaler_y}, f)

        # Save to global state
        global_state['stage2_models'][model_name] = {
            'model': stage2_model,
            'config': config,
            'history': history,
            'residual_data_key': residual_data_key,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_signals,
            'model_path': model_path,
            'scaler_path': scaler_path,
            'test_metrics': {
                'mae': test_mae,
                'rmse': test_rmse,
                'r2': test_r2
            }
        }

        global_state['stage2_scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}

        log_msg.append(f"\nâœ… Stage2æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜:")
        log_msg.append(f"  æ¨¡å‹åç§°: {model_name}")
        log_msg.append(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        log_msg.append(f"  Scalerè·¯å¾„: {scaler_path}")

        results = {
            'model_name': model_name,
            'history': history,
            'test_metrics': {
                'mae': test_mae,
                'rmse': test_rmse,
                'r2': test_r2
            }
        }

        return "\n".join(log_msg), results

    except Exception as e:
        error_msg = f"âŒ Stage2æ¨¡å‹è®­ç»ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, {}


def compute_signal_r2_and_select_threshold(
        base_model_name: str,
        stage2_model_name: str,
        r2_threshold: float = 0.4
) -> Tuple[str, Dict[str, Any]]:
    """
    Compute RÂ² score for each signal, intelligently select RÂ² threshold, generate ensemble inference model

    Args:
        base_model_name: åŸºç¡€SSTModel name
        stage2_model_name: Stage2æ®‹å·®Model name
        r2_threshold: RÂ² threshold (apply Stage2 only to signals with RÂ² < threshold)

    Returns:
        status_msg: Status message
        ensemble_info: Ensemble model info
    """
    try:
        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸ¯ è®¡ç®—ä¿¡å·RÂ²åˆ†æ•°å¹¶ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹")
        log_msg.append("=" * 80)

        # Check if models exist
        if base_model_name not in global_state['trained_models']:
            return f"âŒ åŸºç¡€æ¨¡å‹ {base_model_name} ä¸å­˜åœ¨ï¼", {}

        if stage2_model_name not in global_state['stage2_models']:
            return f"âŒ Stage2æ¨¡å‹ {stage2_model_name} ä¸å­˜åœ¨ï¼", {}

        # Get models
        base_model_info = global_state['trained_models'][base_model_name]
        stage2_model_info = global_state['stage2_models'][stage2_model_name]

        base_model = base_model_info['model']
        stage2_model = stage2_model_info['model']

        # Get residual data
        residual_data_key = stage2_model_info['residual_data_key']
        if residual_data_key not in global_state['residual_data']:
            return f"âŒ æ®‹å·®æ•°æ® {residual_data_key} ä¸å­˜åœ¨ï¼", {}

        residuals_df = global_state['residual_data'][residual_data_key]['data']
        residual_info = global_state['residual_data'][residual_data_key]['info']

        boundary_signals = residual_info['boundary_signals']
        target_signals = residual_info['target_signals']

        log_msg.append(f"\nğŸ“Š æ•°æ®ä¿¡æ¯:")
        log_msg.append(f"  åŸºç¡€æ¨¡å‹: {base_model_name}")
        log_msg.append(f"  Stage2æ¨¡å‹: {stage2_model_name}")
        log_msg.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")

        # Get true values and predictions
        y_true_cols = [f"{sig}_true" for sig in target_signals]
        y_pred_cols = [f"{sig}_pred" for sig in target_signals]

        y_true = residuals_df[y_true_cols].values
        y_pred_base = residuals_df[y_pred_cols].values

        # Stage2 residual prediction using batch inference
        X = residuals_df[boundary_signals].values
        y_residual_pred = batch_inference(
            stage2_model,
            X,
            global_state['stage2_scalers'][stage2_model_name]['X'],
            global_state['stage2_scalers'][stage2_model_name]['y'],
            device,
            batch_size=512,
            model_name="Stage2"
        )

        # Compute RÂ² for each signal using safe computation
        signal_r2_scores = []
        _, per_signal_r2 = compute_r2_safe(y_true, y_pred_base, method='per_output_mean')

        for i, signal in enumerate(target_signals):
            r2 = per_signal_r2[i]
            signal_r2_scores.append({
                'signal': signal,
                'r2': r2,
                'apply_stage2': r2 < r2_threshold
            })

        log_msg.append(f"\nğŸ¯ ä¿¡å·RÂ²åˆ†æ (é˜ˆå€¼: {r2_threshold}):")
        log_msg.append(f"{'ä¿¡å·åç§°':<30} {'RÂ²åˆ†æ•°':>10} {'åº”ç”¨Stage2':>12}")
        log_msg.append("-" * 55)

        num_signals_use_stage2 = 0
        for item in signal_r2_scores:
            log_msg.append(
                f"{item['signal']:<30} {item['r2']:>10.4f} {'âœ“' if item['apply_stage2'] else 'âœ—':>12}"
            )
            if item['apply_stage2']:
                num_signals_use_stage2 += 1

        log_msg.append("-" * 55)
        log_msg.append(f"éœ€è¦Stage2çš„ä¿¡å·æ•°: {num_signals_use_stage2} / {len(target_signals)}")

        # Generate ensemble prediction (selectively apply Stage2)
        y_ensemble = y_pred_base.copy()
        for i, item in enumerate(signal_r2_scores):
            if item['apply_stage2']:
                # Apply Stage2 correction
                y_ensemble[:, i] = y_pred_base[:, i] + y_residual_pred[:, i]

        # Compute ensemble model performance using safe RÂ² computation
        mae_base = mean_absolute_error(y_true, y_pred_base)
        mae_ensemble = mean_absolute_error(y_true, y_ensemble)
        rmse_base = np.sqrt(mean_squared_error(y_true, y_pred_base))
        rmse_ensemble = np.sqrt(mean_squared_error(y_true, y_ensemble))
        r2_base, _ = compute_r2_safe(y_true, y_pred_base, method='per_output_mean')
        r2_ensemble, _ = compute_r2_safe(y_true, y_ensemble, method='per_output_mean')

        improvement_mae = (mae_base - mae_ensemble) / mae_base * 100
        improvement_rmse = (rmse_base - rmse_ensemble) / rmse_base * 100
        improvement_r2 = (r2_ensemble - r2_base) / (1 - r2_base) * 100

        log_msg.append(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        log_msg.append(f"{'æŒ‡æ ‡':<15} {'åŸºç¡€æ¨¡å‹(SST)':>18} {'ç»¼åˆæ¨¡å‹':>18} {'æ”¹è¿›':>15}")
        log_msg.append("-" * 70)
        log_msg.append(f"{'MAE':<15} {mae_base:>18.6f} {mae_ensemble:>18.6f} {improvement_mae:>14.2f}%")
        log_msg.append(f"{'RMSE':<15} {rmse_base:>18.6f} {rmse_ensemble:>18.6f} {improvement_rmse:>14.2f}%")
        log_msg.append(f"{'RÂ²':<15} {r2_base:>18.4f} {r2_ensemble:>18.4f} {improvement_r2:>14.2f}%")

        # ä¿å­˜Ensemble model info
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        ensemble_name = f"Ensemble_{base_model_name}_{stage2_model_name}_{timestamp}"

        ensemble_info = {
            'name': ensemble_name,
            'base_model_name': base_model_name,
            'stage2_model_name': stage2_model_name,
            'r2_threshold': r2_threshold,
            'signal_r2_scores': signal_r2_scores,
            'num_signals_use_stage2': num_signals_use_stage2,
            'metrics': {
                'base': {'mae': mae_base, 'rmse': rmse_base, 'r2': r2_base},
                'ensemble': {'mae': mae_ensemble, 'rmse': rmse_ensemble, 'r2': r2_ensemble},
                'improvement': {
                    'mae_pct': improvement_mae,
                    'rmse_pct': improvement_rmse,
                    'r2_pct': improvement_r2
                }
            },
            'predictions': {
                'y_true': y_true,
                'y_pred_base': y_pred_base,
                'y_pred_ensemble': y_ensemble,
                'y_residual_pred': y_residual_pred
            },
            'signals': {
                'boundary': boundary_signals,
                'target': target_signals
            },
            'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        global_state['ensemble_models'][ensemble_name] = ensemble_info

        # Save ensemble model config
        ensemble_dir = "saved_models/ensemble"
        os.makedirs(ensemble_dir, exist_ok=True)

        config_path = os.path.join(ensemble_dir, f"{ensemble_name}_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            # Save config (excluding large arrays)
            save_config = {
                'name': ensemble_name,
                'base_model_name': base_model_name,
                'stage2_model_name': stage2_model_name,
                'r2_threshold': r2_threshold,
                'signal_r2_scores': signal_r2_scores,
                'num_signals_use_stage2': num_signals_use_stage2,
                'metrics': ensemble_info['metrics'],
                'signals': ensemble_info['signals'],
                'created_time': ensemble_info['created_time']
            }
            json.dump(save_config, f, indent=2, ensure_ascii=False)

        log_msg.append(f"\nâœ… ç»¼åˆæ¨ç†æ¨¡å‹å·²ç”Ÿæˆ:")
        log_msg.append(f"  æ¨¡å‹åç§°: {ensemble_name}")
        log_msg.append(f"  é…ç½®è·¯å¾„: {config_path}")

        return "\n".join(log_msg), ensemble_info

    except Exception as e:
        error_msg = f"âŒ ç»¼åˆæ¨¡å‹ç”Ÿæˆå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, {}


# ============================================================================
# Data loading functions

def load_data_from_csv(file_obj):
    """Load data from CSV file"""
    try:
        df = pd.read_csv(file_obj.name)

        # If there are unnamed columns, set as index
        if 'Unnamed: 0' in df.columns:
            df = df.set_index('Unnamed: 0')
            df.index.name = 'index'
        elif df.index.name is None:
            df.index.name = 'index'

        global_state['df'] = df
        global_state['all_signals'] = list(df.columns)

        status = f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!\n\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ ç‰¹å¾æ•°: {len(df.columns)}\n\n"
        status += f"å‰5åˆ—: {', '.join(df.columns[:5].tolist())}"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display

    except Exception as e:
        error_msg = f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"
        return error_msg, None, ""


def get_available_csv_files():
    """
    Get list of available CSV files in data/ folder

    Returns:
        List of CSV file paths (safe - never raises exceptions)
    """
    try:
        import glob

        csv_files = []

        # Search in data/ folder
        if os.path.exists('data'):
            csv_files.extend(glob.glob('data/*.csv'))

        # Search in current directory
        csv_files.extend(glob.glob('*.csv'))

        # Sort by modification time (newest first)
        csv_files = sorted(csv_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return csv_files if csv_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_available_csv_files: {e}")
        return []  # Return empty list on error


def load_csv_from_path(csv_path):
    """
    Load CSV file from a given path

    Args:
        csv_path: Path to CSV file

    Returns:
        status: Status message
        preview_df: Data preview (first 100 rows)
        signals: Available signals
    """
    if not csv_path or csv_path == "(no CSV files found)":
        return "âŒ è¯·é€‰æ‹©æœ‰æ•ˆçš„CSVæ–‡ä»¶", None, ""

    if not os.path.exists(csv_path):
        return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}", None, ""

    try:
        df = pd.read_csv(csv_path)

        # If there are unnamed columns, set as index
        if 'Unnamed: 0' in df.columns:
            df = df.set_index('Unnamed: 0')
            df.index.name = 'index'
        elif df.index.name is None:
            df.index.name = 'index'

        global_state['df'] = df
        global_state['all_signals'] = list(df.columns)

        status = f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!\n\n"
        status += f"ğŸ“ æ–‡ä»¶: {csv_path}\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ ç‰¹å¾æ•°: {len(df.columns)}\n\n"
        status += f"å‰5åˆ—: {', '.join(df.columns[:5].tolist())}"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display

    except Exception as e:
        error_msg = f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"
        return error_msg, None, ""


def check_preloaded_data():
    """
    Check if data was pre-loaded (from Colab) and return its status

    Returns:
        status: Status message
        preview_df: Data preview (first 100 rows)
        signals_display: Available signals
    """
    if global_state.get('df') is not None:
        df = global_state['df']

        status = f"âœ… [é¢„åŠ è½½] æ•°æ®å·²åŠ è½½!\n\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ ç‰¹å¾æ•°: {len(df.columns)}\n\n"
        status += f"åˆ—å: {', '.join(df.columns[:5].tolist())}"
        if len(df.columns) > 5:
            status += f"... (å…±{len(df.columns)}åˆ—)"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display
    else:
        return "âš ï¸ å°šæœªåŠ è½½æ•°æ®ï¼Œè¯·é€‰æ‹©CSVæ–‡ä»¶ã€ä¸Šä¼ æ–‡ä»¶æˆ–åˆ›å»ºç¤ºä¾‹æ•°æ®", None, ""


def create_sample_data():
    """Create sample data"""
    try:
        np.random.seed(42)
        n_samples = 10000
        n_boundary = 10
        n_target = 5

        # Generate correlated sensor data
        X = np.random.randn(n_samples, n_boundary)
        y = X[:, :n_target] + 0.5 * X[:, :n_target] ** 2 + 0.1 * np.random.randn(n_samples, n_target)

        boundary_cols = [f"boundary_{i + 1}" for i in range(n_boundary)]
        target_cols = [f"target_{i + 1}" for i in range(n_target)]

        df = pd.DataFrame(
            np.column_stack([X, y]),
            columns=boundary_cols + target_cols
        )

        df.index.name = 'index'
        global_state['df'] = df
        global_state['all_signals'] = list(df.columns)

        status = f"âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºæˆåŠŸ!\n\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ è¾¹ç•Œä¿¡å·: {n_boundary}ä¸ª\n"
        status += f"ğŸ¯ ç›®æ ‡ä¿¡å·: {n_target}ä¸ª\n\n"
        status += "ğŸ’¡ æç¤º: ç¤ºä¾‹æ•°æ®æ¨¡æ‹Ÿäº†ä¼ æ„Ÿå™¨ä¹‹é—´çš„éçº¿æ€§å…³ç³»"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display

    except Exception as e:
        error_msg = f"âŒ ç¤ºä¾‹æ•°æ®åˆ›å»ºå¤±è´¥: {str(e)}"
        return error_msg, None, ""


# ============================================================================
# SST model training functions

def train_base_model_ui(
        boundary_signals, target_signals, model_type,
        epochs, batch_size, lr,
        d_model, nhead, num_layers, dropout,
        test_size, val_size,
        temporal_signals=None, apply_smoothing=False,
        progress=gr.Progress()
):
    """UI function for training base model"""
    try:
        if global_state['df'] is None:
            return "âŒ è¯·å…ˆåŠ è½½æ•°æ®ï¼"

        if not boundary_signals or not target_signals:
            return "âŒ è¯·é€‰æ‹©è¾¹ç•Œä¿¡å·å’Œç›®æ ‡ä¿¡å·ï¼"

        log_messages = []
        log_messages.append("=" * 80)
        log_messages.append(f"ğŸš€ å¼€å§‹è®­ç»ƒ {model_type}")
        log_messages.append("=" * 80)
        log_messages.append(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
        log_messages.append(f"  æ¨¡å‹ç±»å‹: {model_type}")
        log_messages.append(f"  è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}")
        log_messages.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")
        log_messages.append(f"  è®­ç»ƒè½®æ•°: {epochs}")
        log_messages.append(f"  æ‰¹å¤§å°: {batch_size}")
        log_messages.append(f"  å­¦ä¹ ç‡: {lr}")

        df = global_state['df']

        # Prepare data
        X = df[boundary_signals].values
        y = df[target_signals].values

        # Apply IFD smoothing (if needed)
        if apply_smoothing and temporal_signals:
            log_messages.append(f"\nğŸ”§ åº”ç”¨IFDå¹³æ»‘...")
            # Apply smoothing to the full y array for specified temporal signals
            y_smoothed = apply_ifd_smoothing(
                y_data=y,
                target_sensors=target_signals,
                ifd_sensor_names=temporal_signals,
                window_length=15,
                polyorder=3
            )
            # Update y with smoothed values
            y = y_smoothed
            # Update df with smoothed target signals
            for i, sig in enumerate(target_signals):
                df[sig] = y[:, i]
            log_messages.append(f"  å·²å¯¹ {len(temporal_signals)} ä¸ªæ—¶åºä¿¡å·åº”ç”¨å¹³æ»‘")

        # Data split
        train_size = int(len(X) * (1 - test_size - val_size))
        val_size_samples = int(len(X) * val_size)

        X_train = X[:train_size]
        X_val = X[train_size:train_size + val_size_samples]
        X_test = X[train_size + val_size_samples:]

        y_train = y[:train_size]
        y_val = y[train_size:train_size + val_size_samples]
        y_test = y[train_size + val_size_samples:]

        log_messages.append(f"\nğŸ”€ æ•°æ®åˆ†å‰²:")
        log_messages.append(f"  è®­ç»ƒé›†: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
        log_messages.append(f"  éªŒè¯é›†: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
        log_messages.append(f"  æµ‹è¯•é›†: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

        # Data standardization
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()

        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)

        y_train_scaled = scaler_y.fit_transform(y_train)
        y_val_scaled = scaler_y.transform(y_val)
        y_test_scaled = scaler_y.transform(y_test)

        # Create DataLoader
        train_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_train_scaled),
            torch.FloatTensor(y_train_scaled)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val_scaled),
            torch.FloatTensor(y_val_scaled)
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False
        )

        # Initialize model
        log_messages.append(f"\nğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹: {model_type}")

        if model_type == 'StaticSensorTransformer':
            model = StaticSensorTransformer(
                num_boundary_sensors=len(boundary_signals),
                num_target_sensors=len(target_signals),
                d_model=d_model,
                nhead=nhead,
                num_layers=num_layers,
                dropout=dropout
            ).to(device)
        else:
            return f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"

        # Optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10, verbose=True
        )
        criterion = nn.MSELoss()

        # Mixed precision training
        scaler = GradScaler()
        grad_clip_norm = 1.0

        # Training loop
        log_messages.append(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (æ··åˆç²¾åº¦)...")
        history = {'train_losses': [], 'val_losses': []}
        best_val_loss = float('inf')
        patience_counter = 0
        early_stop_patience = 25

        for epoch in range(epochs):
            # Training with mixed precision
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                optimizer.zero_grad()

                # Mixed precision forward pass
                with autocast():
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)

                # Mixed precision backward pass
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                scaler.step(optimizer)
                scaler.update()

                train_loss += loss.item()

            train_loss /= len(train_loader)

            # Validation with mixed precision
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    with autocast():
                        outputs = model(batch_X)
                        loss = criterion(outputs, batch_y)
                    val_loss += loss.item()

            val_loss /= len(val_loader)

            history['train_losses'].append(train_loss)
            history['val_losses'].append(val_loss)

            scheduler.step(val_loss)

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1

            # è¿›åº¦æ˜¾ç¤º
            if (epoch + 1) % max(1, epochs // 20) == 0 or epoch == 0:
                msg = f"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}"
                log_messages.append(msg)
                progress((epoch + 1) / epochs, desc=msg)

            if patience_counter >= early_stop_patience:
                log_messages.append(f"\nâ¸ï¸ æ—©åœè§¦å‘ (Epoch {epoch + 1})")
                break

        # Load best model
        model.load_state_dict(best_model_state)

        # Test set evaluation with mixed precision and batch inference
        y_test_pred = batch_inference(
            model, X_test, scaler_X, scaler_y, device,
            batch_size=batch_size, model_name=model_type
        )

        test_mae = mean_absolute_error(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        test_r2, _ = compute_r2_safe(y_test, y_test_pred, method='per_output_mean')

        log_messages.append(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        log_messages.append(f"  MAE: {test_mae:.6f}")
        log_messages.append(f"  RMSE: {test_rmse:.6f}")
        log_messages.append(f"  RÂ²: {test_r2:.4f}")

        # Save model
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f"{model_type}_{timestamp}"

        model_dir = "saved_models"
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"{model_name}.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': {
                'type': model_type,
                'boundary_signals': boundary_signals,
                'target_signals': target_signals,
                'config': {
                    'd_model': d_model,
                    'nhead': nhead,
                    'num_layers': num_layers,
                    'dropout': dropout,
                    'batch_size': batch_size
                }
            },
            'training_history': history
        }, model_path)

        # Save scalers
        scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
        with open(scaler_path, 'wb') as f:
            pickle.dump({'X': scaler_X, 'y': scaler_y}, f)

        # ä¿å­˜æ¨ç†é…ç½®
        save_inference_config(
            model_name, model_type, model_path, scaler_path,
            boundary_signals, target_signals,
            {
                'd_model': d_model,
                'nhead': nhead,
                'num_layers': num_layers,
                'dropout': dropout
            }
        )

        # Save to global state
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': model_type,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'config': {
                'd_model': d_model,
                'nhead': nhead,
                'num_layers': num_layers,
                'dropout': dropout,
                'batch_size': batch_size
            },
            'model_path': model_path,
            'scaler_path': scaler_path
        }

        global_state['scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}

        log_messages.append(f"\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜:")
        log_messages.append(f"  æ¨¡å‹åç§°: {model_name}")
        log_messages.append(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        log_messages.append(f"  Scalerè·¯å¾„: {scaler_path}")

        return "\n".join(log_messages)

    except Exception as e:
        error_msg = f"âŒ è®­ç»ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg


# ============================================================================
# Residual extraction functions

def extract_residuals_ui(model_name, future_horizon, use_segment, start_index, end_index):
    """UI function for residual extraction"""
    try:
        if not model_name:
            return "âŒ è¯·é€‰æ‹©æ¨¡å‹ï¼", None

        if global_state['df'] is None:
            return "âŒ è¯·å…ˆåŠ è½½æ•°æ®ï¼", None

        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸ“Š å¼€å§‹æå–æ®‹å·®")
        log_msg.append("=" * 80)

        df = global_state['df']

        # Segment selection
        if use_segment:
            start_idx = max(0, int(start_index))
            end_idx = min(len(df), int(end_index))
            df_segment = df.iloc[start_idx:end_idx].copy()
            log_msg.append(f"\nâœ‚ï¸ ä½¿ç”¨æ•°æ®ç‰‡æ®µ: index [{start_idx}, {end_idx})")
            log_msg.append(f"   ç‰‡æ®µé•¿åº¦: {len(df_segment):,}")
        else:
            df_segment = df.copy()
            log_msg.append(f"\nğŸ“ˆ ä½¿ç”¨å…¨éƒ¨æ•°æ®: {len(df_segment):,} æ¡")

        # Use ResidualExtractor to extract residuals
        from residual_tft import ResidualExtractor

        residuals_df, info = ResidualExtractor.extract_residuals_from_trained_models(
            model_name, df_segment, global_state, device
        )

        if residuals_df.empty:
            return "âŒ æ®‹å·®æå–å¤±è´¥ï¼", None

        # Save residual data
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        residual_key = f"{model_name}_{timestamp}_h{future_horizon}"

        info['future_horizon'] = future_horizon
        info['base_model_name'] = model_name

        global_state['residual_data'][residual_key] = {
            'data': residuals_df,
            'info': info
        }

        log_msg.append(f"\nâœ… æ®‹å·®æå–å®Œæˆ:")
        log_msg.append(f"  æ®‹å·®æ•°æ®æ ‡è¯†: {residual_key}")
        log_msg.append(f"  æ•°æ®å½¢çŠ¶: {residuals_df.shape}")

        # Create residual visualization
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'æ®‹å·®åˆ†æ - {model_name}', fontsize=16)

        # æ®‹å·®åˆ†å¸ƒ
        residual_cols = info['residual_signals']
        all_residuals = residuals_df[residual_cols].values.flatten()

        axes[0, 0].hist(all_residuals, bins=50, edgecolor='black', alpha=0.7)
        axes[0, 0].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('æ®‹å·®å€¼')
        axes[0, 0].set_ylabel('é¢‘æ•°')

        # æ®‹å·®åºåˆ—
        axes[0, 1].plot(residuals_df[residual_cols[0]].values[:1000])
        axes[0, 1].set_title(f'æ®‹å·®åºåˆ— ({residual_cols[0]})')
        axes[0, 1].set_xlabel('Index')
        axes[0, 1].set_ylabel('æ®‹å·®')

        # æ®‹å·®ç»Ÿè®¡
        residual_stats = residuals_df[residual_cols].describe()
        axes[1, 0].axis('off')
        stats_text = "æ®‹å·®ç»Ÿè®¡:\n"
        stats_text += f"Mean: {residual_stats.loc['mean'].mean():.6f}\n"
        stats_text += f"Std: {residual_stats.loc['std'].mean():.6f}\n"
        stats_text += f"Min: {residual_stats.loc['min'].min():.6f}\n"
        stats_text += f"Max: {residual_stats.loc['max'].max():.6f}\n"
        axes[1, 0].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')

        # é¢„æµ‹vsçœŸå®
        true_cols = [f"{sig}_true" for sig in info['target_signals']]
        pred_cols = [f"{sig}_pred" for sig in info['target_signals']]

        y_true = residuals_df[true_cols].values[:1000, 0]
        y_pred = residuals_df[pred_cols].values[:1000, 0]

        axes[1, 1].plot(y_true, label='True', alpha=0.7)
        axes[1, 1].plot(y_pred, label='Predicted', alpha=0.7)
        axes[1, 1].set_title('é¢„æµ‹ vs çœŸå® (å‰1000ä¸ªæ ·æœ¬)')
        axes[1, 1].legend()

        plt.tight_layout()

        return "\n".join(log_msg), fig

    except Exception as e:
        error_msg = f"âŒ æ®‹å·®æå–å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, None


# ============================================================================
# Helper functions

def get_available_models():
    """Get list of available trained models"""
    return list(global_state['trained_models'].keys())


def get_residual_data_keys():
    """Get list of available residual data keys"""
    return list(global_state['residual_data'].keys())


def get_stage2_model_keys():
    """Get list of available Stage2 models"""
    return list(global_state['stage2_models'].keys())


def get_ensemble_model_keys():
    """Get list of available ensemble models"""
    return list(global_state['ensemble_models'].keys())


# ============================================================================
# Gradio interface creation

def create_unified_interface():
    """Create unified Gradio interface"""

    with gr.Blocks(title="å·¥ä¸šæ•°å­—å­ªç”Ÿæ®‹å·®Boostè®­ç»ƒç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ­ å·¥ä¸šæ•°å­—å­ªç”Ÿæ®‹å·®BoostTrainingç³»ç»Ÿ
        ### Enhanced Residual Boost Training with Stage2 Model

        **æ–°åŠŸèƒ½:**
        - âœ¨ Stage2 residual modelTraining
        - ğŸ¯ æ™ºèƒ½RÂ²é˜ˆå€¼é€‰æ‹©ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹
        - ğŸ“Š Secondary inference comparison (Ensemble model vs Pure SST model)
        - ğŸ”® Sundial time series model predicting future residuals
        """)

        with gr.Tabs():
            # Tab 1: æ•°æ®åŠ è½½
            with gr.Tab("ğŸ“‚ æ•°æ®åŠ è½½", elem_id="data_loading"):
                gr.Markdown("## é€‰æ‹©ã€ä¸Šä¼ æˆ–åˆ›å»ºæ•°æ®")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ é€‰æ‹©å·²æœ‰CSVæ–‡ä»¶")
                        csv_file_selector = gr.Dropdown(
                            choices=[],  # Empty initially, populated on page load
                            label="é€‰æ‹©dataæ–‡ä»¶å¤¹ä¸‹çš„CSVæ–‡ä»¶",
                            info="ç‚¹å‡»'åˆ·æ–°åˆ—è¡¨'æ¥åŠ è½½å¯ç”¨çš„CSVæ–‡ä»¶"
                        )
                        with gr.Row():
                            select_csv_btn = gr.Button("ğŸ“‚ åŠ è½½é€‰ä¸­æ–‡ä»¶", variant="primary", size="lg")
                            refresh_csv_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm")

                        gr.Markdown("### ğŸ“¤ æˆ–ä¸Šä¼ CSVæ–‡ä»¶")
                        data_file = gr.File(label="ä¸Šä¼ CSVæ–‡ä»¶", file_types=['.csv'])
                        upload_btn = gr.Button("ğŸ“¥ åŠ è½½ä¸Šä¼ æ–‡ä»¶", variant="secondary", size="lg")

                        gr.Markdown("### ğŸ² æˆ–åˆ›å»ºç¤ºä¾‹æ•°æ®")
                        sample_btn = gr.Button("ğŸ² åˆ›å»ºç¤ºä¾‹æ•°æ®", size="lg")

                    with gr.Column(scale=1):
                        data_status = gr.Textbox(label="æ•°æ®çŠ¶æ€", lines=10, interactive=False)
                        signals_display = gr.Textbox(label="å¯ç”¨ä¿¡å·", lines=10, interactive=False)

                # Data preview table
                with gr.Row():
                    data_preview = gr.Dataframe(
                        label="ğŸ“Š æ•°æ®é¢„è§ˆ (å‰100è¡Œ)",
                        interactive=False,
                        wrap=True
                    )

            # Tab 2: SSTæ¨¡å‹Training
            with gr.Tab("ğŸ¯ SSTæ¨¡å‹Training", elem_id="sst_training"):
                gr.Markdown("## è®­ç»ƒé™æ€ä¼ æ„Ÿå™¨æ˜ å°„Transformer (SST)")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ›ï¸ ä¿¡å·é€‰æ‹©")
                        boundary_signals_static = gr.Dropdown(
                            choices=[], label="è¾¹ç•Œä¿¡å· (è¾“å…¥)", multiselect=True
                        )
                        target_signals_static = gr.Dropdown(
                            choices=[], label="ç›®æ ‡ä¿¡å· (è¾“å‡º)", multiselect=True
                        )

                        gr.Markdown("### ğŸ—ï¸ æ¨¡å‹æ¶æ„")
                        with gr.Row():
                            d_model_static = gr.Slider(32, 512, 128, 32, label="æ¨¡å‹ç»´åº¦")
                            nhead_static = gr.Slider(2, 16, 8, 2, label="æ³¨æ„åŠ›å¤´æ•°")
                        num_layers_static = gr.Slider(1, 8, 3, 1, label="Transformerå±‚æ•°")

                        gr.Markdown("### ğŸ¯ è®­ç»ƒå‚æ•°")
                        with gr.Row():
                            epochs_static = gr.Slider(10, 500, 100, 10, label="è®­ç»ƒè½®æ•°")
                            batch_size_static = gr.Slider(16, 256, 64, 16, label="æ‰¹å¤§å°")
                        with gr.Row():
                            lr_static = gr.Number(value=0.001, label="å­¦ä¹ ç‡")
                            dropout_static = gr.Slider(0, 0.5, 0.1, 0.05, label="Dropoutç‡")

                        gr.Markdown("### ğŸ”€ æ•°æ®åˆ†å‰²")
                        with gr.Row():
                            test_size_static = gr.Slider(0.1, 0.3, 0.15, 0.05, label="æµ‹è¯•é›†æ¯”ä¾‹")
                            val_size_static = gr.Slider(0.1, 0.3, 0.15, 0.05, label="éªŒè¯é›†æ¯”ä¾‹")

                        train_btn_static = gr.Button("â–¶ï¸ å¼€å§‹TrainingSST", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“Š è®­ç»ƒæ—¥å¿—")
                        training_log_static = gr.Textbox(
                            label="è®­ç»ƒè¿›åº¦",
                            lines=30,
                            autoscroll=True,
                            interactive=False
                        )

            # Tab 3: æ®‹å·®æå–
            with gr.Tab("ğŸ”¬ æ®‹å·®æå–", elem_id="residual_extraction"):
                gr.Markdown("## ä»è®­ç»ƒå¥½çš„SSTæ¨¡å‹æå–æ®‹å·®")

                with gr.Row():
                    with gr.Column(scale=1):
                        model_selector = gr.Dropdown(
                            choices=get_available_models(),
                            label="é€‰æ‹©SSTæ¨¡å‹"
                        )
                        refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        future_horizon = gr.Slider(
                            1, 100, 10, 1,
                            label="æœªæ¥é¢„æµ‹é•¿åº¦",
                            info="ç”¨äºåç»­TFTè®­ç»ƒçš„æœªæ¥æ­¥æ•°"
                        )

                        gr.Markdown("### âœ‚ï¸ æ•°æ®ç‰‡æ®µé€‰æ‹©ï¼ˆå¯é€‰ï¼‰")
                        use_segment_checkbox = gr.Checkbox(
                            label="ä½¿ç”¨æ•°æ®ç‰‡æ®µ",
                            value=False
                        )
                        with gr.Row():
                            start_index_input = gr.Number(value=0, label="èµ·å§‹ç´¢å¼•", precision=0)
                            end_index_input = gr.Number(value=10000, label="ç»“æŸç´¢å¼•", precision=0)

                        with gr.Row():
                            preset_10k_btn = gr.Button("0-10K", size="sm")
                            preset_50k_btn = gr.Button("0-50K", size="sm")
                            preset_100k_btn = gr.Button("0-100K", size="sm")
                            preset_200k_btn = gr.Button("0-200K", size="sm")

                        extract_btn = gr.Button("ğŸ”¬ æå–æ®‹å·®", variant="primary", size="lg")

                        gr.Markdown("### ğŸ“¤ åŠ è½½æ¨ç†é…ç½®")
                        inference_config_file = gr.File(
                            label="ä¸Šä¼ æ¨ç†é…ç½®æ–‡ä»¶ (*_inference.json)",
                            file_types=['.json']
                        )
                        load_inference_btn = gr.Button("ğŸ“¥ åŠ è½½é…ç½®", size="sm")
                        inference_load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", lines=3, interactive=False)

                    with gr.Column(scale=1):
                        residual_status = gr.Textbox(label="æ®‹å·®æå–çŠ¶æ€", lines=15, interactive=False)
                        residual_plot = gr.Plot(label="æ®‹å·®å¯è§†åŒ–")

                # Event binding
                def set_range_preset(start, end):
                    return start, end, True

                preset_10k_btn.click(
                    fn=lambda: set_range_preset(0, 10000),
                    outputs=[start_index_input, end_index_input, use_segment_checkbox]
                )
                preset_50k_btn.click(
                    fn=lambda: set_range_preset(0, 50000),
                    outputs=[start_index_input, end_index_input, use_segment_checkbox]
                )
                preset_100k_btn.click(
                    fn=lambda: set_range_preset(0, 100000),
                    outputs=[start_index_input, end_index_input, use_segment_checkbox]
                )
                preset_200k_btn.click(
                    fn=lambda: set_range_preset(0, 200000),
                    outputs=[start_index_input, end_index_input, use_segment_checkbox]
                )

                refresh_models_btn.click(
                    fn=lambda: gr.update(choices=get_available_models()),
                    outputs=[model_selector]
                )

                extract_btn.click(
                    fn=extract_residuals_ui,
                    inputs=[
                        model_selector,
                        future_horizon,
                        use_segment_checkbox,
                        start_index_input,
                        end_index_input
                    ],
                    outputs=[residual_status, residual_plot]
                )

                load_inference_btn.click(
                    fn=lambda f: load_model_from_inference_config(f.name, device) if f else (None, "âŒ è¯·ä¸Šä¼ æ–‡ä»¶"),
                    inputs=[inference_config_file],
                    outputs=[model_selector, inference_load_status]
                )

            # Tab 4: Stage2 BoostTraining
            with gr.Tab("ğŸš€ Stage2 BoostTraining", elem_id="stage2_training"):
                gr.Markdown("## è®­ç»ƒStage2æ®‹å·®æ¨¡å‹")
                gr.Markdown("åŸºäºæå–çš„æ®‹å·®è®­ç»ƒStage2æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æå‡é¢„æµ‹ç²¾åº¦")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“Š æ•°æ®é€‰æ‹©")
                        residual_data_selector_stage2 = gr.Dropdown(
                            choices=get_residual_data_keys(),
                            label="é€‰æ‹©æ®‹å·®æ•°æ®"
                        )
                        refresh_residual_btn_stage2 = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### ğŸ—ï¸ æ¨¡å‹æ¶æ„")
                        with gr.Row():
                            d_model_stage2 = gr.Slider(32, 256, 128, 32, label="æ¨¡å‹ç»´åº¦")
                            nhead_stage2 = gr.Slider(2, 16, 8, 2, label="æ³¨æ„åŠ›å¤´æ•°")
                        num_layers_stage2 = gr.Slider(1, 8, 3, 1, label="Transformerå±‚æ•°")

                        gr.Markdown("### ğŸ¯ è®­ç»ƒå‚æ•°")
                        with gr.Row():
                            epochs_stage2 = gr.Slider(10, 200, 100, 10, label="è®­ç»ƒè½®æ•°")
                            batch_size_stage2 = gr.Slider(16, 128, 32, 16, label="æ‰¹å¤§å°")
                        with gr.Row():
                            lr_stage2 = gr.Number(value=0.001, label="å­¦ä¹ ç‡")
                            dropout_stage2 = gr.Slider(0, 0.5, 0.1, 0.05, label="Dropoutç‡")
                        with gr.Row():
                            weight_decay_stage2 = gr.Number(value=1e-5, label="æƒé‡è¡°å‡")
                            grad_clip_stage2 = gr.Slider(0.1, 5.0, 1.0, 0.1, label="æ¢¯åº¦è£å‰ª")

                        gr.Markdown("### ğŸ”€ æ•°æ®åˆ†å‰²")
                        with gr.Row():
                            test_size_stage2 = gr.Slider(0.1, 0.3, 0.15, 0.05, label="æµ‹è¯•é›†æ¯”ä¾‹")
                            val_size_stage2 = gr.Slider(0.1, 0.3, 0.15, 0.05, label="éªŒè¯é›†æ¯”ä¾‹")

                        train_stage2_btn = gr.Button("ğŸš€ å¼€å§‹TrainingStage2", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“Š è®­ç»ƒæ—¥å¿—")
                        stage2_training_log = gr.Textbox(
                            label="è®­ç»ƒè¿›åº¦",
                            lines=30,
                            autoscroll=True,
                            interactive=False
                        )

                # Stage2Trainingå‡½æ•°
                def train_stage2_ui(residual_data_key, d_model, nhead, num_layers, dropout,
                                    epochs, batch_size, lr, weight_decay, grad_clip,
                                    test_size, val_size, progress=gr.Progress()):

                    config = {
                        'd_model': int(d_model),
                        'nhead': int(nhead),
                        'num_layers': int(num_layers),
                        'dropout': float(dropout),
                        'epochs': int(epochs),
                        'batch_size': int(batch_size),
                        'lr': float(lr),
                        'weight_decay': float(weight_decay),
                        'grad_clip': float(grad_clip),
                        'test_size': float(test_size),
                        'val_size': float(val_size),
                        'early_stop_patience': 25,
                        'scheduler_patience': 10
                    }

                    def progress_callback(msg):
                        progress(0.5, desc="è®­ç»ƒä¸­...")
                        return msg

                    status_msg, results = train_stage2_boost_model(
                        residual_data_key, config, progress_callback
                    )

                    return status_msg

                refresh_residual_btn_stage2.click(
                    fn=lambda: gr.update(choices=get_residual_data_keys()),
                    outputs=[residual_data_selector_stage2]
                )

                train_stage2_btn.click(
                    fn=train_stage2_ui,
                    inputs=[
                        residual_data_selector_stage2,
                        d_model_stage2, nhead_stage2, num_layers_stage2, dropout_stage2,
                        epochs_stage2, batch_size_stage2, lr_stage2,
                        weight_decay_stage2, grad_clip_stage2,
                        test_size_stage2, val_size_stage2
                    ],
                    outputs=[stage2_training_log]
                )

            # Tab 5: ç»¼åˆæ¨ç†æ¨¡å‹ç”Ÿæˆ
            with gr.Tab("ğŸ¯ ç»¼åˆæ¨ç†æ¨¡å‹", elem_id="ensemble_model"):
                gr.Markdown("## ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹")
                gr.Markdown("æ™ºèƒ½é€‰æ‹©RÂ²é˜ˆå€¼ï¼Œç»„åˆSSTæ¨¡å‹å’ŒStage2æ¨¡å‹ç”Ÿæˆç»¼åˆé¢„æµ‹")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ”§ æ¨¡å‹é€‰æ‹©")
                        base_model_selector = gr.Dropdown(
                            choices=get_available_models(),
                            label="é€‰æ‹©åŸºç¡€SSTæ¨¡å‹"
                        )
                        stage2_model_selector = gr.Dropdown(
                            choices=get_stage2_model_keys(),
                            label="é€‰æ‹©Stage2æ¨¡å‹"
                        )
                        refresh_ensemble_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### ğŸšï¸ RÂ²é˜ˆå€¼è®¾ç½®")
                        r2_threshold_slider = gr.Slider(
                            0.0, 1.0, 0.4, 0.05,
                            label="RÂ²é˜ˆå€¼",
                            info="åªå¯¹RÂ² < é˜ˆå€¼çš„ä¿¡å·åº”ç”¨Stage2ä¿®æ­£"
                        )

                        generate_ensemble_btn = gr.Button("ğŸ¯ ç”Ÿæˆç»¼åˆæ¨¡å‹", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        ensemble_status = gr.Textbox(
                            label="ç”ŸæˆçŠ¶æ€",
                            lines=30,
                            autoscroll=True,
                            interactive=False
                        )

                def generate_ensemble_ui(base_model_name, stage2_model_name, r2_threshold):
                    if not base_model_name or not stage2_model_name:
                        return "âŒ è¯·é€‰æ‹©åŸºç¡€æ¨¡å‹å’ŒStage2æ¨¡å‹ï¼"

                    status_msg, ensemble_info = compute_signal_r2_and_select_threshold(
                        base_model_name, stage2_model_name, r2_threshold
                    )
                    return status_msg

                refresh_ensemble_btn.click(
                    fn=lambda: (gr.update(choices=get_available_models()),
                                gr.update(choices=get_stage2_model_keys())),
                    outputs=[base_model_selector, stage2_model_selector]
                )

                generate_ensemble_btn.click(
                    fn=generate_ensemble_ui,
                    inputs=[base_model_selector, stage2_model_selector, r2_threshold_slider],
                    outputs=[ensemble_status]
                )

            # Tab 6: äºŒæ¬¡æ¨ç†æ¯”è¾ƒ
            with gr.Tab("ğŸ“Š äºŒæ¬¡æ¨ç†æ¯”è¾ƒ", elem_id="reinference_comparison"):
                gr.Markdown("## äºŒæ¬¡æ¨ç†æ¯”è¾ƒ")
                gr.Markdown("é€‰æ‹©indexèŒƒå›´ï¼Œæ¯”è¾ƒç»¼åˆæ¨¡å‹ä¸çº¯SSTæ¨¡å‹çš„æ€§èƒ½æå‡")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¯ æ¨¡å‹é€‰æ‹©")
                        ensemble_selector_reinf = gr.Dropdown(
                            choices=get_ensemble_model_keys(),
                            label="é€‰æ‹©ç»¼åˆæ¨¡å‹"
                        )
                        refresh_reinf_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### ğŸ“ IndexèŒƒå›´é€‰æ‹©")
                        with gr.Row():
                            reinf_start_idx = gr.Number(value=0, label="èµ·å§‹Index", precision=0)
                            reinf_end_idx = gr.Number(value=1000, label="ç»“æŸIndex", precision=0)

                        compare_reinf_btn = gr.Button("ğŸ“Š æ‰§è¡Œæ¯”è¾ƒ", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        reinf_status = gr.Textbox(
                            label="æ¯”è¾ƒç»“æœ",
                            lines=20,
                            autoscroll=True,
                            interactive=False
                        )
                        reinf_plot = gr.Plot(label="æ€§èƒ½å¯¹æ¯”å¯è§†åŒ–")

                def compare_reinference_ui(ensemble_name, start_idx, end_idx):
                    if not ensemble_name:
                        return "âŒ è¯·é€‰æ‹©ç»¼åˆæ¨¡å‹ï¼", None

                    if ensemble_name not in global_state['ensemble_models']:
                        return "âŒ ç»¼åˆæ¨¡å‹ä¸å­˜åœ¨ï¼", None

                    try:
                        ensemble_info = global_state['ensemble_models'][ensemble_name]

                        # è·å–é¢„æµ‹æ•°æ®
                        y_true = ensemble_info['predictions']['y_true']
                        y_pred_base = ensemble_info['predictions']['y_pred_base']
                        y_pred_ensemble = ensemble_info['predictions']['y_pred_ensemble']

                        # åˆ‡ç‰‡
                        start_idx = max(0, int(start_idx))
                        end_idx = min(len(y_true), int(end_idx))

                        y_true_seg = y_true[start_idx:end_idx]
                        y_pred_base_seg = y_pred_base[start_idx:end_idx]
                        y_pred_ensemble_seg = y_pred_ensemble[start_idx:end_idx]

                        # è®¡ç®—æ€§èƒ½ using safe RÂ² computation
                        mae_base = mean_absolute_error(y_true_seg, y_pred_base_seg)
                        mae_ensemble = mean_absolute_error(y_true_seg, y_pred_ensemble_seg)
                        rmse_base = np.sqrt(mean_squared_error(y_true_seg, y_pred_base_seg))
                        rmse_ensemble = np.sqrt(mean_squared_error(y_true_seg, y_pred_ensemble_seg))
                        r2_base, _ = compute_r2_safe(y_true_seg, y_pred_base_seg, method='per_output_mean')
                        r2_ensemble, _ = compute_r2_safe(y_true_seg, y_pred_ensemble_seg, method='per_output_mean')

                        improvement_mae = (mae_base - mae_ensemble) / mae_base * 100
                        improvement_rmse = (rmse_base - rmse_ensemble) / rmse_base * 100

                        status = f"ğŸ“Š äºŒæ¬¡æ¨ç†æ¯”è¾ƒç»“æœ\n"
                        status += f"=" * 60 + "\n\n"
                        status += f"ğŸ“ IndexèŒƒå›´: [{start_idx}, {end_idx})\n"
                        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(y_true_seg):,}\n\n"

                        status += f"æ€§èƒ½å¯¹æ¯”:\n"
                        status += f"{'æŒ‡æ ‡':<15} {'SSTæ¨¡å‹':>15} {'ç»¼åˆæ¨¡å‹':>15} {'æ”¹è¿›':>12}\n"
                        status += "-" * 60 + "\n"
                        status += f"{'MAE':<15} {mae_base:>15.6f} {mae_ensemble:>15.6f} {improvement_mae:>11.2f}%\n"
                        status += f"{'RMSE':<15} {rmse_base:>15.6f} {rmse_ensemble:>15.6f} {improvement_rmse:>11.2f}%\n"
                        status += f"{'RÂ²':<15} {r2_base:>15.4f} {r2_ensemble:>15.4f}\n"

                        # åˆ›å»ºå¯è§†åŒ–
                        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                        fig.suptitle(f'äºŒæ¬¡æ¨ç†æ¯”è¾ƒ - {ensemble_name}', fontsize=16)

                        # é¢„æµ‹å¯¹æ¯”ï¼ˆç¬¬ä¸€ä¸ªä¿¡å·ï¼‰
                        plot_samples = min(500, len(y_true_seg))
                        axes[0, 0].plot(y_true_seg[:plot_samples, 0], label='True', alpha=0.7)
                        axes[0, 0].plot(y_pred_base_seg[:plot_samples, 0], label='SST', alpha=0.7)
                        axes[0, 0].plot(y_pred_ensemble_seg[:plot_samples, 0], label='Ensemble', alpha=0.7)
                        axes[0, 0].set_title('é¢„æµ‹å¯¹æ¯” (ä¿¡å·1)')
                        axes[0, 0].legend()
                        axes[0, 0].set_xlabel('Index')
                        axes[0, 0].set_ylabel('Value')

                        # è¯¯å·®å¯¹æ¯”
                        error_base = np.abs(y_true_seg - y_pred_base_seg).mean(axis=1)
                        error_ensemble = np.abs(y_true_seg - y_pred_ensemble_seg).mean(axis=1)

                        axes[0, 1].plot(error_base[:plot_samples], label='SST Error', alpha=0.7)
                        axes[0, 1].plot(error_ensemble[:plot_samples], label='Ensemble Error', alpha=0.7)
                        axes[0, 1].set_title('å¹³å‡ç»å¯¹è¯¯å·®å¯¹æ¯”')
                        axes[0, 1].legend()
                        axes[0, 1].set_xlabel('Index')
                        axes[0, 1].set_ylabel('MAE')

                        # è¯¯å·®åˆ†å¸ƒ
                        axes[1, 0].hist(error_base, bins=50, alpha=0.5, label='SST', edgecolor='black')
                        axes[1, 0].hist(error_ensemble, bins=50, alpha=0.5, label='Ensemble', edgecolor='black')
                        axes[1, 0].set_title('è¯¯å·®åˆ†å¸ƒ')
                        axes[1, 0].legend()
                        axes[1, 0].set_xlabel('Error')
                        axes[1, 0].set_ylabel('Frequency')

                        # æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾
                        metrics = ['MAE', 'RMSE', 'RÂ²']
                        base_values = [mae_base, rmse_base, r2_base]
                        ensemble_values = [mae_ensemble, rmse_ensemble, r2_ensemble]

                        x = np.arange(len(metrics))
                        width = 0.35

                        axes[1, 1].bar(x - width / 2, base_values, width, label='SST', alpha=0.8)
                        axes[1, 1].bar(x + width / 2, ensemble_values, width, label='Ensemble', alpha=0.8)
                        axes[1, 1].set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
                        axes[1, 1].set_xticks(x)
                        axes[1, 1].set_xticklabels(metrics)
                        axes[1, 1].legend()

                        plt.tight_layout()

                        return status, fig

                    except Exception as e:
                        error_msg = f"âŒ æ¯”è¾ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
                        return error_msg, None

                refresh_reinf_btn.click(
                    fn=lambda: gr.update(choices=get_ensemble_model_keys()),
                    outputs=[ensemble_selector_reinf]
                )

                compare_reinf_btn.click(
                    fn=compare_reinference_ui,
                    inputs=[ensemble_selector_reinf, reinf_start_idx, reinf_end_idx],
                    outputs=[reinf_status, reinf_plot]
                )

            # Tab 7: Sundialæ—¶åºé¢„æµ‹
            with gr.Tab("ğŸ”® Sundialæ®‹å·®é¢„æµ‹", elem_id="sundial_forecast"):
                gr.Markdown("## Sundialæ—¶åºæ¨¡å‹é¢„æµ‹æœªæ¥æ®‹å·®")
                gr.Markdown("åŸºäºç»¼åˆæ¨¡å‹çš„æœ€ç»ˆæ®‹å·®ï¼Œä½¿ç”¨Sundialé¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¯ æ¨¡å‹é€‰æ‹©")
                        ensemble_selector_sundial = gr.Dropdown(
                            choices=get_ensemble_model_keys(),
                            label="é€‰æ‹©ç»¼åˆæ¨¡å‹"
                        )
                        refresh_sundial_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### âš™ï¸ Sundialé…ç½®")
                        sundial_forecast_horizon = gr.Slider(
                            10, 500, 100, 10,
                            label="é¢„æµ‹æ­¥æ•°",
                            info="é¢„æµ‹æœªæ¥å¤šå°‘ä¸ªæ—¶é—´æ­¥çš„æ®‹å·®"
                        )

                        with gr.Row():
                            sundial_d_model = gr.Slider(32, 256, 64, 32, label="æ¨¡å‹ç»´åº¦")
                            sundial_nhead = gr.Slider(2, 8, 4, 2, label="æ³¨æ„åŠ›å¤´æ•°")

                        train_sundial_btn = gr.Button("ğŸ”® TrainingSundial", variant="primary", size="lg")
                        predict_sundial_btn = gr.Button("ğŸ“ˆ é¢„æµ‹æœªæ¥æ®‹å·®", size="lg")

                    with gr.Column(scale=1):
                        sundial_status = gr.Textbox(
                            label="çŠ¶æ€ä¿¡æ¯",
                            lines=20,
                            autoscroll=True,
                            interactive=False
                        )
                        sundial_plot = gr.Plot(label="æ®‹å·®é¢„æµ‹å¯è§†åŒ–")

                def train_sundial_ui(ensemble_name, forecast_horizon, d_model, nhead):
                    if not ensemble_name:
                        return "âŒ è¯·é€‰æ‹©ç»¼åˆæ¨¡å‹ï¼", None

                    # è¿™é‡Œæ˜¯SundialTrainingçš„å ä½ç¬¦ï¼Œå®é™…éœ€è¦å®ç°Sundialæ¨¡å‹
                    status = f"ğŸ”® Sundialæ¨¡å‹è®­ç»ƒ\n"
                    status += f"=" * 60 + "\n\n"
                    status += f"ğŸ“Š ç»¼åˆæ¨¡å‹: {ensemble_name}\n"
                    status += f"ğŸ“ é¢„æµ‹æ­¥æ•°: {int(forecast_horizon)}\n"
                    status += f"âš™ï¸ æ¨¡å‹é…ç½®: d_model={int(d_model)}, nhead={int(nhead)}\n\n"
                    status += f"âš ï¸ Sundialæ¨¡å‹è®­ç»ƒåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...\n"
                    status += f"ğŸ’¡ Sundialå°†åŸºäºæ—¶é—´åºåˆ—ç‰¹æ€§é¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿\n"

                    return status, None

                def predict_sundial_ui(ensemble_name):
                    if not ensemble_name:
                        return "âŒ è¯·é€‰æ‹©ç»¼åˆæ¨¡å‹ï¼", None

                    status = f"ğŸ“ˆ Sundialæ®‹å·®é¢„æµ‹\n"
                    status += f"=" * 60 + "\n\n"
                    status += f"âš ï¸ Sundialé¢„æµ‹åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...\n"

                    return status, None

                refresh_sundial_btn.click(
                    fn=lambda: gr.update(choices=get_ensemble_model_keys()),
                    outputs=[ensemble_selector_sundial]
                )

                train_sundial_btn.click(
                    fn=train_sundial_ui,
                    inputs=[ensemble_selector_sundial, sundial_forecast_horizon,
                            sundial_d_model, sundial_nhead],
                    outputs=[sundial_status, sundial_plot]
                )

                predict_sundial_btn.click(
                    fn=predict_sundial_ui,
                    inputs=[ensemble_selector_sundial],
                    outputs=[sundial_status, sundial_plot]
                )

        # Footer info
        gr.Markdown("""
        ---
        ## ğŸ“– ä½¿ç”¨æµç¨‹

        ### å®Œæ•´æµç¨‹
        1ï¸âƒ£ **æ•°æ®åŠ è½½** â†’ ä¸Šä¼ CSVæˆ–Create sample data
        2ï¸âƒ£ **SSTæ¨¡å‹Training** â†’ Trainingé™æ€ä¼ æ„Ÿå™¨æ˜ å°„Transformer
        3ï¸âƒ£ **æ®‹å·®æå–** â†’ ä»SSTæ¨¡å‹æå–é¢„æµ‹æ®‹å·®
        4ï¸âƒ£ **Stage2Training** â†’ TrainingStage2 residual model
        5ï¸âƒ£ **ç”Ÿæˆç»¼åˆæ¨¡å‹** â†’ æ™ºèƒ½RÂ²é˜ˆå€¼é€‰æ‹©ï¼Œç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹
        6ï¸âƒ£ **äºŒæ¬¡æ¨ç†æ¯”è¾ƒ** â†’ å¯¹æ¯”ç»¼åˆæ¨¡å‹ä¸SSTæ¨¡å‹çš„æ€§èƒ½æå‡
        7ï¸âƒ£ **Sundialé¢„æµ‹** â†’ é¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿

        **ğŸ¯ åˆ›æ–°ç‚¹**: 
        - âœ¨ Stage2 Boostæ¶æ„ï¼šé’ˆå¯¹æ€§æ”¹è¿›ä½RÂ²ä¿¡å·
        - ğŸ¯ æ™ºèƒ½é˜ˆå€¼é€‰æ‹©ï¼šè‡ªåŠ¨å†³å®šå“ªäº›ä¿¡å·éœ€è¦Stage2
        - ğŸ“Š ç»¼åˆæ¨ç†æ¨¡å‹ï¼šæœ€ä¼˜ç»„åˆSSTå’ŒStage2
        - ğŸ”® æ®‹å·®é¢„æµ‹ï¼šSundialé¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿
        """)

        # Auto refresh dropdowns on page load
        # Initial load: populate dropdowns and check for pre-loaded data
        def initial_load():
            """Load initial state including pre-loaded data from Colab"""
            # Get dropdown choices
            models = get_available_models()
            residual_keys = get_residual_data_keys()
            stage2_keys = get_stage2_model_keys()
            ensemble_keys = get_ensemble_model_keys()

            # Get available CSV files (safe - won't break interface)
            csv_files = get_available_csv_files()

            # Check for pre-loaded data (but don't auto-load)
            status, preview_df, signals = check_preloaded_data()

            # Get column choices if data exists
            if global_state.get('df') is not None:
                cols = list(global_state['df'].columns)
            else:
                cols = []

            return (
                gr.update(choices=models),
                gr.update(choices=residual_keys),
                gr.update(choices=stage2_keys),
                gr.update(choices=ensemble_keys),
                gr.update(choices=csv_files),  # Populate CSV file selector
                status, signals, preview_df,
                gr.update(choices=cols), gr.update(choices=cols)
            )

        demo.load(
            fn=initial_load,
            outputs=[
                model_selector, residual_data_selector_stage2,
                stage2_model_selector, ensemble_selector_reinf,
                csv_file_selector,  # Add CSV file selector to outputs
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # Data loading events
        def load_data_and_update(file_obj):
            status, preview_df, signals = load_data_from_csv(file_obj)
            if preview_df is not None:
                cols = list(global_state['df'].columns)
                return (
                    status, signals, preview_df,
                    gr.update(choices=cols), gr.update(choices=cols)
                )
            return (
                status, signals, None,
                gr.update(choices=[]), gr.update(choices=[])
            )

        def create_sample_and_update():
            status, preview_df, signals = create_sample_data()
            if preview_df is not None:
                cols = list(global_state['df'].columns)
                return (
                    status, signals, preview_df,
                    gr.update(choices=cols), gr.update(choices=cols)
                )
            return (
                status, signals, None,
                gr.update(choices=[]), gr.update(choices=[])
            )

        # CSV file selector event - load from data/ folder
        def load_from_selector_and_update(csv_path):
            status, preview_df, signals = load_csv_from_path(csv_path)
            if preview_df is not None:
                cols = list(global_state['df'].columns)
                return (
                    status, signals, preview_df,
                    gr.update(choices=cols), gr.update(choices=cols)
                )
            return (
                status, signals, None,
                gr.update(choices=[]), gr.update(choices=[])
            )

        select_csv_btn.click(
            fn=load_from_selector_and_update,
            inputs=[csv_file_selector],
            outputs=[
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # Refresh CSV file list
        refresh_csv_btn.click(
            fn=lambda: gr.update(choices=get_available_csv_files()),
            outputs=[csv_file_selector]
        )

        # Upload button event - load from uploaded file
        upload_btn.click(
            fn=load_data_and_update,
            inputs=[data_file],
            outputs=[
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # Sample button event - create sample data
        sample_btn.click(
            fn=create_sample_and_update,
            outputs=[
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # TrainingæŒ‰é’®ç»‘å®š
        train_btn_static.click(
            fn=train_base_model_ui,
            inputs=[
                boundary_signals_static, target_signals_static,
                gr.Textbox(value="StaticSensorTransformer", visible=False),
                epochs_static, batch_size_static, lr_static,
                d_model_static, nhead_static, num_layers_static, dropout_static,
                test_size_static, val_size_static,
                gr.State(value=None), gr.State(value=False)
            ],
            outputs=[training_log_static]
        )

    return demo


# ============================================================================
# Launch application

if __name__ == "__main__":
    import sys

    print("å¯åŠ¨å·¥ä¸šæ•°å­—å­ªç”Ÿæ®‹å·®Boostè®­ç»ƒç³»ç»Ÿ...")
    print("="*80)

    # Check if running in Colab
    try:
        import google.colab
        IN_COLAB = True
        print("âœ… æ£€æµ‹åˆ°Colabç¯å¢ƒ")
    except:
        IN_COLAB = False
        print("âœ… æœ¬åœ°ç¯å¢ƒ")

    demo = create_unified_interface()
    print("âœ… ç•Œé¢åˆ›å»ºå®Œæˆ")
    print("="*80)

    if IN_COLAB:
        # Colab environment - use share=True for public URL
        print("\nğŸŒ åœ¨Colabä¸­å¯åŠ¨Gradio...")
        print("ğŸ“ æç¤ºï¼šGradioå°†ç”Ÿæˆä¸€ä¸ªå…¬ç½‘é“¾æ¥")
        demo.launch(
            share=True,
            debug=True,
            show_error=True,
            inline=False  # Use separate window
        )
    else:
        # Local environment - try multiple ports
        print("\nğŸŒ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å¯åŠ¨Gradio...")
        for port in range(7860, 7870):
            try:
                print(f"å°è¯•ç«¯å£ {port}...")
                demo.launch(
                    server_name="127.0.0.1",
                    server_port=port,
                    share=False,
                    debug=True,
                    show_error=True,
                    quiet=False
                )
                print(f"âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼")
                print(f"ğŸ”— è®¿é—®åœ°å€: http://localhost:{port}")
                print("="*80)
                break
            except OSError:
                print(f"âš ï¸  ç«¯å£ {port} è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                continue
        else:
            print("âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ (7860-7869)")
