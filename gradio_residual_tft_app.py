#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Gradio Interface for Industrial Digital Twin with Residual Boost Training
Complete Residual Boost Training System - Enhanced Gradio Interface
New features:
1. Stage2 residual model training (based on residuals generated by SST)
2. Intelligent R2 threshold selection to generate ensemble inference model
3. Secondary inference comparison (Ensemble model vs Pure SST model)
4. Sundial time series model predicting future residuals
"""
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from typing import Dict, List, Tuple, Any, Optional
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
import sys
import warnings
import traceback
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import matplotlib
import platform
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ============================================================================
# TFT model save function (added at top of file)

def save_tft_model_with_config(
        model_name: str,
        tft_model: nn.Module,
        config: Dict[str, Any],
        scalers: Dict[str, StandardScaler],
        residual_data_key: str,
        residual_info: Dict[str, Any],
        history: Dict[str, List[float]]
) -> Tuple[str, str, str]:
    """
    Save TFT model, config and scalers

    Args:
        model_name: TFTModel name
        tft_model: Trained TFT model
        config: Training config
        scalers: Data scalers
        residual_data_key: Residual data key
        residual_info: Residual data info
        history: Training history

    Returns:
        model_path: Model weight file path
        scaler_path: ScalerFile path
        inference_config_path: æ¨ç†é…ç½®File path
    """
    model_dir = "saved_models/tft_models"
    os.makedirs(model_dir, exist_ok=True)

    # 1. Save model weights
    model_path = os.path.join(model_dir, f"{model_name}.pth")
    torch.save({
        'model_state_dict': tft_model.state_dict(),
        'model_config': {
            'num_targets': tft_model.num_targets,
            'num_external_factors': tft_model.num_external_factors,
            'd_model': tft_model.d_model,
            'use_grouping': tft_model.use_grouping,
            'signal_groups': tft_model.signal_groups if hasattr(tft_model, 'signal_groups') else None
        },
        'training_config': config,
        'training_history': history,
        'residual_data_key': residual_data_key,
        'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }, model_path)

    # 2. Save scalers
    scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
    with open(scaler_path, 'wb') as f:
        pickle.dump(scalers, f)

    # 3. Save inference config JSON (most important)
    inference_config_path = os.path.join(model_dir, f"{model_name}_inference.json")
    inference_config = {
        'model_name': model_name,
        'model_type': 'ResidualTFT',
        'model_path': model_path,
        'scaler_path': scaler_path,

        # TFT model architecture
        'architecture': {
            'd_model': config['d_model'],
            'nhead': config['nhead'],
            'num_encoder_layers': config['num_encoder_layers'],
            'num_decoder_layers': config['num_decoder_layers'],
            'dropout': config['dropout'],
            'use_grouping': config.get('use_grouping', False),
            'signal_groups': config.get('signal_groups', None)
        },

        # Data config
        'data_config': {
            'encoder_length': config['encoder_length'],
            'future_horizon': residual_info['future_horizon'],
            'residual_data_key': residual_data_key,
            'base_model_name': residual_info['base_model_name'],
            'num_targets': len(residual_info['target_signals']),
            'num_external_factors': len(residual_info['boundary_signals'])
        },

        # Signal info
        'signals': {
            'boundary_signals': residual_info['boundary_signals'],
            'target_signals': residual_info['target_signals'],
            'residual_signals': residual_info['residual_signals']
        },

        # Training info
        'training_info': {
            'epochs_trained': len(history['train_losses']),
            'best_val_loss': min(history['val_losses']),
            'batch_size': config['batch_size'],
            'learning_rate': config['lr']
        },

        'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    with open(inference_config_path, 'w', encoding='utf-8') as f:
        json.dump(inference_config, f, indent=2, ensure_ascii=False)

    print(f"âœ… TFTæ¨¡å‹å·²ä¿å­˜:")
    print(f"   ğŸ“¦ æ¨¡å‹æƒé‡: {model_path}")
    print(f"   ğŸ“Š Scalers: {scaler_path}")
    print(f"   ğŸ“„ æ¨ç†é…ç½®: {inference_config_path}")

    return model_path, scaler_path, inference_config_path


# ============================================================================
# TFT model load function

def load_tft_model_from_config(config_file_path: str, device: torch.device) -> Tuple[str, str]:
    """
    Load TFT model from inference config file

    Args:
        config_file_path: æ¨ç†é…ç½®JSONFile path
        device: PyTorch device

    Returns:
        model_name: Model name
        status_msg: åŠ è½½Status message
    """
    try:
        # Read config
        with open(config_file_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        model_name = config['model_name']
        model_path = config['model_path']
        scaler_path = config['scaler_path']

        # Check if files exist
        if not os.path.exists(model_path):
            return None, f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
        if not os.path.exists(scaler_path):
            return None, f"âŒ Scaleræ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}"

        # Load model
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model_config = checkpoint['model_config']

        # Rebuild TFT model
        tft_model = GroupedMultiTargetTFT(
            num_targets=model_config['num_targets'],
            num_external_factors=model_config['num_external_factors'],
            d_model=config['architecture']['d_model'],
            nhead=config['architecture']['nhead'],
            num_encoder_layers=config['architecture']['num_encoder_layers'],
            num_decoder_layers=config['architecture']['num_decoder_layers'],
            dropout=config['architecture']['dropout'],
            use_grouping=config['architecture'].get('use_grouping', False),
            signal_groups=config['architecture'].get('signal_groups', None)
        )

        tft_model.load_state_dict(checkpoint['model_state_dict'])
        tft_model.to(device)
        tft_model.eval()

        # Load scalers
        with open(scaler_path, 'rb') as f:
            scalers = pickle.load(f)

        # Save to global state
        global_state['residual_models'][model_name] = {
            'model': tft_model,
            'config': config['architecture'],
            'history': checkpoint.get('training_history', {'train_losses': [], 'val_losses': []}),
            'residual_data_key': config['data_config']['residual_data_key'],
            'info': {
                'base_model_name': config['data_config']['base_model_name'],
                'target_signals': config['signals']['target_signals'],
                'boundary_signals': config['signals']['boundary_signals'],
                'residual_signals': config['signals']['residual_signals'],
                'model_type': 'StaticSensorTransformer',  # ä»base modelç»§æ‰¿
                'future_horizon': config['data_config']['future_horizon']
            },
            'encoder_length': config['data_config']['encoder_length'],
            'future_horizon': config['data_config']['future_horizon']
        }

        global_state['residual_scalers'][model_name] = scalers

        # æ„å»ºStatus message
        status_msg = f"âœ… TFTæ¨¡å‹åŠ è½½æˆåŠŸ!\n\n"
        status_msg += f"ğŸ“Œ Model name: {model_name}\n"
        status_msg += f"ğŸ“Š åŸºç¡€æ¨¡å‹: {config['data_config']['base_model_name']}\n"
        status_msg += f"ğŸ¯ ç›®æ ‡ä¿¡å·æ•°: {config['data_config']['num_targets']}\n"
        status_msg += f"ğŸ“ˆ è¾¹ç•Œä¿¡å·æ•°: {config['data_config']['num_external_factors']}\n"
        status_msg += f"ğŸ“ å†å²çª—å£é•¿åº¦: {config['data_config']['encoder_length']}\n"
        status_msg += f"ğŸ”® æœªæ¥é¢„æµ‹é•¿åº¦: {config['data_config']['future_horizon']}\n"
        status_msg += f"âš™ï¸ æ¨¡å‹ç»´åº¦: {config['architecture']['d_model']}\n"
        status_msg += f"ğŸ•’ åˆ›å»ºæ—¶é—´: {config['created_time']}\n"

        if 'training_info' in config:
            ti = config['training_info']
            status_msg += f"\nğŸ“š Training info:\n"
            status_msg += f"   - Trainingè½®æ•°: {ti['epochs_trained']}\n"
            status_msg += f"   - æœ€ä½³ValidationæŸå¤±: {ti['best_val_loss']:.6f}\n"
            status_msg += f"   - æ‰¹å¤§å°: {ti['batch_size']}\n"
            status_msg += f"   - å­¦ä¹ ç‡: {ti['learning_rate']}\n"

        print(status_msg)
        return model_name, status_msg

    except Exception as e:
        error_msg = f"âŒ TFTæ¨¡å‹åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return None, error_msg


# ============================================================================
# Configure Chinese font

def configure_chinese_font():
    """Configure matplotlib for Chinese font display"""
    import matplotlib
    import platform

    system = platform.system()
    if system == 'Darwin':  # macOS
        matplotlib.rc('font', family='Arial Unicode MS')
    elif system == 'Windows':
        matplotlib.rc('font', family='SimHei')
    else:  # Linux
        matplotlib.rc('font', family='DejaVu Sans')

    matplotlib.rcParams['axes.unicode_minus'] = False
    sns.set_style("whitegrid")


# ============================================================================
# Import modules

try:
    import gradio as gr

    print("âœ… Gradioå¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âŒ è¯·å®‰è£…gradio: pip install gradio")
    sys.exit(1)

# å°è¯•å¯¼å…¥æœ¬åœ°æ¨¡å—
try:
    from models.static_transformer import StaticSensorTransformer
    from models.residual_tft import (
        GroupedMultiTargetTFT,
        ResidualExtractor,
        train_residual_tft,
        prepare_residual_sequence_data,
        compute_r2_safe,
        compute_residuals_correctly,
        batch_inference,
        inference_with_boosting,
        compute_per_signal_metrics,
        clear_gpu_memory,
        print_gpu_memory
    )
    from models.utils import apply_ifd_smoothing

    print("âœ… æœ¬åœ°æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ æœ¬åœ°æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("å°è¯•ä½¿ç”¨ç›¸å¯¹å¯¼å…¥...")

    try:
        from static_transformer import StaticSensorTransformer
        from residual_tft import (
            GroupedMultiTargetTFT,
            ResidualExtractor,
            train_residual_tft,
            prepare_residual_sequence_data,
            compute_r2_safe,
            compute_residuals_correctly,
            batch_inference,
            inference_with_boosting,
            compute_per_signal_metrics,
            clear_gpu_memory,
            print_gpu_memory
        )
        from utils import apply_ifd_smoothing

        print("âœ… ç›¸å¯¹å¯¼å…¥æˆåŠŸ")
    except ImportError as e2:
        print(f"âŒ ç›¸å¯¹å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}")
        print("å°†ä½¿ç”¨å†…è”å®šä¹‰...")


# Setup device with enhanced GPU detection
def setup_device():
    """Setup computing device with GPU detection and configuration"""
    configure_chinese_font()
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"GPUæ£€æµ‹æˆåŠŸ: {torch.cuda.get_device_name(0)}")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        return device
    else:
        device = torch.device('cpu')
        print("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
        return device


device = setup_device()


def load_saved_models():
    """Load saved models from file system"""
    model_dir = "saved_models"
    if not os.path.exists(model_dir):
        return

    print(f"æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ä» {model_dir}...")

    for filename in os.listdir(model_dir):
        if filename.endswith('.pth') and not filename.endswith('_scalers.pkl'):
            model_name = filename[:-4]
            model_path = os.path.join(model_dir, filename)
            scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")

            try:
                checkpoint = torch.load(model_path, map_location=device, weights_only=False)
                model_config = checkpoint['model_config']

                if model_config['type'] == 'StaticSensorTransformer':
                    model = StaticSensorTransformer(
                        num_boundary_sensors=len(model_config['boundary_signals']),
                        num_target_sensors=len(model_config['target_signals']),
                        d_model=model_config['config']['d_model'],
                        nhead=model_config['config']['nhead'],
                        num_layers=model_config['config']['num_layers'],
                        dropout=model_config['config']['dropout']
                    )
                else:
                    continue

                model.load_state_dict(checkpoint['model_state_dict'])
                model.to(device)
                model.eval()

                scalers = {}
                if os.path.exists(scaler_path):
                    with open(scaler_path, 'rb') as f:
                        scalers = pickle.load(f)

                global_state['trained_models'][model_name] = {
                    'model': model,
                    'type': model_config['type'],
                    'boundary_signals': model_config['boundary_signals'],
                    'target_signals': model_config['target_signals'],
                    'config': model_config['config'],
                    'model_path': model_path,
                    'scaler_path': scaler_path
                }

                global_state['scalers'][model_name] = scalers
                print(f"  åŠ è½½æ¨¡å‹: {model_name}")

            except Exception as e:
                print(f"  åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {e}")
                continue

    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(global_state['trained_models'])} ä¸ªæ¨¡å‹")


# Global state management
global_state = {
    'df': None,
    'trained_models': {},
    'scalers': {},
    'residual_data': {},
    'residual_models': {},
    'residual_scalers': {},
    'final_predictions': {},
    'training_logs': {},
    'model_configs': {},
    'all_signals': [],
    # New: Stage2 Boost model storage
    'stage2_models': {},  # Stage2 residual model
    'stage2_scalers': {},  # Stage2 Scalers
    'ensemble_models': {},  # Ensemble inference model (SST + Stage2)
    'sundial_models': {},  # Sundial time series prediction model
    # Training control flags
    'stop_training_tab2': False,  # Flag to stop Tab2 training
    'stop_training_tab4': False,  # Flag to stop Tab4 training
}


# ============================================================================
# Colab Auto-load Support
# ============================================================================
def autoload_colab_data():
    """
    Automatically load pre-defined data from Colab environment

    This function checks for pre-saved CSV files and automatically loads them
    into global_state, making them immediately available in Tab1.

    Supports:
    - Standard predefined paths
    - Environment variable: COLAB_DATA_PATH
    - Wildcard matching in data/ folder
    - Google Drive mounted paths
    """
    import glob

    # Priority 1: Environment variable
    env_path = os.environ.get('COLAB_DATA_PATH')
    if env_path and os.path.exists(env_path):
        preload_paths = [env_path]
    else:
        # Priority 2: Standard predefined paths
        preload_paths = [
            'data/colab_preloaded_data.csv',
            'data/test_data.csv',
            '/content/colab_data.csv',
            # Add more common names
            'data/leap_data.csv',
            'data/sensor_data.csv',
            'data/training_data.csv',
            # Google Drive paths
            '/content/drive/MyDrive/data.csv',
            '/content/drive/MyDrive/colab_data.csv',
        ]

        # Priority 3: Wildcard search in data/ folder
        if os.path.exists('data'):
            csv_files = glob.glob('data/*.csv')
            if csv_files:
                # Add all CSV files in data/ folder
                preload_paths.extend(csv_files)

    for preload_path in preload_paths:
        if os.path.exists(preload_path):
            try:
                df_auto = pd.read_csv(preload_path)

                # Validate: must have at least 2 columns
                if df_auto.shape[1] < 2:
                    print(f"âš ï¸ [Colab Auto-load] Skipping {preload_path}: too few columns")
                    continue

                # Validate: must have at least 10 rows
                if df_auto.shape[0] < 10:
                    print(f"âš ï¸ [Colab Auto-load] Skipping {preload_path}: too few rows")
                    continue

                global_state['df'] = df_auto
                global_state['data_loaded'] = True

                print("=" * 80)
                print("âœ…âœ…âœ… [Colab Auto-load] Data successfully loaded into Tab1! âœ…âœ…âœ…")
                print(f"ğŸ“Š Data shape: {df_auto.shape}")
                print(f"ğŸ“‹ Columns: {list(df_auto.columns)[:10]}")  # Show first 10 columns
                if df_auto.shape[1] > 10:
                    print(f"    ... and {df_auto.shape[1] - 10} more columns")
                print(f"ğŸ“ Source: {preload_path}")
                print("=" * 80)

                return df_auto
            except Exception as e:
                print(f"âš ï¸ [Colab Auto-load] Failed to load {preload_path}: {e}")
                continue

    return None

# Auto-load disabled - user can manually select files in Tab1
# To enable auto-load, uncomment the line below:
# autoload_colab_data()


load_saved_models()

plt.style.use('default')
sns.set_palette("husl")

print("=" * 80)
print("Industrial Digital Twin with Residual Boost - Enhanced Interface")
print("=" * 80)
print(f"Using device: {device}")
print(f"PyTorch version: {torch.__version__}")
print("=" * 80)


# ============================================================================
# Model loading and inference config management

def save_inference_config(model_name, model_type, model_path, scaler_path,
                          boundary_signals, target_signals, config_dict):
    """
    Save inference config file - for direct model loading for inference later

    Args:
        model_name: Model name
        model_type: Model type
        model_path: Model weight file path
        scaler_path: ScalerFile path
        boundary_signals: Boundary signal list
        target_signals: Target signal list
        config_dict: Model architecture config
    """
    inference_config = {
        'model_name': model_name,
        'model_type': model_type,
        'model_path': model_path,
        'scaler_path': scaler_path,
        'boundary_signals': boundary_signals,
        'target_signals': target_signals,
        'architecture': config_dict,
        'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    model_dir = os.path.dirname(model_path)
    config_path = os.path.join(model_dir, f"{model_name}_inference.json")

    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(inference_config, f, indent=2, ensure_ascii=False)

    print(f"âœ… æ¨ç†é…ç½®å·²ä¿å­˜: {config_path}")
    return config_path


def load_model_from_inference_config(config_file_path, device):
    """
    ä»æ¨ç†é…ç½®æ–‡ä»¶Load model

    Args:
        config_file_path: æ¨ç†é…ç½®JSONFile path
        device: PyTorch device

    Returns:
        model_name: Model name
        success_msg: Success message
    """
    try:
        with open(config_file_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        model_name = config['model_name']
        model_type = config['model_type']
        model_path = config['model_path']
        scaler_path = config['scaler_path']

        # Check if files exist
        if not os.path.exists(model_path):
            return None, f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
        if not os.path.exists(scaler_path):
            return None, f"âŒ Scaleræ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}"

        # Load model
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        arch = config['architecture']

        if model_type == 'StaticSensorTransformer':
            model = StaticSensorTransformer(
                num_boundary_sensors=len(config['boundary_signals']),
                num_target_sensors=len(config['target_signals']),
                d_model=arch['d_model'],
                nhead=arch['nhead'],
                num_layers=arch['num_layers'],
                dropout=arch['dropout']
            )
        else:
            return None, f"âŒ ä¸æ”¯æŒçš„Model type: {model_type}"

        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()

        # Load scalers
        with open(scaler_path, 'rb') as f:
            scalers = pickle.load(f)

        # Save to global state
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': model_type,
            'boundary_signals': config['boundary_signals'],
            'target_signals': config['target_signals'],
            'config': arch,
            'model_path': model_path,
            'scaler_path': scaler_path
        }

        global_state['scalers'][model_name] = scalers

        success_msg = f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\n\n"
        success_msg += f"ğŸ“Œ Model name: {model_name}\n"
        success_msg += f"ğŸ“Š Model type: {model_type}\n"
        success_msg += f"ğŸ¯ è¾¹ç•Œä¿¡å·æ•°: {len(config['boundary_signals'])}\n"
        success_msg += f"ğŸ“ˆ ç›®æ ‡ä¿¡å·æ•°: {len(config['target_signals'])}\n"
        success_msg += f"âš™ï¸ æ¨¡å‹å‚æ•°: d_model={arch['d_model']}, nhead={arch['nhead']}, layers={arch['num_layers']}\n"
        success_msg += f"ğŸ•’ åˆ›å»ºæ—¶é—´: {config['created_time']}\n"

        print(success_msg)
        return model_name, success_msg

    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return None, error_msg


# ============================================================================
# Stage2 Boost model definition and training functions

def train_stage2_boost_model(
        residual_data_key: str,
        config: Dict[str, Any],
        progress=None
) -> Tuple[str, Dict[str, Any]]:
    """
    Train Stage2 Boost residual model

    Args:
        residual_data_key: Residual data key
        config: Training config
        progress: Gradio progress object for real-time updates

    Returns:
        status_msg: Training status message
        results: Training result dictionary
    """
    try:
        if residual_data_key not in global_state['residual_data']:
            return "âŒ æ®‹å·®æ•°æ®ä¸å­˜åœ¨ï¼", {}

        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸš€ å¼€å§‹è®­ç»ƒ Stage2 Boost æ®‹å·®æ¨¡å‹")
        log_msg.append("=" * 80)

        # Get residual data
        residuals_df = global_state['residual_data'][residual_data_key]['data']
        residual_info = global_state['residual_data'][residual_data_key]['info']

        boundary_signals = residual_info['boundary_signals']
        target_signals = residual_info['target_signals']
        residual_signals = residual_info['residual_signals']

        log_msg.append(f"\nğŸ“Š æ•°æ®ä¿¡æ¯:")
        log_msg.append(f"  æ®‹å·®æ•°æ®: {residual_data_key}")
        log_msg.append(f"  è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}")
        log_msg.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")
        log_msg.append(f"  æ•°æ®é•¿åº¦: {len(residuals_df)}")

        # Prepare training data
        X = residuals_df[boundary_signals].values
        y_residual = residuals_df[residual_signals].values

        # Data split
        train_size = int(len(X) * (1 - config['test_size'] - config['val_size']))
        val_size = int(len(X) * config['val_size'])

        X_train = X[:train_size]
        X_val = X[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]

        y_train = y_residual[:train_size]
        y_val = y_residual[train_size:train_size + val_size]
        y_test = y_residual[train_size + val_size:]

        log_msg.append(f"\nğŸ”€ æ•°æ®åˆ†å‰²:")
        log_msg.append(f"  è®­ç»ƒé›†: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
        log_msg.append(f"  éªŒè¯é›†: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
        log_msg.append(f"  æµ‹è¯•é›†: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

        # Data standardization
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()

        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)

        y_train_scaled = scaler_y.fit_transform(y_train)
        y_val_scaled = scaler_y.transform(y_val)
        y_test_scaled = scaler_y.transform(y_test)

        # Create DataLoader
        train_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_train_scaled),
            torch.FloatTensor(y_train_scaled)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val_scaled),
            torch.FloatTensor(y_val_scaled)
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config['batch_size'],
            shuffle=True
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            shuffle=False
        )

        # Initialize Stage2 model (using SST architecture)
        log_msg.append(f"\nğŸ—ï¸ åˆå§‹åŒ–Stage2æ®‹å·®æ¨¡å‹:")
        log_msg.append(f"  æ¶æ„: StaticSensorTransformer")
        log_msg.append(f"  d_model: {config['d_model']}")
        log_msg.append(f"  nhead: {config['nhead']}")
        log_msg.append(f"  num_layers: {config['num_layers']}")

        stage2_model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            dropout=config['dropout']
        ).to(device)

        # Optimizer and scheduler
        optimizer = torch.optim.AdamW(
            stage2_model.parameters(),
            lr=config['lr'],
            weight_decay=config.get('weight_decay', 1e-5)
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=config.get('scheduler_factor', 0.7),
            patience=config.get('scheduler_patience', 15)
        )
        log_msg.append(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau (factor={config.get('scheduler_factor', 0.7)}, patience={config.get('scheduler_patience', 15)})")

        criterion = nn.MSELoss()

        # Mixed precision training
        scaler = GradScaler()

        # Training loop
        log_msg.append(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (æ··åˆç²¾åº¦, æ€»è½®æ•°: {config['epochs']})")

        history = {
            'train_losses': [],
            'val_losses': [],
            'train_r2': [],
            'val_r2': [],
            'train_mae': [],
            'val_mae': []
        }

        best_val_loss = float('inf')
        patience_counter = 0
        early_stop_patience = config.get('early_stop_patience', 25)

        for epoch in range(config['epochs']):
            # Training phase with mixed precision
            stage2_model.train()
            train_loss = 0.0
            train_preds = []
            train_targets = []

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)

                optimizer.zero_grad()

                # Mixed precision forward pass
                with autocast():
                    outputs = stage2_model(batch_X)
                    loss = criterion(outputs, batch_y)

                # Mixed precision backward pass
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)

                # Gradient clipping
                if config.get('grad_clip', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(stage2_model.parameters(), config['grad_clip'])

                scaler.step(optimizer)
                scaler.update()

                train_loss += loss.item()
                train_preds.append(outputs.detach().cpu().numpy())
                train_targets.append(batch_y.detach().cpu().numpy())

            train_loss /= len(train_loader)
            train_preds = np.vstack(train_preds)
            train_targets = np.vstack(train_targets)
            train_r2, _ = compute_r2_safe(train_targets, train_preds, method='per_output_mean')
            train_mae = mean_absolute_error(train_targets, train_preds)

            # Validation phase with mixed precision
            stage2_model.eval()
            val_loss = 0.0
            val_preds = []
            val_targets = []

            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    with autocast():
                        outputs = stage2_model(batch_X)
                        loss = criterion(outputs, batch_y)

                    val_loss += loss.item()
                    val_preds.append(outputs.cpu().numpy())
                    val_targets.append(batch_y.cpu().numpy())

            val_loss /= len(val_loader)
            val_preds = np.vstack(val_preds)
            val_targets = np.vstack(val_targets)
            val_r2, _ = compute_r2_safe(val_targets, val_preds, method='per_output_mean')
            val_mae = mean_absolute_error(val_targets, val_preds)

            # Record history
            history['train_losses'].append(train_loss)
            history['val_losses'].append(val_loss)
            history['train_r2'].append(train_r2)
            history['val_r2'].append(val_r2)
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            # Learning rate scheduling
            scheduler.step(val_loss)

            # Early stopping check
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0

                # Save best model
                best_model_state = stage2_model.state_dict().copy()
            else:
                patience_counter += 1

            # Progress output (å¢å¼ºç‰ˆ)
            if (epoch + 1) % max(1, config['epochs'] // 20) == 0 or epoch == 0 or epoch == config['epochs'] - 1:
                # è·å–å½“å‰å­¦ä¹ ç‡
                current_lr = optimizer.param_groups[0]['lr']

                # è®¡ç®—RMSE
                train_rmse = np.sqrt(train_loss)
                val_rmse = np.sqrt(val_loss)

                msg = f"\nEpoch {epoch + 1}/{config['epochs']}"
                msg += f"\n  ğŸ“‰ Train: Loss={train_loss:.4f}, RMSE={train_rmse:.4f}, MAE={train_mae:.4f}, RÂ²={train_r2:.4f}"
                msg += f"\n  ğŸ“Š Val:   Loss={val_loss:.4f}, RMSE={val_rmse:.4f}, MAE={val_mae:.4f}, RÂ²={val_r2:.4f}"
                msg += f"\n  ğŸ¯ Val/Train Ratio: {val_loss/train_loss:.2f}x"
                msg += f"\n  ğŸ“š LR: {current_lr:.2e}"
                log_msg.append(msg)

                # Update progress bar with current status
                if progress:
                    progress((epoch + 1) / config['epochs'], desc=f"Epoch {epoch+1}/{config['epochs']} - Val RÂ²: {val_r2:.4f}")

            # Early stopping
            if patience_counter >= early_stop_patience:
                log_msg.append(f"\nâ¸ï¸ æ—©åœè§¦å‘ (Epoch {epoch + 1})")
                break

        # Load best model
        stage2_model.load_state_dict(best_model_state)

        # Test set evaluation with batch inference
        y_test_pred = batch_inference(
            stage2_model, X_test, scaler_X, scaler_y, device,
            batch_size=config['batch_size'], model_name="Stage2"
        )

        test_mae = mean_absolute_error(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        test_r2, _ = compute_r2_safe(y_test, y_test_pred, method='per_output_mean')

        # Training history summary
        log_msg.append(f"\nğŸ“ˆ è®­ç»ƒå†å²æ€»ç»“ ({len(history['train_losses'])} epochs):")
        log_msg.append(f"  æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f} (Epoch {np.argmin(history['val_losses']) + 1})")
        log_msg.append(f"  æœ€ä½³éªŒè¯RÂ²: {max(history['val_r2']):.4f} (Epoch {np.argmax(history['val_r2']) + 1})")
        log_msg.append(f"  æœ€ä½³éªŒè¯MAE: {min(history['val_mae']):.4f} (Epoch {np.argmin(history['val_mae']) + 1})")
        log_msg.append(f"  æœ€ç»ˆè®­ç»ƒLoss: {history['train_losses'][-1]:.4f}")
        log_msg.append(f"  æœ€ç»ˆéªŒè¯Loss: {history['val_losses'][-1]:.4f}")

        log_msg.append(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        log_msg.append(f"  MAE: {test_mae:.6f}")
        log_msg.append(f"  RMSE: {test_rmse:.6f}")
        log_msg.append(f"  RÂ²: {test_r2:.4f}")

        # Save model
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f"Stage2_Boost_{residual_data_key}_{timestamp}"

        model_dir = "saved_models/stage2_boost"
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"{model_name}.pth")
        torch.save({
            'model_state_dict': stage2_model.state_dict(),
            'config': config,
            'history': history,
            'residual_data_key': residual_data_key,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_signals,
            'test_metrics': {
                'mae': test_mae,
                'rmse': test_rmse,
                'r2': test_r2
            }
        }, model_path)

        # Save scalers
        scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
        with open(scaler_path, 'wb') as f:
            pickle.dump({'X': scaler_X, 'y': scaler_y}, f)

        # Save to global state
        global_state['stage2_models'][model_name] = {
            'model': stage2_model,
            'config': config,
            'history': history,
            'residual_data_key': residual_data_key,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_signals,
            'model_path': model_path,
            'scaler_path': scaler_path,
            'test_metrics': {
                'mae': test_mae,
                'rmse': test_rmse,
                'r2': test_r2
            }
        }

        global_state['stage2_scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}

        log_msg.append(f"\nâœ… Stage2æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜:")
        log_msg.append(f"  æ¨¡å‹åç§°: {model_name}")
        log_msg.append(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        log_msg.append(f"  Scalerè·¯å¾„: {scaler_path}")

        results = {
            'model_name': model_name,
            'history': history,
            'test_metrics': {
                'mae': test_mae,
                'rmse': test_rmse,
                'r2': test_r2
            }
        }

        return "\n".join(log_msg), results

    except Exception as e:
        error_msg = f"âŒ Stage2æ¨¡å‹è®­ç»ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, {}


def train_stage2_boost_model_generator(residual_data_key: str, config: Dict[str, Any], progress=None):
    """
    Generator version of train_stage2_boost_model for real-time log updates

    Yields:
        Current log message string after each progress update
    """
    try:
        if residual_data_key not in global_state['residual_data']:
            yield "âŒ æ®‹å·®æ•°æ®ä¸å­˜åœ¨ï¼"
            return

        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸš€ å¼€å§‹è®­ç»ƒ Stage2 Boost æ®‹å·®æ¨¡å‹")
        log_msg.append("=" * 80)

        # Get residual data
        residuals_df = global_state['residual_data'][residual_data_key]['data']
        residual_info = global_state['residual_data'][residual_data_key]['info']

        boundary_signals = residual_info['boundary_signals']
        target_signals = residual_info['target_signals']
        residual_signals = residual_info['residual_signals']

        log_msg.append(f"\nğŸ“Š æ•°æ®ä¿¡æ¯:")
        log_msg.append(f"  æ®‹å·®æ•°æ®: {residual_data_key}")
        log_msg.append(f"  è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}")
        log_msg.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")
        log_msg.append(f"  æ•°æ®é•¿åº¦: {len(residuals_df)}")

        yield "\n".join(log_msg)

        # Prepare training data
        X = residuals_df[boundary_signals].values
        y_residual = residuals_df[residual_signals].values

        # Data split
        train_size = int(len(X) * (1 - config['test_size'] - config['val_size']))
        val_size = int(len(X) * config['val_size'])

        X_train = X[:train_size]
        X_val = X[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]

        y_train = y_residual[:train_size]
        y_val = y_residual[train_size:train_size + val_size]
        y_test = y_residual[train_size + val_size:]

        log_msg.append(f"\nğŸ”€ æ•°æ®åˆ†å‰²:")
        log_msg.append(f"  è®­ç»ƒé›†: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
        log_msg.append(f"  éªŒè¯é›†: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
        log_msg.append(f"  æµ‹è¯•é›†: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

        # Data standardization
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()

        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)

        y_train_scaled = scaler_y.fit_transform(y_train)
        y_val_scaled = scaler_y.transform(y_val)
        y_test_scaled = scaler_y.transform(y_test)

        # Create DataLoader
        train_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_train_scaled),
            torch.FloatTensor(y_train_scaled)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val_scaled),
            torch.FloatTensor(y_val_scaled)
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=config['batch_size'], shuffle=True
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=config['batch_size'], shuffle=False
        )

        # Initialize Stage2 model
        log_msg.append(f"\nğŸ—ï¸ åˆå§‹åŒ–Stage2æ®‹å·®æ¨¡å‹:")
        log_msg.append(f"  æ¶æ„: StaticSensorTransformer")
        log_msg.append(f"  d_model: {config['d_model']}")
        log_msg.append(f"  nhead: {config['nhead']}")
        log_msg.append(f"  num_layers: {config['num_layers']}")

        stage2_model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            dropout=config['dropout']
        ).to(device)

        # Optimizer and scheduler
        optimizer = torch.optim.AdamW(
            stage2_model.parameters(),
            lr=config['lr'],
            weight_decay=config.get('weight_decay', 1e-5)
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min',
            factor=config.get('scheduler_factor', 0.7),
            patience=config.get('scheduler_patience', 15)
        )
        log_msg.append(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau")

        criterion = nn.MSELoss()
        scaler_amp = GradScaler()

        # Training loop
        log_msg.append(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (æ··åˆç²¾åº¦, æ€»è½®æ•°: {config['epochs']})")
        yield "\n".join(log_msg)

        history = {
            'train_losses': [], 'val_losses': [],
            'train_r2': [], 'val_r2': [],
            'train_mae': [], 'val_mae': []
        }

        best_val_loss = float('inf')
        patience_counter = 0
        early_stop_patience = config.get('early_stop_patience', 25)

        for epoch in range(config['epochs']):
            # Check if training should be stopped
            if global_state['stop_training_tab4']:
                log_msg.append(f"\nâš ï¸  è®­ç»ƒåœ¨ Epoch {epoch+1}/{config['epochs']} æ—¶è¢«ç”¨æˆ·ä¸­æ­¢")
                global_state['stop_training_tab4'] = False  # Reset flag
                yield "\n".join(log_msg)
                break

            # Training phase
            stage2_model.train()
            train_loss = 0.0
            train_preds = []
            train_targets = []

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                optimizer.zero_grad()

                with autocast():
                    outputs = stage2_model(batch_X)
                    loss = criterion(outputs, batch_y)

                scaler_amp.scale(loss).backward()
                scaler_amp.unscale_(optimizer)

                if config.get('grad_clip', 0) > 0:
                    torch.nn.utils.clip_grad_norm_(stage2_model.parameters(), config['grad_clip'])

                scaler_amp.step(optimizer)
                scaler_amp.update()

                train_loss += loss.item()
                train_preds.append(outputs.detach().cpu().numpy())
                train_targets.append(batch_y.detach().cpu().numpy())

            train_loss /= len(train_loader)
            train_preds = np.vstack(train_preds)
            train_targets = np.vstack(train_targets)
            train_r2, _ = compute_r2_safe(train_targets, train_preds, method='per_output_mean')
            train_mae = mean_absolute_error(train_targets, train_preds)

            # Validation phase
            stage2_model.eval()
            val_loss = 0.0
            val_preds = []
            val_targets = []

            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    with autocast():
                        outputs = stage2_model(batch_X)
                        loss = criterion(outputs, batch_y)

                    val_loss += loss.item()
                    val_preds.append(outputs.cpu().numpy())
                    val_targets.append(batch_y.cpu().numpy())

            val_loss /= len(val_loader)
            val_preds = np.vstack(val_preds)
            val_targets = np.vstack(val_targets)
            val_r2, _ = compute_r2_safe(val_targets, val_preds, method='per_output_mean')
            val_mae = mean_absolute_error(val_targets, val_preds)

            # Record history
            history['train_losses'].append(train_loss)
            history['val_losses'].append(val_loss)
            history['train_r2'].append(train_r2)
            history['val_r2'].append(val_r2)
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            scheduler.step(val_loss)

            # Early stopping check
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = stage2_model.state_dict().copy()
            else:
                patience_counter += 1

            # Progress output - yield updates periodically
            if (epoch + 1) % max(1, config['epochs'] // 20) == 0 or epoch == 0 or epoch == config['epochs'] - 1:
                current_lr = optimizer.param_groups[0]['lr']
                train_rmse = np.sqrt(train_loss)
                val_rmse = np.sqrt(val_loss)

                msg = f"\nEpoch {epoch + 1}/{config['epochs']}"
                msg += f"\n  ğŸ“‰ Train: Loss={train_loss:.4f}, RMSE={train_rmse:.4f}, MAE={train_mae:.4f}, RÂ²={train_r2:.4f}"
                msg += f"\n  ğŸ“Š Val:   Loss={val_loss:.4f}, RMSE={val_rmse:.4f}, MAE={val_mae:.4f}, RÂ²={val_r2:.4f}"
                msg += f"\n  ğŸ¯ Val/Train Ratio: {val_loss/train_loss:.2f}x"
                msg += f"\n  ğŸ“š LR: {current_lr:.2e}"
                log_msg.append(msg)

                if progress:
                    progress((epoch + 1) / config['epochs'], desc=f"Epoch {epoch+1}/{config['epochs']} - Val RÂ²: {val_r2:.4f}")

                # Yield current log state
                yield "\n".join(log_msg)

            # Early stopping
            if patience_counter >= early_stop_patience:
                log_msg.append(f"\nâ¸ï¸ æ—©åœè§¦å‘ (Epoch {epoch + 1})")
                yield "\n".join(log_msg)
                break

        # Load best model
        stage2_model.load_state_dict(best_model_state)

        # Test set evaluation
        y_test_pred = batch_inference(
            stage2_model, X_test, scaler_X, scaler_y, device,
            batch_size=config['batch_size'], model_name="Stage2"
        )

        test_mae = mean_absolute_error(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        test_r2, _ = compute_r2_safe(y_test, y_test_pred, method='per_output_mean')

        # Training history summary
        log_msg.append(f"\nğŸ“ˆ è®­ç»ƒå†å²æ€»ç»“ ({len(history['train_losses'])} epochs):")
        log_msg.append(f"  æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f} (Epoch {np.argmin(history['val_losses']) + 1})")
        log_msg.append(f"  æœ€ä½³éªŒè¯RÂ²: {max(history['val_r2']):.4f} (Epoch {np.argmax(history['val_r2']) + 1})")
        log_msg.append(f"  æœ€ä½³éªŒè¯MAE: {min(history['val_mae']):.4f} (Epoch {np.argmin(history['val_mae']) + 1})")

        log_msg.append(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        log_msg.append(f"  MAE: {test_mae:.6f}")
        log_msg.append(f"  RMSE: {test_rmse:.6f}")
        log_msg.append(f"  RÂ²: {test_r2:.4f}")

        # Save model
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f"Stage2_Boost_{residual_data_key}_{timestamp}"

        model_dir = "saved_models/stage2_boost"
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"{model_name}.pth")
        torch.save({
            'model_state_dict': stage2_model.state_dict(),
            'config': config,
            'history': history,
            'residual_data_key': residual_data_key,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_signals,
            'test_metrics': {'mae': test_mae, 'rmse': test_rmse, 'r2': test_r2}
        }, model_path)

        # Save scalers
        scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
        with open(scaler_path, 'wb') as f:
            pickle.dump({'X': scaler_X, 'y': scaler_y}, f)

        # Save inference config JSON
        inference_config_path = os.path.join(model_dir, f"{model_name}_inference.json")
        inference_config = {
            'model_name': model_name,
            'model_type': 'Stage2_ResidualTransformer',
            'model_path': model_path,
            'scaler_path': scaler_path,

            # Model architecture
            'architecture': {
                'd_model': config['d_model'],
                'nhead': config['nhead'],
                'num_layers': config['num_layers'],
                'dropout': config['dropout']
            },

            # Data config
            'data_config': {
                'residual_data_key': residual_data_key,
                'num_boundary_sensors': len(boundary_signals),
                'num_target_sensors': len(target_signals)
            },

            # Signal info
            'signals': {
                'boundary_signals': boundary_signals,
                'target_signals': target_signals,
                'residual_signals': residual_signals
            },

            # Training info
            'training_info': {
                'epochs_trained': len(history['train_losses']),
                'best_val_loss': min(history['val_losses']),
                'final_test_mae': test_mae,
                'final_test_rmse': test_rmse,
                'final_test_r2': test_r2,
                'batch_size': config['batch_size'],
                'learning_rate': config['lr']
            },

            'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        with open(inference_config_path, 'w', encoding='utf-8') as f:
            json.dump(inference_config, f, indent=2, ensure_ascii=False)

        # Save to global state
        global_state['stage2_models'][model_name] = {
            'model': stage2_model,
            'config': config,
            'history': history,
            'residual_data_key': residual_data_key,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_signals,
            'model_path': model_path,
            'scaler_path': scaler_path,
            'test_metrics': {'mae': test_mae, 'rmse': test_rmse, 'r2': test_r2}
        }

        global_state['stage2_scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}

        log_msg.append(f"\nâœ… Stage2æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜:")
        log_msg.append(f"  æ¨¡å‹åç§°: {model_name}")
        log_msg.append(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        log_msg.append(f"  Scalerè·¯å¾„: {scaler_path}")
        log_msg.append(f"  æ¨ç†é…ç½®: {inference_config_path}")

        yield "\n".join(log_msg)

    except Exception as e:
        error_msg = f"âŒ Stage2æ¨¡å‹è®­ç»ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        yield error_msg


def create_ensemble_visualization(ensemble_info: Dict[str, Any]):
    """
    ä¸ºç»¼åˆæ¨¡å‹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨

    åŒ…å«ï¼š
    1. æ‰€æœ‰ä¿¡å·çš„RÂ²å¯¹æ¯”æŸ±çŠ¶å›¾
    2. Delta RÂ²åˆ†å¸ƒå›¾
    3. é€‰æ‹©ä¿¡å·çš„é¥¼å›¾
    4. é¢„æµ‹æ•ˆæœå¯¹æ¯”ï¼ˆéšæœºé€‰æ‹©å‡ ä¸ªä¿¡å·ï¼‰
    """
    try:
        import matplotlib.pyplot as plt
        import matplotlib
        matplotlib.use('Agg')

        signal_analysis = ensemble_info['signal_analysis']
        target_signals = ensemble_info['signals']['target']

        # åˆ›å»º2x2å­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'ç»¼åˆæ¨¡å‹åˆ†æ - {ensemble_info["name"]}', fontsize=16, fontweight='bold')

        # å›¾1: RÂ²å¯¹æ¯”æŸ±çŠ¶å›¾
        ax = axes[0, 0]
        signals = [item['signal'] for item in signal_analysis]
        r2_stage1 = [item['r2_stage1'] for item in signal_analysis]
        r2_ensemble = [item['r2_ensemble'] for item in signal_analysis]

        x = np.arange(len(signals))
        width = 0.35

        ax.bar(x - width/2, r2_stage1, width, label='Stage1', alpha=0.8, color='skyblue')
        ax.bar(x + width/2, r2_ensemble, width, label='Ensemble', alpha=0.8, color='orange')

        ax.set_xlabel('ä¿¡å·', fontsize=10)
        ax.set_ylabel('RÂ² åˆ†æ•°', fontsize=10)
        ax.set_title('æ‰€æœ‰ä¿¡å·çš„RÂ²å¯¹æ¯”', fontsize=12, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(signals, rotation=45, ha='right', fontsize=8)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)

        # å›¾2: Delta RÂ²åˆ†å¸ƒå›¾
        ax = axes[0, 1]
        delta_r2_values = [item['delta_r2'] for item in signal_analysis]
        colors = ['green' if item['use_stage2'] else 'gray' for item in signal_analysis]

        bars = ax.barh(signals, delta_r2_values, color=colors, alpha=0.7)
        ax.axvline(x=ensemble_info['delta_r2_threshold'], color='red', linestyle='--',
                   linewidth=2, label=f'é˜ˆå€¼ ({ensemble_info["delta_r2_threshold"]:.3f})')
        ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)

        ax.set_xlabel('Delta RÂ² (Ensemble - Stage1)', fontsize=10)
        ax.set_ylabel('ä¿¡å·', fontsize=10)
        ax.set_title('Delta RÂ²åˆ†å¸ƒ (ç»¿è‰²=ä½¿ç”¨Stage2, ç°è‰²=ä»…Stage1)', fontsize=12, fontweight='bold')
        ax.legend()
        ax.grid(axis='x', alpha=0.3)

        # å›¾3: é€‰æ‹©ç­–ç•¥é¥¼å›¾
        ax = axes[1, 0]
        num_use_stage2 = ensemble_info['num_use_stage2']
        num_use_stage1 = ensemble_info['num_use_stage1_only']

        sizes = [num_use_stage2, num_use_stage1]
        labels = [f'Stage1+Stage2\n({num_use_stage2}ä¸ª)', f'ä»…Stage1\n({num_use_stage1}ä¸ª)']
        colors_pie = ['#ff9999', '#66b3ff']
        explode = (0.05, 0)

        ax.pie(sizes, explode=explode, labels=labels, colors=colors_pie, autopct='%1.1f%%',
               shadow=True, startangle=90, textprops={'fontsize': 11})
        ax.set_title('ä¿¡å·é€‰æ‹©ç­–ç•¥åˆ†å¸ƒ', fontsize=12, fontweight='bold')

        # å›¾4: æ•´ä½“æ€§èƒ½å¯¹æ¯”
        ax = axes[1, 1]
        metrics = ensemble_info['metrics']

        categories = ['MAE', 'RMSE', 'RÂ²']
        stage1_values = [metrics['stage1']['mae'], metrics['stage1']['rmse'], metrics['stage1']['r2']]
        ensemble_values = [metrics['ensemble']['mae'], metrics['ensemble']['rmse'], metrics['ensemble']['r2']]

        # å½’ä¸€åŒ–æ˜¾ç¤ºï¼ˆRÂ²æœ¬èº«å°±æ˜¯0-1ï¼ŒMAEå’ŒRMSEéœ€è¦å½’ä¸€åŒ–ï¼‰
        max_mae_rmse = max(max(stage1_values[:2]), max(ensemble_values[:2]))
        stage1_normalized = [stage1_values[0]/max_mae_rmse, stage1_values[1]/max_mae_rmse, stage1_values[2]]
        ensemble_normalized = [ensemble_values[0]/max_mae_rmse, ensemble_values[1]/max_mae_rmse, ensemble_values[2]]

        x = np.arange(len(categories))
        width = 0.35

        bars1 = ax.bar(x - width/2, stage1_normalized, width, label='Stage1', alpha=0.8, color='skyblue')
        bars2 = ax.bar(x + width/2, ensemble_normalized, width, label='Ensemble', alpha=0.8, color='orange')

        ax.set_ylabel('å½’ä¸€åŒ–å€¼', fontsize=10)
        ax.set_title('æ•´ä½“æ€§èƒ½å¯¹æ¯” (æµ‹è¯•é›†)', fontsize=12, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(categories)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)

        # æ·»åŠ å®é™…å€¼æ ‡æ³¨
        for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
            height1 = bar1.get_height()
            height2 = bar2.get_height()
            ax.text(bar1.get_x() + bar1.get_width()/2., height1,
                   f'{stage1_values[i]:.4f}',
                   ha='center', va='bottom', fontsize=8)
            ax.text(bar2.get_x() + bar2.get_width()/2., height2,
                   f'{ensemble_values[i]:.4f}',
                   ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        return fig

    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        traceback.print_exc()
        return None


def compute_signal_r2_and_select_threshold(
        base_model_name: str,
        stage2_model_name: str,
        delta_r2_threshold: float = 0.05
) -> Tuple[str, Dict[str, Any], Any]:
    """
    ä½¿ç”¨Delta RÂ²ç­–ç•¥ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹ (ä»…åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°)

    æ–°é€»è¾‘ï¼š
    1. ä½¿ç”¨Stage2è®­ç»ƒçš„æµ‹è¯•é›†æ•°æ®
    2. è®¡ç®—æ¯ä¸ªä¿¡å·çš„ Delta RÂ² = RÂ²_ensemble - RÂ²_stage1
    3. å¦‚æœ Delta RÂ² > é˜ˆå€¼ï¼Œè¯´æ˜Stage2æœ‰æ˜¾è‘—æå‡ï¼Œä½¿ç”¨Stage1+Stage2
    4. å¦åˆ™åªä½¿ç”¨Stage1é¢„æµ‹

    Args:
        base_model_name: åŸºç¡€SSTæ¨¡å‹åç§°
        stage2_model_name: Stage2æ®‹å·®æ¨¡å‹åç§°
        delta_r2_threshold: Delta RÂ²é˜ˆå€¼ (é»˜è®¤0.05ï¼Œå³5%æå‡)

    Returns:
        status_msg: çŠ¶æ€ä¿¡æ¯
        ensemble_info: ç»¼åˆæ¨¡å‹ä¿¡æ¯
        fig: å¯è§†åŒ–å›¾è¡¨
    """
    try:
        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸ¯ ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹ (Delta RÂ² ç­–ç•¥)")
        log_msg.append("=" * 80)

        # Check if models exist
        if base_model_name not in global_state['trained_models']:
            return f"âŒ åŸºç¡€æ¨¡å‹ {base_model_name} ä¸å­˜åœ¨ï¼", {}, None

        if stage2_model_name not in global_state['stage2_models']:
            return f"âŒ Stage2æ¨¡å‹ {stage2_model_name} ä¸å­˜åœ¨ï¼", {}, None

        # Get models
        base_model_info = global_state['trained_models'][base_model_name]
        stage2_model_info = global_state['stage2_models'][stage2_model_name]

        base_model = base_model_info['model']
        stage2_model = stage2_model_info['model']
        stage2_config = stage2_model_info['config']

        # Get residual data
        residual_data_key = stage2_model_info['residual_data_key']

        # å¦‚æœåŸå§‹æ®‹å·®æ•°æ®ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ä»»ä½•å¯ç”¨çš„æ®‹å·®æ•°æ®
        if residual_data_key not in global_state['residual_data']:
            available_residual_keys = list(global_state['residual_data'].keys())
            if not available_residual_keys:
                return f"âŒ æ²¡æœ‰å¯ç”¨çš„æ®‹å·®æ•°æ®ï¼è¯·å…ˆåœ¨ Tab3 ä¸­ç”Ÿæˆæ®‹å·®æ•°æ®ã€‚", {}, None

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ®‹å·®æ•°æ®
            residual_data_key = available_residual_keys[0]
            log_msg.append(f"\nâš ï¸  åŸå§‹æ®‹å·®æ•°æ®ä¸å­˜åœ¨ï¼Œä½¿ç”¨: {residual_data_key}")

        residuals_df = global_state['residual_data'][residual_data_key]['data']
        residual_info = global_state['residual_data'][residual_data_key]['info']

        boundary_signals = residual_info['boundary_signals']
        target_signals = residual_info['target_signals']

        # éªŒè¯ä¿¡å·åŒ¹é…
        stage2_boundary = stage2_model_info.get('boundary_signals', boundary_signals)
        stage2_target = stage2_model_info.get('target_signals', target_signals)

        if set(stage2_boundary) != set(boundary_signals):
            log_msg.append(f"\nâš ï¸  è­¦å‘Šï¼šStage2æ¨¡å‹çš„è¾¹ç•Œä¿¡å·ä¸æ®‹å·®æ•°æ®ä¸å®Œå…¨åŒ¹é…")
        if set(stage2_target) != set(target_signals):
            log_msg.append(f"\nâš ï¸  è­¦å‘Šï¼šStage2æ¨¡å‹çš„ç›®æ ‡ä¿¡å·ä¸æ®‹å·®æ•°æ®ä¸å®Œå…¨åŒ¹é…")

        log_msg.append(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        log_msg.append(f"  åŸºç¡€æ¨¡å‹: {base_model_name}")
        log_msg.append(f"  Stage2æ¨¡å‹: {stage2_model_name}")
        log_msg.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")
        log_msg.append(f"  Delta RÂ²é˜ˆå€¼: {delta_r2_threshold:.3f} ({delta_r2_threshold*100:.1f}%)")

        # ä½¿ç”¨Stage2è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®åˆ†å‰²è·å–æµ‹è¯•é›†
        test_size = stage2_config.get('test_size', 0.2)
        val_size = stage2_config.get('val_size', 0.1)

        total_size = len(residuals_df)
        train_size = int(total_size * (1 - test_size - val_size))
        val_size_actual = int(total_size * val_size)
        test_start_idx = train_size + val_size_actual

        log_msg.append(f"\nğŸ”€ æ•°æ®åˆ†å‰² (ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°):")
        log_msg.append(f"  æ€»æ•°æ®: {total_size}")
        log_msg.append(f"  è®­ç»ƒé›†: {train_size} ({train_size/total_size*100:.1f}%)")
        log_msg.append(f"  éªŒè¯é›†: {val_size_actual} ({val_size_actual/total_size*100:.1f}%)")
        log_msg.append(f"  æµ‹è¯•é›†: {total_size - test_start_idx} ({(total_size - test_start_idx)/total_size*100:.1f}%)")

        # æå–æµ‹è¯•é›†æ•°æ®
        y_true_cols = [f"{sig}_true" for sig in target_signals]
        y_pred_cols = [f"{sig}_pred" for sig in target_signals]

        y_true_test = residuals_df[y_true_cols].iloc[test_start_idx:].values
        y_pred_stage1_test = residuals_df[y_pred_cols].iloc[test_start_idx:].values
        X_test = residuals_df[boundary_signals].iloc[test_start_idx:].values

        # ä½¿ç”¨Stage2æ¨¡å‹é¢„æµ‹æµ‹è¯•é›†æ®‹å·®
        y_residual_pred_test = batch_inference(
            stage2_model,
            X_test,
            global_state['stage2_scalers'][stage2_model_name]['X'],
            global_state['stage2_scalers'][stage2_model_name]['y'],
            device,
            batch_size=512,
            model_name="Stage2"
        )

        # è®¡ç®—æ¯ä¸ªä¿¡å·çš„RÂ²åˆ†æ•°
        signal_analysis = []

        for i, signal in enumerate(target_signals):
            y_true_sig = y_true_test[:, i]
            y_pred_stage1_sig = y_pred_stage1_test[:, i]
            y_pred_ensemble_sig = y_pred_stage1_sig + y_residual_pred_test[:, i]

            # è®¡ç®—Stage1çš„RÂ²
            r2_stage1, _ = compute_r2_safe(
                y_true_sig.reshape(-1, 1),
                y_pred_stage1_sig.reshape(-1, 1),
                method='per_output_mean'
            )

            # è®¡ç®—Ensembleçš„RÂ²
            r2_ensemble, _ = compute_r2_safe(
                y_true_sig.reshape(-1, 1),
                y_pred_ensemble_sig.reshape(-1, 1),
                method='per_output_mean'
            )

            # è®¡ç®—Delta RÂ²
            delta_r2 = r2_ensemble - r2_stage1

            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨Stage2
            use_stage2 = delta_r2 > delta_r2_threshold

            signal_analysis.append({
                'signal': signal,
                'r2_stage1': float(r2_stage1),
                'r2_ensemble': float(r2_ensemble),
                'delta_r2': float(delta_r2),
                'use_stage2': bool(use_stage2)
            })

        # ç»Ÿè®¡ä½¿ç”¨Stage2çš„ä¿¡å·æ•°
        num_use_stage2 = sum(1 for item in signal_analysis if item['use_stage2'])
        num_use_stage1_only = len(target_signals) - num_use_stage2

        log_msg.append(f"\nğŸ¯ ä¿¡å·Delta RÂ²åˆ†æ:")
        log_msg.append(f"{'ä¿¡å·åç§°':<30} {'Stage1 RÂ²':>12} {'Ensemble RÂ²':>12} {'Delta RÂ²':>12} {'é€‰æ‹©':>10}")
        log_msg.append("-" * 80)

        for item in signal_analysis:
            choice = "Stage1+2" if item['use_stage2'] else "Stage1"
            log_msg.append(
                f"{item['signal']:<30} {item['r2_stage1']:>12.4f} {item['r2_ensemble']:>12.4f} "
                f"{item['delta_r2']:>12.4f} {choice:>10}"
            )

        log_msg.append("-" * 80)
        log_msg.append(f"ä½¿ç”¨Stage1+Stage2: {num_use_stage2} ä¸ªä¿¡å·")
        log_msg.append(f"ä»…ä½¿ç”¨Stage1: {num_use_stage1_only} ä¸ªä¿¡å·")

        # ç”Ÿæˆæœ€ç»ˆç»¼åˆé¢„æµ‹ï¼ˆåœ¨æµ‹è¯•é›†ä¸Šï¼‰
        y_ensemble_test = y_pred_stage1_test.copy()
        for i, item in enumerate(signal_analysis):
            if item['use_stage2']:
                y_ensemble_test[:, i] = y_pred_stage1_test[:, i] + y_residual_pred_test[:, i]

        # è®¡ç®—æ•´ä½“æ€§èƒ½
        mae_stage1 = mean_absolute_error(y_true_test, y_pred_stage1_test)
        mae_ensemble = mean_absolute_error(y_true_test, y_ensemble_test)
        rmse_stage1 = np.sqrt(mean_squared_error(y_true_test, y_pred_stage1_test))
        rmse_ensemble = np.sqrt(mean_squared_error(y_true_test, y_ensemble_test))
        r2_stage1, _ = compute_r2_safe(y_true_test, y_pred_stage1_test, method='per_output_mean')
        r2_ensemble, _ = compute_r2_safe(y_true_test, y_ensemble_test, method='per_output_mean')

        improvement_mae = (mae_stage1 - mae_ensemble) / mae_stage1 * 100 if mae_stage1 > 0 else 0
        improvement_rmse = (rmse_stage1 - rmse_ensemble) / rmse_stage1 * 100 if rmse_stage1 > 0 else 0
        improvement_r2 = (r2_ensemble - r2_stage1) / (1 - r2_stage1) * 100 if r2_stage1 < 1 else 0

        log_msg.append(f"\nğŸ“ˆ æ•´ä½“æ€§èƒ½å¯¹æ¯” (æµ‹è¯•é›†):")
        log_msg.append(f"{'æŒ‡æ ‡':<15} {'Stage1':>15} {'Ensemble':>15} {'æ”¹è¿›':>15}")
        log_msg.append("-" * 65)
        log_msg.append(f"{'MAE':<15} {mae_stage1:>15.6f} {mae_ensemble:>15.6f} {improvement_mae:>14.2f}%")
        log_msg.append(f"{'RMSE':<15} {rmse_stage1:>15.6f} {rmse_ensemble:>15.6f} {improvement_rmse:>14.2f}%")
        log_msg.append(f"{'RÂ²':<15} {r2_stage1:>15.4f} {r2_ensemble:>15.4f} {improvement_r2:>14.2f}%")

        # ä¿å­˜ç»¼åˆæ¨¡å‹ä¿¡æ¯
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        ensemble_name = f"Ensemble_{base_model_name}_{timestamp}"

        ensemble_info = {
            'name': ensemble_name,
            'base_model_name': base_model_name,
            'stage2_model_name': stage2_model_name,
            'delta_r2_threshold': float(delta_r2_threshold),
            'signal_analysis': signal_analysis,
            'num_use_stage2': int(num_use_stage2),
            'num_use_stage1_only': int(num_use_stage1_only),
            'metrics': {
                'stage1': {
                    'mae': float(mae_stage1),
                    'rmse': float(rmse_stage1),
                    'r2': float(r2_stage1)
                },
                'ensemble': {
                    'mae': float(mae_ensemble),
                    'rmse': float(rmse_ensemble),
                    'r2': float(r2_ensemble)
                },
                'improvement': {
                    'mae_pct': float(improvement_mae),
                    'rmse_pct': float(improvement_rmse),
                    'r2_pct': float(improvement_r2)
                }
            },
            'predictions': {
                'y_true': y_true_test,
                'y_pred_stage1': y_pred_stage1_test,
                'y_pred_ensemble': y_ensemble_test,
                'y_residual_pred': y_residual_pred_test
            },
            'signals': {
                'boundary': boundary_signals,
                'target': target_signals
            },
            'data_split': {
                'test_size': float(test_size),
                'val_size': float(val_size),
                'test_start_idx': int(test_start_idx),
                'test_samples': int(len(y_true_test))
            },
            'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        global_state['ensemble_models'][ensemble_name] = ensemble_info

        # ä¿å­˜é…ç½®æ–‡ä»¶
        ensemble_dir = "saved_models/ensemble"
        os.makedirs(ensemble_dir, exist_ok=True)

        config_path = os.path.join(ensemble_dir, f"{ensemble_name}_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            # ä¿å­˜é…ç½®ï¼ˆæ’é™¤å¤§æ•°ç»„ï¼Œç¡®ä¿æ‰€æœ‰ç±»å‹å¯JSONåºåˆ—åŒ–ï¼‰
            save_config = {
                'name': ensemble_name,
                'base_model_name': base_model_name,
                'stage2_model_name': stage2_model_name,
                'delta_r2_threshold': float(delta_r2_threshold),
                'signal_analysis': signal_analysis,  # å·²è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                'num_use_stage2': int(num_use_stage2),
                'num_use_stage1_only': int(num_use_stage1_only),
                'metrics': ensemble_info['metrics'],  # å·²è½¬æ¢
                'signals': ensemble_info['signals'],
                'data_split': ensemble_info['data_split'],
                'created_time': ensemble_info['created_time']
            }
            json.dump(save_config, f, indent=2, ensure_ascii=False)

        # ç”Ÿæˆæ±‡æ€»CSVæ–‡ä»¶
        csv_path = os.path.join(ensemble_dir, f"{ensemble_name}_summary.csv")
        summary_data = []
        for item in signal_analysis:
            summary_data.append({
                'ä¿¡å·åç§°': item['signal'],
                'Stage1_R2': item['r2_stage1'],
                'Ensemble_R2': item['r2_ensemble'],
                'Delta_R2': item['delta_r2'],
                'R2æå‡(%)': item['delta_r2'] * 100,
                'é€‰æ‹©æ¨¡å‹': 'Stage1+Stage2' if item['use_stage2'] else 'Stage1',
                'æ˜¯å¦ä½¿ç”¨Stage2': 'Yes' if item['use_stage2'] else 'No'
            })

        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        log_msg.append(f"\nâœ… ç»¼åˆæ¨ç†æ¨¡å‹å·²ç”Ÿæˆ:")
        log_msg.append(f"  æ¨¡å‹åç§°: {ensemble_name}")
        log_msg.append(f"  é…ç½®è·¯å¾„: {config_path}")
        log_msg.append(f"  æ±‡æ€»CSV: {csv_path}")

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        fig = create_ensemble_visualization(ensemble_info)

        return "\n".join(log_msg), ensemble_info, fig

    except Exception as e:
        error_msg = f"âŒ ç»¼åˆæ¨¡å‹ç”Ÿæˆå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, {}, None


# ============================================================================
# Data loading functions

def load_data_from_csv(file_obj):
    """Load data from CSV file"""
    try:
        df = pd.read_csv(file_obj.name)

        # If there are unnamed columns, set as index
        if 'Unnamed: 0' in df.columns:
            df = df.set_index('Unnamed: 0')
            df.index.name = 'index'
        elif df.index.name is None:
            df.index.name = 'index'

        global_state['df'] = df
        global_state['all_signals'] = list(df.columns)

        status = f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!\n\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ ç‰¹å¾æ•°: {len(df.columns)}\n\n"
        status += f"å‰5åˆ—: {', '.join(df.columns[:5].tolist())}"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display

    except Exception as e:
        error_msg = f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"
        return error_msg, None, ""


def get_available_csv_files():
    """
    Get list of available CSV files in data/ folder

    Returns:
        List of CSV file paths (safe - never raises exceptions)
    """
    try:
        import glob

        csv_files = []

        # Search in data/ folder
        if os.path.exists('data'):
            csv_files.extend(glob.glob('data/*.csv'))

        # Search in current directory
        csv_files.extend(glob.glob('*.csv'))

        # Sort by modification time (newest first)
        csv_files = sorted(csv_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return csv_files if csv_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_available_csv_files: {e}")
        return []  # Return empty list on error


def load_csv_from_path(csv_path):
    """
    Load CSV file from a given path

    Args:
        csv_path: Path to CSV file

    Returns:
        status: Status message
        preview_df: Data preview (first 100 rows)
        signals: Available signals
    """
    if not csv_path or csv_path == "(no CSV files found)":
        return "âŒ è¯·é€‰æ‹©æœ‰æ•ˆçš„CSVæ–‡ä»¶", None, ""

    if not os.path.exists(csv_path):
        return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}", None, ""

    try:
        df = pd.read_csv(csv_path)

        # If there are unnamed columns, set as index
        if 'Unnamed: 0' in df.columns:
            df = df.set_index('Unnamed: 0')
            df.index.name = 'index'
        elif df.index.name is None:
            df.index.name = 'index'

        global_state['df'] = df
        global_state['all_signals'] = list(df.columns)

        status = f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!\n\n"
        status += f"ğŸ“ æ–‡ä»¶: {csv_path}\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ ç‰¹å¾æ•°: {len(df.columns)}\n\n"
        status += f"å‰5åˆ—: {', '.join(df.columns[:5].tolist())}"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display

    except Exception as e:
        error_msg = f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"
        return error_msg, None, ""


def check_preloaded_data():
    """
    Check if data was pre-loaded (from Colab) and return its status

    Returns:
        status: Status message
        preview_df: Data preview (first 100 rows)
        signals_display: Available signals
    """
    if global_state.get('df') is not None:
        df = global_state['df']

        status = f"âœ… [é¢„åŠ è½½] æ•°æ®å·²åŠ è½½!\n\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ ç‰¹å¾æ•°: {len(df.columns)}\n\n"
        status += f"åˆ—å: {', '.join(df.columns[:5].tolist())}"
        if len(df.columns) > 5:
            status += f"... (å…±{len(df.columns)}åˆ—)"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display
    else:
        return "âš ï¸ å°šæœªåŠ è½½æ•°æ®ï¼Œè¯·é€‰æ‹©CSVæ–‡ä»¶ã€ä¸Šä¼ æ–‡ä»¶æˆ–åˆ›å»ºç¤ºä¾‹æ•°æ®", None, ""


def load_signals_config_from_json(json_file):
    """
    Load boundary and target signals configuration from JSON file

    Args:
        json_file: Gradio File object or file path

    Returns:
        boundary_signals: List of boundary signal names
        target_signals: List of target signal names
        status_message: Status message
    """
    try:
        # Handle Gradio File object or direct path
        if hasattr(json_file, 'name'):
            file_path = json_file.name
        else:
            file_path = json_file

        if not file_path:
            return [], [], "âŒ è¯·ä¸Šä¼ JSONé…ç½®æ–‡ä»¶"

        if not os.path.exists(file_path):
            return [], [], f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"

        # Load JSON
        with open(file_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        boundary_signals = config.get('boundary_signals', [])
        target_signals = config.get('target_signals', [])

        if not boundary_signals or not target_signals:
            return [], [], "âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ 'boundary_signals' æˆ– 'target_signals'"

        status = f"âœ… JSONé…ç½®åŠ è½½æˆåŠŸ!\n\n"
        status += f"ğŸ“¥ è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}\n"
        status += f"ğŸ“¤ ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}\n"
        status += f"ğŸ“ æ–‡ä»¶: {os.path.basename(file_path)}"

        return boundary_signals, target_signals, status

    except json.JSONDecodeError as e:
        return [], [], f"âŒ JSONè§£æå¤±è´¥: {str(e)}"
    except Exception as e:
        return [], [], f"âŒ åŠ è½½å¤±è´¥: {str(e)}"


def get_available_json_configs():
    """
    Get list of available JSON config files in data/ folder

    Returns:
        List of JSON file paths
    """
    try:
        import glob

        json_files = []

        # Search in data/ folder
        if os.path.exists('data'):
            json_files.extend(glob.glob('data/*.json'))

        # Search in current directory
        json_files.extend(glob.glob('*.json'))

        # Sort by modification time (newest first)
        json_files = sorted(json_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return json_files if json_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_available_json_configs: {e}")
        return []


def create_sample_data():
    """Create sample data"""
    try:
        np.random.seed(42)
        n_samples = 10000
        n_boundary = 10
        n_target = 5

        # Generate correlated sensor data
        X = np.random.randn(n_samples, n_boundary)
        y = X[:, :n_target] + 0.5 * X[:, :n_target] ** 2 + 0.1 * np.random.randn(n_samples, n_target)

        boundary_cols = [f"boundary_{i + 1}" for i in range(n_boundary)]
        target_cols = [f"target_{i + 1}" for i in range(n_target)]

        df = pd.DataFrame(
            np.column_stack([X, y]),
            columns=boundary_cols + target_cols
        )

        df.index.name = 'index'
        global_state['df'] = df
        global_state['all_signals'] = list(df.columns)

        status = f"âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºæˆåŠŸ!\n\n"
        status += f"ğŸ“Š æ•°æ®ç»´åº¦: {df.shape}\n"
        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(df):,}\n"
        status += f"ğŸ¯ è¾¹ç•Œä¿¡å·: {n_boundary}ä¸ª\n"
        status += f"ğŸ¯ ç›®æ ‡ä¿¡å·: {n_target}ä¸ª\n\n"
        status += "ğŸ’¡ æç¤º: ç¤ºä¾‹æ•°æ®æ¨¡æ‹Ÿäº†ä¼ æ„Ÿå™¨ä¹‹é—´çš„éçº¿æ€§å…³ç³»"

        signals_display = f"å¯ç”¨ä¿¡å· ({len(df.columns)}ä¸ª):\n" + ", ".join(df.columns.tolist())

        # Data preview (first 100 rows)
        preview_df = df.head(100)

        return status, preview_df, signals_display

    except Exception as e:
        error_msg = f"âŒ ç¤ºä¾‹æ•°æ®åˆ›å»ºå¤±è´¥: {str(e)}"
        return error_msg, None, ""


# ============================================================================
# SST model training functions

def train_base_model_ui(
        boundary_signals, target_signals, model_type,
        epochs, batch_size, lr,
        d_model, nhead, num_layers, dropout,
        test_size, val_size,
        weight_decay, scheduler_patience, scheduler_factor, grad_clip_norm,
        temporal_signals=None, apply_smoothing=False,
        progress=gr.Progress()
):
    """UI function for training base model"""
    try:
        if global_state['df'] is None:
            return "âŒ è¯·å…ˆåŠ è½½æ•°æ®ï¼"

        if not boundary_signals or not target_signals:
            return "âŒ è¯·é€‰æ‹©è¾¹ç•Œä¿¡å·å’Œç›®æ ‡ä¿¡å·ï¼"

        log_messages = []
        log_messages.append("=" * 80)
        log_messages.append(f"ğŸš€ å¼€å§‹è®­ç»ƒ {model_type}")
        log_messages.append("=" * 80)
        log_messages.append(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
        log_messages.append(f"  æ¨¡å‹ç±»å‹: {model_type}")
        log_messages.append(f"  è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}")
        log_messages.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")
        log_messages.append(f"  è®­ç»ƒè½®æ•°: {epochs}")
        log_messages.append(f"  æ‰¹å¤§å°: {batch_size}")
        log_messages.append(f"  å­¦ä¹ ç‡: {lr}")

        df = global_state['df']

        # Prepare data
        X = df[boundary_signals].values
        y = df[target_signals].values

        # Apply IFD smoothing (if needed)
        if apply_smoothing and temporal_signals:
            log_messages.append(f"\nğŸ”§ åº”ç”¨IFDå¹³æ»‘...")
            # Apply smoothing to the full y array for specified temporal signals
            y_smoothed = apply_ifd_smoothing(
                y_data=y,
                target_sensors=target_signals,
                ifd_sensor_names=temporal_signals,
                window_length=15,
                polyorder=3
            )
            # Update y with smoothed values
            y = y_smoothed
            # Update df with smoothed target signals
            for i, sig in enumerate(target_signals):
                df[sig] = y[:, i]
            log_messages.append(f"  å·²å¯¹ {len(temporal_signals)} ä¸ªæ—¶åºä¿¡å·åº”ç”¨å¹³æ»‘")

        # Data split
        train_size = int(len(X) * (1 - test_size - val_size))
        val_size_samples = int(len(X) * val_size)

        X_train = X[:train_size]
        X_val = X[train_size:train_size + val_size_samples]
        X_test = X[train_size + val_size_samples:]

        y_train = y[:train_size]
        y_val = y[train_size:train_size + val_size_samples]
        y_test = y[train_size + val_size_samples:]

        log_messages.append(f"\nğŸ”€ æ•°æ®åˆ†å‰²:")
        log_messages.append(f"  è®­ç»ƒé›†: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
        log_messages.append(f"  éªŒè¯é›†: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
        log_messages.append(f"  æµ‹è¯•é›†: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

        # Data standardization
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()

        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)

        y_train_scaled = scaler_y.fit_transform(y_train)
        y_val_scaled = scaler_y.transform(y_val)
        y_test_scaled = scaler_y.transform(y_test)

        # Create DataLoader
        train_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_train_scaled),
            torch.FloatTensor(y_train_scaled)
        )
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val_scaled),
            torch.FloatTensor(y_val_scaled)
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False
        )

        # Initialize model
        log_messages.append(f"\nğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹: {model_type}")

        if model_type == 'StaticSensorTransformer':
            model = StaticSensorTransformer(
                num_boundary_sensors=len(boundary_signals),
                num_target_sensors=len(target_signals),
                d_model=d_model,
                nhead=nhead,
                num_layers=num_layers,
                dropout=dropout
            ).to(device)
        else:
            return f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"

        # Optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience
        )
        log_messages.append(f"ğŸ“Š ä¼˜åŒ–å™¨: AdamW (lr={lr:.2e}, weight_decay={weight_decay:.2e})")
        log_messages.append(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau (factor={scheduler_factor}, patience={scheduler_patience})")
        log_messages.append(f"âœ‚ï¸ æ¢¯åº¦è£å‰ª: {grad_clip_norm}")
        criterion = nn.MSELoss()

        # Mixed precision training
        scaler = GradScaler()

        # Training loop
        log_messages.append(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ (æ··åˆç²¾åº¦)...")
        history = {
            'train_losses': [],
            'val_losses': [],
            'train_r2': [],
            'val_r2': [],
            'train_mae': [],
            'val_mae': []
        }
        best_val_loss = float('inf')
        patience_counter = 0
        early_stop_patience = 25

        for epoch in range(epochs):
            # Check if training should be stopped
            if global_state['stop_training_tab2']:
                log_messages.append(f"\nâš ï¸  è®­ç»ƒåœ¨ Epoch {epoch+1}/{epochs} æ—¶è¢«ç”¨æˆ·ä¸­æ­¢")
                global_state['stop_training_tab2'] = False  # Reset flag
                break

            # Training with mixed precision
            model.train()
            train_loss = 0.0
            train_preds = []
            train_targets = []
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                optimizer.zero_grad()

                # Mixed precision forward pass
                with autocast():
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)

                # Mixed precision backward pass
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                scaler.step(optimizer)
                scaler.update()

                train_loss += loss.item()
                train_preds.append(outputs.detach().cpu().numpy())
                train_targets.append(batch_y.detach().cpu().numpy())

            train_loss /= len(train_loader)

            # Calculate training metrics
            train_preds_arr = np.vstack(train_preds)
            train_targets_arr = np.vstack(train_targets)

            # Inverse transform to original space for metrics
            train_preds_orig = scaler_y.inverse_transform(train_preds_arr)
            train_targets_orig = scaler_y.inverse_transform(train_targets_arr)
            train_r2, _ = compute_r2_safe(train_targets_orig, train_preds_orig, method='per_output_mean')
            train_mae = mean_absolute_error(train_targets_orig, train_preds_orig)

            # Validation with mixed precision
            model.eval()
            val_loss = 0.0
            val_preds = []
            val_targets = []
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    with autocast():
                        outputs = model(batch_X)
                        loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    val_preds.append(outputs.cpu().numpy())
                    val_targets.append(batch_y.cpu().numpy())

            val_loss /= len(val_loader)

            # Calculate validation metrics
            val_preds_arr = np.vstack(val_preds)
            val_targets_arr = np.vstack(val_targets)

            # Inverse transform to original space for metrics
            val_preds_orig = scaler_y.inverse_transform(val_preds_arr)
            val_targets_orig = scaler_y.inverse_transform(val_targets_arr)
            val_r2, _ = compute_r2_safe(val_targets_orig, val_preds_orig, method='per_output_mean')
            val_mae = mean_absolute_error(val_targets_orig, val_preds_orig)

            history['train_losses'].append(train_loss)
            history['val_losses'].append(val_loss)
            history['train_r2'].append(train_r2)
            history['val_r2'].append(val_r2)
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            scheduler.step(val_loss)

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1

            # è¿›åº¦æ˜¾ç¤º (å¢å¼ºç‰ˆ - æ˜¾ç¤ºMAE, RMSE, R2å’Œå­¦ä¹ ç‡)
            if (epoch + 1) % max(1, epochs // 20) == 0 or epoch == 0:
                # è·å–å½“å‰å­¦ä¹ ç‡
                current_lr = optimizer.param_groups[0]['lr']

                # è®¡ç®—RMSE (æ›´ç›´è§‚)
                train_rmse = np.sqrt(train_loss)
                val_rmse = np.sqrt(val_loss)

                msg = f"\nEpoch {epoch + 1}/{epochs}"
                msg += f"\n  ğŸ“‰ Train: Loss={train_loss:.4f}, RMSE={train_rmse:.4f}, MAE={train_mae:.4f}, RÂ²={train_r2:.4f}"
                msg += f"\n  ğŸ“Š Val:   Loss={val_loss:.4f}, RMSE={val_rmse:.4f}, MAE={val_mae:.4f}, RÂ²={val_r2:.4f}"
                msg += f"\n  ğŸ¯ Val/Train Ratio: {val_loss/train_loss:.2f}x"
                msg += f"\n  ğŸ“š LR: {current_lr:.2e}"

                log_messages.append(msg)
                progress((epoch + 1) / epochs, desc=f"Epoch {epoch+1}/{epochs} - Val RÂ²: {val_r2:.4f}")

            if patience_counter >= early_stop_patience:
                log_messages.append(f"\nâ¸ï¸ æ—©åœè§¦å‘ (Epoch {epoch + 1})")
                break

        # Load best model
        model.load_state_dict(best_model_state)

        # Test set evaluation with mixed precision and batch inference
        y_test_pred = batch_inference(
            model, X_test, scaler_X, scaler_y, device,
            batch_size=batch_size, model_name=model_type
        )

        test_mae = mean_absolute_error(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        test_r2, _ = compute_r2_safe(y_test, y_test_pred, method='per_output_mean')

        # Training history summary
        log_messages.append(f"\nğŸ“ˆ è®­ç»ƒå†å²æ€»ç»“ ({len(history['train_losses'])} epochs):")
        log_messages.append(f"  æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f} (Epoch {np.argmin(history['val_losses']) + 1})")
        log_messages.append(f"  æœ€ä½³éªŒè¯RÂ²: {max(history['val_r2']):.4f} (Epoch {np.argmax(history['val_r2']) + 1})")
        log_messages.append(f"  æœ€ä½³éªŒè¯MAE: {min(history['val_mae']):.4f} (Epoch {np.argmin(history['val_mae']) + 1})")
        log_messages.append(f"  æœ€ç»ˆè®­ç»ƒLoss: {history['train_losses'][-1]:.4f}")
        log_messages.append(f"  æœ€ç»ˆéªŒè¯Loss: {history['val_losses'][-1]:.4f}")

        log_messages.append(f"\nğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
        log_messages.append(f"  MAE: {test_mae:.6f}")
        log_messages.append(f"  RMSE: {test_rmse:.6f}")
        log_messages.append(f"  RÂ²: {test_r2:.4f}")

        # Save model
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f"{model_type}_{timestamp}"

        model_dir = "saved_models"
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"{model_name}.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': {
                'type': model_type,
                'boundary_signals': boundary_signals,
                'target_signals': target_signals,
                'config': {
                    'd_model': d_model,
                    'nhead': nhead,
                    'num_layers': num_layers,
                    'dropout': dropout,
                    'batch_size': batch_size
                }
            },
            'training_history': history
        }, model_path)

        # Save scalers
        scaler_path = os.path.join(model_dir, f"{model_name}_scalers.pkl")
        with open(scaler_path, 'wb') as f:
            pickle.dump({'X': scaler_X, 'y': scaler_y}, f)

        # ä¿å­˜æ¨ç†é…ç½®
        save_inference_config(
            model_name, model_type, model_path, scaler_path,
            boundary_signals, target_signals,
            {
                'd_model': d_model,
                'nhead': nhead,
                'num_layers': num_layers,
                'dropout': dropout
            }
        )

        # Save to global state
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': model_type,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'config': {
                'd_model': d_model,
                'nhead': nhead,
                'num_layers': num_layers,
                'dropout': dropout,
                'batch_size': batch_size
            },
            'model_path': model_path,
            'scaler_path': scaler_path
        }

        global_state['scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}

        log_messages.append(f"\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜:")
        log_messages.append(f"  æ¨¡å‹åç§°: {model_name}")
        log_messages.append(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        log_messages.append(f"  Scalerè·¯å¾„: {scaler_path}")

        return "\n".join(log_messages)

    except Exception as e:
        error_msg = f"âŒ è®­ç»ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg


# ============================================================================
# Residual extraction functions

def get_inference_config_files():
    """
    Get list of inference config JSON files in saved_models folder

    Returns:
        List of inference config file paths
    """
    try:
        import glob

        config_files = []

        # Search in saved_models folder and subdirectories
        if os.path.exists('saved_models'):
            config_files.extend(glob.glob('saved_models/**/*_inference.json', recursive=True))

        # Sort by modification time (newest first)
        config_files = sorted(config_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return config_files if config_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_inference_config_files: {e}")
        return []


def get_scalers_files():
    """
    Get list of scaler pkl files in saved_models folder

    Returns:
        List of scaler file paths
    """
    try:
        import glob

        scaler_files = []

        # Search in saved_models folder and subdirectories
        if os.path.exists('saved_models'):
            scaler_files.extend(glob.glob('saved_models/**/*_scalers.pkl', recursive=True))

        # Sort by modification time (newest first)
        scaler_files = sorted(scaler_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return scaler_files if scaler_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_scalers_files: {e}")
        return []


def get_model_files():
    """
    Get list of model pth files in saved_models folder

    Returns:
        List of model file paths
    """
    try:
        import glob

        model_files = []

        # Search in saved_models folder and subdirectories
        if os.path.exists('saved_models'):
            # Get all .pth files, excluding scalers
            all_pth_files = glob.glob('saved_models/**/*.pth', recursive=True)
            # Filter out files that are not model files (e.g., optimizer states)
            model_files = [f for f in all_pth_files if not f.endswith('_scalers.pth')]

        # Sort by modification time (newest first)
        model_files = sorted(model_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return model_files if model_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_model_files: {e}")
        return []


def load_model_from_inference_config_path(config_path):
    """
    Load model from inference config file path

    Args:
        config_path: Path to inference config JSON file

    Returns:
        model_name: Loaded model name
        status: Status message
    """
    try:
        if not config_path:
            return None, "âŒ è¯·é€‰æ‹©æ¨ç†é…ç½®æ–‡ä»¶"

        if not os.path.exists(config_path):
            return None, f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"

        # Load config
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        model_name = config.get('model_name')
        if not model_name:
            return None, "âŒ é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ model_name"

        status = f"âœ… é…ç½®åŠ è½½æˆåŠŸ!\n\n"
        status += f"ğŸ“ é…ç½®æ–‡ä»¶: {os.path.basename(config_path)}\n"
        status += f"ğŸ¤– æ¨¡å‹åç§°: {model_name}\n"
        status += f"ğŸ“¥ è¾¹ç•Œä¿¡å·æ•°: {len(config.get('boundary_signals', []))}\n"
        status += f"ğŸ“¤ ç›®æ ‡ä¿¡å·æ•°: {len(config.get('target_signals', []))}"

        return model_name, status

    except json.JSONDecodeError as e:
        return None, f"âŒ JSONè§£æå¤±è´¥: {str(e)}"
    except Exception as e:
        return None, f"âŒ åŠ è½½å¤±è´¥: {str(e)}"


def load_scalers_from_path(scaler_path, model_name):
    """
    Load scalers from a pickle file path for a specific model

    Args:
        scaler_path: Path to scaler pickle file
        model_name: Model name to associate the scalers with

    Returns:
        status_msg: Status message
    """
    try:
        if not scaler_path:
            return "âŒ è¯·é€‰æ‹©scalersæ–‡ä»¶ï¼"

        if not model_name:
            return "âŒ è¯·å…ˆé€‰æ‹©æ¨¡å‹ï¼"

        if not os.path.exists(scaler_path):
            return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}"

        # Load scalers from file
        with open(scaler_path, 'rb') as f:
            scalers = pickle.load(f)

        # Save to global state
        if 'manual_scalers' not in global_state:
            global_state['manual_scalers'] = {}

        global_state['manual_scalers'][model_name] = scalers

        success_msg = f"âœ… ScalersåŠ è½½æˆåŠŸ!\n\n"
        success_msg += f"ğŸ“Œ Model name: {model_name}\n"
        success_msg += f"ğŸ“Š ScalersåŒ…å«: {list(scalers.keys())}\n"

        # Verify scalers have required keys
        if 'X' in scalers and 'y' in scalers:
            success_msg += f"âœ“ åŒ…å«å¿…éœ€çš„Xå’Œy scalers\n"
        else:
            success_msg += f"âš ï¸ è­¦å‘Š: scalerså¯èƒ½ç¼ºå°‘Xæˆ–yé”®\n"

        print(success_msg)
        return success_msg

    except Exception as e:
        error_msg = f"âŒ ScalersåŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg


def load_model_from_path(model_path):
    """
    Load SST model from a .pth file path

    Args:
        model_path: Path to model .pth file

    Returns:
        model_name: Loaded model name
        status_msg: Status message
    """
    try:
        if not model_path:
            return None, "âŒ è¯·é€‰æ‹©æ¨¡å‹æ–‡ä»¶ï¼"

        if not os.path.exists(model_path):
            return None, f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"

        # Extract model name from path
        model_name = os.path.splitext(os.path.basename(model_path))[0]

        # Load checkpoint
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        if 'model_config' not in checkpoint:
            return None, f"âŒ æ¨¡å‹æ–‡ä»¶æ ¼å¼é”™è¯¯: ç¼ºå°‘model_config"

        model_config = checkpoint['model_config']

        if model_config.get('type') != 'StaticSensorTransformer':
            return None, f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_config.get('type')}"

        boundary_signals = model_config['boundary_signals']
        target_signals = model_config['target_signals']
        config = model_config['config']

        # Create model
        model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            dropout=config['dropout']
        ).to(device)

        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        # Try to load scalers from checkpoint
        scalers = None
        scaler_source = "æœªåŠ è½½"
        if 'scalers' in checkpoint:
            scalers = checkpoint['scalers']
            scaler_source = "ä»checkpointåŠ è½½"
            # Also save to manual_scalers for consistency
            if 'manual_scalers' not in global_state:
                global_state['manual_scalers'] = {}
            global_state['manual_scalers'][model_name] = scalers

        # Save to global state
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': model_config['type'],
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'config': config,
            'model_path': model_path,
            'scaler_path': model_path.replace('.pth', '_scalers.pkl')
        }

        success_msg = f"âœ… SSTæ¨¡å‹åŠ è½½æˆåŠŸ!\n\n"
        success_msg += f"ğŸ“Œ Model name: {model_name}\n"
        success_msg += f"ğŸ“Š Model type: {model_config['type']}\n"
        success_msg += f"ğŸ¯ è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}\n"
        success_msg += f"ğŸ“ˆ ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}\n"
        success_msg += f"âš™ï¸ æ¨¡å‹å‚æ•°: d_model={config['d_model']}, nhead={config['nhead']}, layers={config['num_layers']}\n"
        success_msg += f"ğŸ“Š ScalersçŠ¶æ€: {scaler_source}\n"

        if scalers is None:
            success_msg += f"\nâš ï¸ æç¤º: è¯¥æ¨¡å‹checkpointä¸­ä¸åŒ…å«scalers\n"
            success_msg += f"   å¦‚éœ€æå–æ®‹å·®ï¼Œè¯·ä»ä¸‹æ–¹'åŠ è½½Scalersæ–‡ä»¶'åŒºåŸŸæ‰‹åŠ¨åŠ è½½\n"

        print(success_msg)
        return model_name, success_msg

    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return None, error_msg


def load_model_from_path_ui(model_path):
    """
    UI wrapper for load_model_from_path to properly update dropdown

    Args:
        model_path: Path to model file

    Returns:
        tuple: (dropdown_update, status_msg)
    """
    model_name, status_msg = load_model_from_path(model_path)

    if model_name:
        # Update dropdown choices and set the value
        return gr.update(choices=get_available_models(), value=model_name), status_msg
    else:
        # Just return status without updating dropdown
        return gr.update(), status_msg


def load_model_from_inference_config_path_ui(config_path):
    """
    UI wrapper for load_model_from_inference_config_path to properly update dropdown

    Args:
        config_path: Path to inference config file

    Returns:
        tuple: (dropdown_update, status_msg)
    """
    model_name, status_msg = load_model_from_inference_config_path(config_path)

    if model_name:
        # Update dropdown choices and set the value
        return gr.update(choices=get_available_models(), value=model_name), status_msg
    else:
        # Just return status without updating dropdown
        return gr.update(), status_msg


def extract_residuals_ui(model_name):
    """UI function for residual extraction - full dataset inference"""
    try:
        if not model_name:
            return "âŒ è¯·é€‰æ‹©æ¨¡å‹ï¼", None

        if global_state['df'] is None:
            return "âŒ è¯·å…ˆåŠ è½½æ•°æ®ï¼", None

        log_msg = []
        log_msg.append("=" * 80)
        log_msg.append("ğŸ“Š å¼€å§‹æå–æ®‹å·®ï¼ˆå…¨æ•°æ®é›†ï¼‰")
        log_msg.append("=" * 80)

        df = global_state['df']
        log_msg.append(f"\nğŸ“ˆ æ•°æ®é›†å¤§å°: {len(df):,} æ¡")

        # Load model
        model_path = os.path.join("saved_models", f"{model_name}.pth")
        if not os.path.exists(model_path):
            return f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}", None

        log_msg.append(f"\nğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        model_config = checkpoint['model_config']
        boundary_signals = model_config['boundary_signals']
        target_signals = model_config['target_signals']

        log_msg.append(f"  è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}")
        log_msg.append(f"  ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}")

        # Check if signals exist in dataframe
        missing_boundary = [s for s in boundary_signals if s not in df.columns]
        missing_target = [s for s in target_signals if s not in df.columns]

        if missing_boundary:
            return f"âŒ æ•°æ®é›†ä¸­ç¼ºå°‘è¾¹ç•Œä¿¡å·: {missing_boundary}", None
        if missing_target:
            return f"âŒ æ•°æ®é›†ä¸­ç¼ºå°‘ç›®æ ‡ä¿¡å·: {missing_target}", None

        # Prepare data
        X = df[boundary_signals].values
        y = df[target_signals].values

        # Load scalers - try checkpoint first, then manual_scalers
        scalers = None
        if 'scalers' in checkpoint:
            scalers = checkpoint['scalers']
            log_msg.append(f"  âœ“ ä»checkpointåŠ è½½scalers")
        elif 'manual_scalers' in global_state and model_name in global_state['manual_scalers']:
            scalers = global_state['manual_scalers'][model_name]
            log_msg.append(f"  âœ“ ä»æ‰‹åŠ¨åŠ è½½çš„scalersåŠ è½½")
        else:
            error_msg = "âŒ æœªæ‰¾åˆ°scalersï¼\n\n"
            error_msg += f"checkpointä¸­æ— scalersï¼Œä¸”æœªæ‰‹åŠ¨åŠ è½½scalersã€‚\n\n"
            error_msg += "ğŸ’¡ è§£å†³æ–¹æ³•:\n"
            error_msg += "1. åœ¨ä¸‹æ–¹'åŠ è½½Scalersæ–‡ä»¶'åŒºåŸŸä¸Šä¼ å¯¹åº”çš„scalers.pklæ–‡ä»¶\n"
            error_msg += f"2. æ–‡ä»¶ååº”è¯¥ç±»ä¼¼: {model_name}_scalers.pkl\n"
            error_msg += "3. ç‚¹å‡»'ğŸ“¥ åŠ è½½Scalers'æŒ‰é’®åï¼Œå†æ¬¡ç‚¹å‡»'ğŸ”¬ æå–æ®‹å·®'æŒ‰é’®\n"
            return error_msg, None

        scaler_X = scalers['X']
        scaler_y = scalers['y']

        # Load model
        config = model_config['config']
        model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            dropout=config['dropout']
        ).to(device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        log_msg.append(f"\nğŸ”„ å¼€å§‹æ¨ç†...")

        # Batch inference
        from models.residual_tft import batch_inference
        y_pred = batch_inference(
            model, X, scaler_X, scaler_y, device,
            batch_size=512, model_name="SST"
        )

        # Calculate residuals in original space
        residuals = y - y_pred

        # Calculate metrics
        from models.residual_tft import compute_r2_safe
        r2, per_signal_r2 = compute_r2_safe(y, y_pred, method='per_output_mean')
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))

        log_msg.append(f"\nğŸ“Š æ¨ç†å®Œæˆ:")
        log_msg.append(f"  MAE: {mae:.6f}")
        log_msg.append(f"  RMSE: {rmse:.6f}")
        log_msg.append(f"  RÂ²: {r2:.4f}")

        # Create residuals dataframe
        residual_cols = [f"{sig}_residual" for sig in target_signals]
        pred_cols = [f"{sig}_pred" for sig in target_signals]
        true_cols = [f"{sig}_true" for sig in target_signals]

        residuals_data = {}

        # Add boundary signals (input features) - needed for Stage2 training
        for i, sig in enumerate(boundary_signals):
            residuals_data[sig] = X[:, i]

        # Add residuals, predictions, and true values
        for i, sig in enumerate(target_signals):
            residuals_data[f"{sig}_residual"] = residuals[:, i]
            residuals_data[f"{sig}_pred"] = y_pred[:, i]
            residuals_data[f"{sig}_true"] = y[:, i]

        residuals_df = pd.DataFrame(residuals_data)

        # Save residual data
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        residual_key = f"{model_name}_{timestamp}"

        info = {
            'base_model_name': model_name,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_signals': residual_cols,
            'metrics': {
                'mae': float(mae),
                'rmse': float(rmse),
                'r2': float(r2),
                'per_signal_r2': per_signal_r2.tolist() if isinstance(per_signal_r2, np.ndarray) else per_signal_r2
            }
        }

        global_state['residual_data'][residual_key] = {
            'data': residuals_df,
            'info': info
        }

        log_msg.append(f"\nâœ… æ®‹å·®æå–å®Œæˆ:")
        log_msg.append(f"  æ®‹å·®æ•°æ®æ ‡è¯†: {residual_key}")
        log_msg.append(f"  æ•°æ®å½¢çŠ¶: {residuals_df.shape}")

        # Create residual visualization
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'æ®‹å·®åˆ†æ - {model_name}', fontsize=16)

        # æ®‹å·®åˆ†å¸ƒ
        residual_cols = info['residual_signals']
        all_residuals = residuals_df[residual_cols].values.flatten()

        axes[0, 0].hist(all_residuals, bins=50, edgecolor='black', alpha=0.7)
        axes[0, 0].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('æ®‹å·®å€¼')
        axes[0, 0].set_ylabel('é¢‘æ•°')

        # æ®‹å·®åºåˆ—
        axes[0, 1].plot(residuals_df[residual_cols[0]].values[:1000])
        axes[0, 1].set_title(f'æ®‹å·®åºåˆ— ({residual_cols[0]})')
        axes[0, 1].set_xlabel('Index')
        axes[0, 1].set_ylabel('æ®‹å·®')

        # æ®‹å·®ç»Ÿè®¡
        residual_stats = residuals_df[residual_cols].describe()
        axes[1, 0].axis('off')
        stats_text = "æ®‹å·®ç»Ÿè®¡:\n"
        stats_text += f"Mean: {residual_stats.loc['mean'].mean():.6f}\n"
        stats_text += f"Std: {residual_stats.loc['std'].mean():.6f}\n"
        stats_text += f"Min: {residual_stats.loc['min'].min():.6f}\n"
        stats_text += f"Max: {residual_stats.loc['max'].max():.6f}\n"
        axes[1, 0].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')

        # é¢„æµ‹vsçœŸå®
        true_cols = [f"{sig}_true" for sig in info['target_signals']]
        pred_cols = [f"{sig}_pred" for sig in info['target_signals']]

        y_true = residuals_df[true_cols].values[:1000, 0]
        y_pred = residuals_df[pred_cols].values[:1000, 0]

        axes[1, 1].plot(y_true, label='True', alpha=0.7)
        axes[1, 1].plot(y_pred, label='Predicted', alpha=0.7)
        axes[1, 1].set_title('é¢„æµ‹ vs çœŸå® (å‰1000ä¸ªæ ·æœ¬)')
        axes[1, 1].legend()

        plt.tight_layout()

        return "\n".join(log_msg), fig

    except Exception as e:
        error_msg = f"âŒ æ®‹å·®æå–å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, None


# ============================================================================
# Helper functions

def get_available_models():
    """Get list of available trained models"""
    return list(global_state['trained_models'].keys())


def get_residual_data_keys():
    """Get list of available residual data keys"""
    return list(global_state['residual_data'].keys())


def get_stage2_model_keys():
    """Get list of available Stage2 models"""
    return list(global_state['stage2_models'].keys())


def get_stage2_inference_config_files():
    """
    Get list of Stage2 inference config JSON files in saved_models/tft_models folder

    Returns:
        List of Stage2 inference config file paths
    """
    try:
        import glob

        config_files = []

        # Search in saved_models/tft_models folder
        if os.path.exists('saved_models/tft_models'):
            config_files.extend(glob.glob('saved_models/tft_models/*_inference.json', recursive=False))

        # Sort by modification time (newest first)
        config_files = sorted(config_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return config_files if config_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_stage2_inference_config_files: {e}")
        return []


def get_stage2_model_files():
    """
    Get list of Stage2 model .pth files in saved_models/tft_models folder

    Returns:
        List of Stage2 model file paths
    """
    try:
        import glob

        model_files = []

        # Search in saved_models/tft_models folder
        if os.path.exists('saved_models/tft_models'):
            model_files.extend(glob.glob('saved_models/tft_models/*.pth', recursive=False))

        # Sort by modification time (newest first)
        model_files = sorted(model_files, key=lambda x: os.path.getmtime(x) if os.path.exists(x) else 0, reverse=True)

        return model_files if model_files else []

    except Exception as e:
        print(f"âš ï¸ Error in get_stage2_model_files: {e}")
        return []


def load_stage2_from_inference_config(config_path):
    """
    Load Stage2 model from inference config JSON file

    Args:
        config_path: Path to inference config JSON file

    Returns:
        tuple: (model_key, status_message)
    """
    try:
        if not config_path or not os.path.exists(config_path):
            return None, "âŒ è¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨ç†é…ç½®æ–‡ä»¶ï¼"

        with open(config_path, 'r') as f:
            config = json.load(f)

        model_name = config.get('model_name', '')
        model_path = config.get('model_path', '')
        scaler_path = config.get('scaler_path', '')

        if not model_name or not model_path:
            return None, "âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘ model_name æˆ– model_pathï¼"

        # Load model checkpoint
        if not os.path.exists(model_path):
            return None, f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"

        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        # Load scalers
        scalers = None
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                scalers = pickle.load(f)
        elif 'scalers' in checkpoint:
            scalers = checkpoint['scalers']
        else:
            return None, "âŒ æœªæ‰¾åˆ° scalersï¼è¯·ç¡®ä¿é…ç½®æ–‡ä»¶ä¸­åŒ…å« scaler_path æˆ–æ¨¡å‹ checkpoint ä¸­åŒ…å« scalersã€‚"

        # Extract model config and training config
        model_config = checkpoint.get('model_config', {})
        training_config = checkpoint.get('training_config', {})
        residual_data_key = checkpoint.get('residual_data_key', 'unknown')

        # Get boundary and target signals from training config
        boundary_signals = training_config.get('boundary_signals', [])
        target_signals = training_config.get('target_signals', [])

        if not boundary_signals or not target_signals:
            return None, "âŒ é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ boundary_signals æˆ– target_signalsï¼"

        # Initialize model
        stage2_model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=model_config.get('d_model', 128),
            nhead=model_config.get('nhead', 8),
            num_layers=model_config.get('num_layers', 4),
            dropout=model_config.get('dropout', 0.15)
        ).to(device)

        # Load state dict
        stage2_model.load_state_dict(checkpoint['model_state_dict'])
        stage2_model.eval()

        # Store in global state with a unique key
        model_key = f"stage2_{model_name}"
        global_state['stage2_models'][model_key] = {
            'model': stage2_model,
            'config': training_config,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_data_key': residual_data_key,
            'model_path': model_path
        }
        global_state['stage2_scalers'][model_key] = scalers

        status_msg = f"âœ… æˆåŠŸåŠ è½½ Stage2 æ¨¡å‹ï¼\n\n"
        status_msg += f"æ¨¡å‹åç§°: {model_name}\n"
        status_msg += f"æ¨¡å‹é”®: {model_key}\n"
        status_msg += f"æ¨¡å‹è·¯å¾„: {model_path}\n"
        status_msg += f"è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}\n"
        status_msg += f"ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}\n"
        status_msg += f"æ®‹å·®æ•°æ®é”®: {residual_data_key}\n\n"
        status_msg += f"è¯·åœ¨ä¸‹æ–¹çš„ 'Stage2æ¨¡å‹' ä¸‹æ‹‰æ¡†ä¸­é€‰æ‹©: {model_key}"

        return model_key, status_msg

    except Exception as e:
        return None, f"âŒ åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"


def load_stage2_from_model_file(model_path):
    """
    Load Stage2 model from .pth model file

    Args:
        model_path: Path to model .pth file

    Returns:
        tuple: (model_key, status_message)
    """
    try:
        if not model_path or not os.path.exists(model_path):
            return None, "âŒ è¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶ï¼"

        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        # Extract configurations
        model_config = checkpoint.get('model_config', {})
        training_config = checkpoint.get('training_config', {})
        residual_data_key = checkpoint.get('residual_data_key', 'unknown')

        # Get signals
        boundary_signals = training_config.get('boundary_signals', [])
        target_signals = training_config.get('target_signals', [])

        if not boundary_signals or not target_signals:
            return None, "âŒ æ¨¡å‹æ–‡ä»¶ä¸­ç¼ºå°‘ boundary_signals æˆ– target_signalsï¼"

        # Load scalers from checkpoint or find corresponding scaler file
        scalers = None
        if 'scalers' in checkpoint:
            scalers = checkpoint['scalers']
        else:
            # Try to find corresponding scaler file
            scaler_path = model_path.replace('.pth', '_scalers.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    scalers = pickle.load(f)

        if not scalers:
            return None, "âŒ æœªæ‰¾åˆ° scalersï¼è¯·ç¡®ä¿æ¨¡å‹ checkpoint ä¸­åŒ…å« scalers æˆ–å­˜åœ¨å¯¹åº”çš„ *_scalers.pkl æ–‡ä»¶ã€‚"

        # Initialize model
        stage2_model = StaticSensorTransformer(
            num_boundary_sensors=len(boundary_signals),
            num_target_sensors=len(target_signals),
            d_model=model_config.get('d_model', 128),
            nhead=model_config.get('nhead', 8),
            num_layers=model_config.get('num_layers', 4),
            dropout=model_config.get('dropout', 0.15)
        ).to(device)

        # Load state dict
        stage2_model.load_state_dict(checkpoint['model_state_dict'])
        stage2_model.eval()

        # Extract model name from path
        model_name = os.path.basename(model_path).replace('.pth', '')
        model_key = f"stage2_{model_name}"

        # Store in global state
        global_state['stage2_models'][model_key] = {
            'model': stage2_model,
            'config': training_config,
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'residual_data_key': residual_data_key,
            'model_path': model_path
        }
        global_state['stage2_scalers'][model_key] = scalers

        status_msg = f"âœ… æˆåŠŸåŠ è½½ Stage2 æ¨¡å‹ï¼\n\n"
        status_msg += f"æ¨¡å‹åç§°: {model_name}\n"
        status_msg += f"æ¨¡å‹é”®: {model_key}\n"
        status_msg += f"æ¨¡å‹è·¯å¾„: {model_path}\n"
        status_msg += f"è¾¹ç•Œä¿¡å·æ•°: {len(boundary_signals)}\n"
        status_msg += f"ç›®æ ‡ä¿¡å·æ•°: {len(target_signals)}\n"
        status_msg += f"æ®‹å·®æ•°æ®é”®: {residual_data_key}\n\n"
        status_msg += f"è¯·åœ¨ä¸‹æ–¹çš„ 'Stage2æ¨¡å‹' ä¸‹æ‹‰æ¡†ä¸­é€‰æ‹©: {model_key}"

        return model_key, status_msg

    except Exception as e:
        return None, f"âŒ åŠ è½½å¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"


def get_ensemble_model_keys():
    """Get list of available ensemble models"""
    return list(global_state['ensemble_models'].keys())


# ============================================================================
# Gradio interface creation

def create_unified_interface():
    """Create unified Gradio interface"""

    with gr.Blocks(title="å·¥ä¸šæ•°å­—å­ªç”Ÿæ®‹å·®Boostè®­ç»ƒç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ­ å·¥ä¸šæ•°å­—å­ªç”Ÿæ®‹å·®BoostTrainingç³»ç»Ÿ
        ### Enhanced Residual Boost Training with Stage2 Model

        **æ–°åŠŸèƒ½:**
        - âœ¨ Stage2 residual modelTraining
        - ğŸ¯ æ™ºèƒ½RÂ²é˜ˆå€¼é€‰æ‹©ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹
        - ğŸ“Š Secondary inference comparison (Ensemble model vs Pure SST model)
        - ğŸ”® Sundial time series model predicting future residuals
        """)

        with gr.Tabs():
            # Tab 1: æ•°æ®åŠ è½½
            with gr.Tab("ğŸ“‚ æ•°æ®åŠ è½½", elem_id="data_loading"):
                gr.Markdown("## é€‰æ‹©ã€ä¸Šä¼ æˆ–åˆ›å»ºæ•°æ®")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ é€‰æ‹©å·²æœ‰CSVæ–‡ä»¶")
                        csv_file_selector = gr.Dropdown(
                            choices=[],  # Empty initially, populated on page load
                            label="é€‰æ‹©dataæ–‡ä»¶å¤¹ä¸‹çš„CSVæ–‡ä»¶",
                            info="ç‚¹å‡»'åˆ·æ–°åˆ—è¡¨'æ¥åŠ è½½å¯ç”¨çš„CSVæ–‡ä»¶"
                        )
                        with gr.Row():
                            select_csv_btn = gr.Button("ğŸ“‚ åŠ è½½é€‰ä¸­æ–‡ä»¶", variant="primary", size="lg")
                            refresh_csv_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm")

                        gr.Markdown("### ğŸ“¤ æˆ–ä¸Šä¼ CSVæ–‡ä»¶")
                        data_file = gr.File(label="ä¸Šä¼ CSVæ–‡ä»¶", file_types=['.csv'])
                        upload_btn = gr.Button("ğŸ“¥ åŠ è½½ä¸Šä¼ æ–‡ä»¶", variant="secondary", size="lg")

                        gr.Markdown("### ğŸ² æˆ–åˆ›å»ºç¤ºä¾‹æ•°æ®")
                        sample_btn = gr.Button("ğŸ² åˆ›å»ºç¤ºä¾‹æ•°æ®", size="lg")

                    with gr.Column(scale=1):
                        data_status = gr.Textbox(label="æ•°æ®çŠ¶æ€", lines=10, interactive=False)
                        signals_display = gr.Textbox(label="å¯ç”¨ä¿¡å·", lines=10, interactive=False)

                # Data preview table
                with gr.Row():
                    data_preview = gr.Dataframe(
                        label="ğŸ“Š æ•°æ®é¢„è§ˆ (å‰100è¡Œ)",
                        interactive=False,
                        wrap=True
                    )

            # Tab 2: SSTæ¨¡å‹Training
            with gr.Tab("ğŸ¯ SSTæ¨¡å‹Training", elem_id="sst_training"):
                gr.Markdown("## è®­ç»ƒé™æ€ä¼ æ„Ÿå™¨æ˜ å°„Transformer (SST)")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ›ï¸ ä¿¡å·é€‰æ‹©")

                        # JSONé…ç½®åŠ è½½
                        with gr.Accordion("ğŸ“ ä»JSONåŠ è½½ä¿¡å·é…ç½®", open=False):
                            json_config_selector = gr.Dropdown(
                                choices=get_available_json_configs(),
                                label="é€‰æ‹©dataæ–‡ä»¶å¤¹ä¸‹çš„JSONé…ç½®",
                                info="æˆ–æ‰‹åŠ¨ä¸Šä¼ JSONæ–‡ä»¶"
                            )
                            with gr.Row():
                                load_json_btn = gr.Button("ğŸ“‚ åŠ è½½é…ç½®", size="sm", variant="secondary")
                                refresh_json_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")
                            json_upload = gr.File(
                                label="ä¸Šä¼ JSONé…ç½®æ–‡ä»¶",
                                file_types=['.json'],
                                type="filepath"
                            )
                            json_status = gr.Textbox(
                                label="é…ç½®åŠ è½½çŠ¶æ€",
                                lines=3,
                                interactive=False
                            )

                        boundary_signals_static = gr.Dropdown(
                            choices=[], label="è¾¹ç•Œä¿¡å· (è¾“å…¥)", multiselect=True
                        )
                        target_signals_static = gr.Dropdown(
                            choices=[], label="ç›®æ ‡ä¿¡å· (è¾“å‡º)", multiselect=True
                        )

                        gr.Markdown("### ğŸ—ï¸ æ¨¡å‹æ¶æ„")
                        with gr.Row():
                            d_model_static = gr.Slider(32, 1280, 256, 32, label="æ¨¡å‹ç»´åº¦")
                            nhead_static = gr.Slider(2, 80, 16, 2, label="æ³¨æ„åŠ›å¤´æ•°")
                        with gr.Row():
                            num_layers_static = gr.Slider(1, 30, 6, 1, label="Transformerå±‚æ•°")
                            dropout_static = gr.Slider(0, 0.5, 0.1, 0.05, label="Dropoutç‡")

                        gr.Markdown("### ğŸ¯ è®­ç»ƒå‚æ•°")
                        with gr.Row():
                            epochs_static = gr.Slider(10, 250, 50, 10, label="è®­ç»ƒè½®æ•°")
                            batch_size_static = gr.Slider(16, 2560, 512, 16, label="æ‰¹å¤§å°")
                        with gr.Row():
                            lr_static = gr.Number(value=0.0001, label="å­¦ä¹ ç‡")
                            weight_decay_static = gr.Number(value=1e-5, label="æƒé‡è¡°å‡")

                        gr.Markdown("### âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®")
                        with gr.Row():
                            grad_clip_norm_static = gr.Slider(0.1, 5.0, 1.0, 0.1, label="æ¢¯åº¦è£å‰ª")
                            scheduler_patience_static = gr.Slider(1, 15, 3, 1, label="å­¦ä¹ ç‡è°ƒåº¦è€å¿ƒå€¼")
                        scheduler_factor_static = gr.Slider(0.1, 0.9, 0.5, 0.1, label="å­¦ä¹ ç‡è¡°å‡å› å­")

                        gr.Markdown("### ğŸ”€ æ•°æ®åˆ†å‰²")
                        with gr.Row():
                            test_size_static = gr.Slider(0.1, 0.3, 0.15, 0.05, label="æµ‹è¯•é›†æ¯”ä¾‹")
                            val_size_static = gr.Slider(0.1, 0.3, 0.15, 0.05, label="éªŒè¯é›†æ¯”ä¾‹")

                        train_btn_static = gr.Button("â–¶ï¸ å¼€å§‹TrainingSST", variant="primary", size="lg")
                        stop_btn_tab2 = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="stop", size="lg")

                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“Š è®­ç»ƒæ—¥å¿—")
                        training_log_static = gr.Textbox(
                            label="è®­ç»ƒè¿›åº¦",
                            lines=30,
                            autoscroll=True,
                            interactive=False
                        )

            # Tab 3: æ®‹å·®æå–
            with gr.Tab("ğŸ”¬ æ®‹å·®æå–", elem_id="residual_extraction"):
                gr.Markdown("## ä»è®­ç»ƒå¥½çš„SSTæ¨¡å‹æå–æ®‹å·®")
                gr.Markdown("å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆæ®‹å·®ç”¨äºStage2è®­ç»ƒ")

                with gr.Row():
                    with gr.Column(scale=1):
                        model_selector = gr.Dropdown(
                            choices=get_available_models(),
                            label="é€‰æ‹©SSTæ¨¡å‹"
                        )
                        refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨", size="sm")

                        gr.Markdown("### ğŸ“¤ æ¨ç†é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
                        gr.Markdown("å¯é€‰æ‹©å·²ä¿å­˜çš„æ¨ç†é…ç½®æ–‡ä»¶æ¥åŠ è½½æ¨¡å‹")

                        inference_config_selector = gr.Dropdown(
                            choices=get_inference_config_files(),
                            label="é€‰æ‹©saved_modelsæ–‡ä»¶å¤¹ä¸‹çš„æ¨ç†é…ç½®",
                            info="é€‰æ‹© *_inference.json æ–‡ä»¶"
                        )
                        with gr.Row():
                            load_inference_btn = gr.Button("ğŸ“¥ åŠ è½½é…ç½®", size="sm", variant="secondary")
                            refresh_inference_btn = gr.Button("ğŸ”„ åˆ·æ–°é…ç½®åˆ—è¡¨", size="sm")

                        inference_load_status = gr.Textbox(label="é…ç½®åŠ è½½çŠ¶æ€", lines=3, interactive=False)

                        gr.Markdown("### ğŸ¤– åŠ è½½SSTæ¨¡å‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
                        gr.Markdown("ä»saved_modelsæ–‡ä»¶å¤¹é€‰æ‹©.pthæ¨¡å‹æ–‡ä»¶ç›´æ¥åŠ è½½")
                        model_file_selector = gr.Dropdown(
                            choices=get_model_files(),
                            label="é€‰æ‹©saved_modelsæ–‡ä»¶å¤¹ä¸‹çš„æ¨¡å‹æ–‡ä»¶",
                            info="é€‰æ‹© *.pth æ–‡ä»¶"
                        )
                        with gr.Row():
                            load_model_file_btn = gr.Button("ğŸ“¥ åŠ è½½æ¨¡å‹", size="sm", variant="secondary")
                            refresh_model_files_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨", size="sm")
                        model_load_status = gr.Textbox(label="æ¨¡å‹åŠ è½½çŠ¶æ€", lines=3, interactive=False)

                        gr.Markdown("### ğŸ“Š åŠ è½½Scalersæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
                        gr.Markdown("å¦‚æœæ¨¡å‹checkpointä¸­ä¸åŒ…å«scalersï¼Œä»saved_modelsæ–‡ä»¶å¤¹é€‰æ‹©")
                        scalers_file_selector = gr.Dropdown(
                            choices=get_scalers_files(),
                            label="é€‰æ‹©saved_modelsæ–‡ä»¶å¤¹ä¸‹çš„Scalersæ–‡ä»¶",
                            info="é€‰æ‹© *_scalers.pkl æ–‡ä»¶"
                        )
                        with gr.Row():
                            load_scalers_btn = gr.Button("ğŸ“¥ åŠ è½½Scalers", size="sm", variant="secondary")
                            refresh_scalers_btn = gr.Button("ğŸ”„ åˆ·æ–°Scalersåˆ—è¡¨", size="sm")
                        scalers_load_status = gr.Textbox(label="ScalersåŠ è½½çŠ¶æ€", lines=3, interactive=False)

                        extract_btn = gr.Button("ğŸ”¬ æå–æ®‹å·®ï¼ˆå…¨æ•°æ®é›†ï¼‰", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        residual_status = gr.Textbox(label="æ®‹å·®æå–çŠ¶æ€", lines=20, interactive=False)
                        residual_plot = gr.Plot(label="æ®‹å·®å¯è§†åŒ–")

                # Event binding
                refresh_models_btn.click(
                    fn=lambda: gr.update(choices=get_available_models()),
                    outputs=[model_selector]
                )

                # Refresh inference config list
                refresh_inference_btn.click(
                    fn=lambda: gr.update(choices=get_inference_config_files()),
                    outputs=[inference_config_selector]
                )

                # Load inference config from selector
                load_inference_btn.click(
                    fn=load_model_from_inference_config_path_ui,
                    inputs=[inference_config_selector],
                    outputs=[model_selector, inference_load_status]
                )

                # Refresh model file list
                refresh_model_files_btn.click(
                    fn=lambda: gr.update(choices=get_model_files()),
                    outputs=[model_file_selector]
                )

                # Load model file
                load_model_file_btn.click(
                    fn=load_model_from_path_ui,
                    inputs=[model_file_selector],
                    outputs=[model_selector, model_load_status]
                )

                # Refresh scalers file list
                refresh_scalers_btn.click(
                    fn=lambda: gr.update(choices=get_scalers_files()),
                    outputs=[scalers_file_selector]
                )

                # Load scalers file
                load_scalers_btn.click(
                    fn=load_scalers_from_path,
                    inputs=[scalers_file_selector, model_selector],
                    outputs=[scalers_load_status]
                )

                # Extract residuals (full dataset)
                extract_btn.click(
                    fn=extract_residuals_ui,
                    inputs=[model_selector],
                    outputs=[residual_status, residual_plot]
                )

            # Tab 4: Stage2 BoostTraining
            with gr.Tab("ğŸš€ Stage2 BoostTraining", elem_id="stage2_training"):
                gr.Markdown("## è®­ç»ƒStage2æ®‹å·®æ¨¡å‹")
                gr.Markdown("åŸºäºæå–çš„æ®‹å·®è®­ç»ƒStage2æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æå‡é¢„æµ‹ç²¾åº¦")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“Š æ•°æ®é€‰æ‹©")
                        residual_data_selector_stage2 = gr.Dropdown(
                            choices=get_residual_data_keys(),
                            label="é€‰æ‹©æ®‹å·®æ•°æ®"
                        )
                        refresh_residual_btn_stage2 = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### ğŸ—ï¸ æ¨¡å‹æ¶æ„")
                        with gr.Row():
                            d_model_stage2 = gr.Slider(32, 640, 128, 32, label="æ¨¡å‹ç»´åº¦")
                            nhead_stage2 = gr.Slider(2, 40, 8, 2, label="æ³¨æ„åŠ›å¤´æ•°")
                        with gr.Row():
                            num_layers_stage2 = gr.Slider(1, 20, 4, 1, label="Transformerå±‚æ•°")
                            dropout_stage2 = gr.Slider(0, 0.5, 0.15, 0.05, label="Dropoutç‡")

                        gr.Markdown("### ğŸ¯ è®­ç»ƒå‚æ•°")
                        with gr.Row():
                            epochs_stage2 = gr.Slider(10, 400, 80, 10, label="è®­ç»ƒè½®æ•°")
                            batch_size_stage2 = gr.Slider(16, 2560, 512, 16, label="æ‰¹å¤§å°")
                        with gr.Row():
                            lr_stage2 = gr.Number(value=0.0001, label="å­¦ä¹ ç‡")
                            weight_decay_stage2 = gr.Number(value=5e-6, label="æƒé‡è¡°å‡")

                        gr.Markdown("### âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®")
                        with gr.Row():
                            grad_clip_stage2 = gr.Slider(0.1, 2.5, 0.5, 0.1, label="æ¢¯åº¦è£å‰ª")
                            scheduler_patience_stage2 = gr.Slider(1, 75, 15, 1, label="å­¦ä¹ ç‡è°ƒåº¦è€å¿ƒå€¼")
                        scheduler_factor_stage2 = gr.Slider(0.1, 0.9, 0.7, 0.1, label="å­¦ä¹ ç‡è¡°å‡å› å­")

                        gr.Markdown("### ğŸ”€ æ•°æ®åˆ†å‰²")
                        with gr.Row():
                            test_size_stage2 = gr.Slider(0.1, 0.3, 0.15, 0.05, label="æµ‹è¯•é›†æ¯”ä¾‹")
                            val_size_stage2 = gr.Slider(0.1, 0.3, 0.15, 0.05, label="éªŒè¯é›†æ¯”ä¾‹")

                        train_stage2_btn = gr.Button("ğŸš€ å¼€å§‹TrainingStage2", variant="primary", size="lg")
                        stop_btn_tab4 = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="stop", size="lg")

                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“Š è®­ç»ƒæ—¥å¿—")
                        stage2_training_log = gr.Textbox(
                            label="è®­ç»ƒè¿›åº¦",
                            lines=30,
                            autoscroll=True,
                            interactive=False
                        )

                # Stage2Trainingå‡½æ•°
                def train_stage2_ui_generator(residual_data_key, d_model, nhead, num_layers, dropout,
                                             epochs, batch_size, lr, weight_decay, grad_clip,
                                             scheduler_patience, scheduler_factor,
                                             test_size, val_size, progress=gr.Progress()):
                    """Generator function for real-time log updates"""

                    config = {
                        'd_model': int(d_model),
                        'nhead': int(nhead),
                        'num_layers': int(num_layers),
                        'dropout': float(dropout),
                        'epochs': int(epochs),
                        'batch_size': int(batch_size),
                        'lr': float(lr),
                        'weight_decay': float(weight_decay),
                        'grad_clip': float(grad_clip),
                        'scheduler_patience': int(scheduler_patience),
                        'scheduler_factor': float(scheduler_factor),
                        'test_size': float(test_size),
                        'val_size': float(val_size),
                        'early_stop_patience': 25
                    }

                    # Yield initial message
                    yield "ğŸš€ æ­£åœ¨åˆå§‹åŒ–Stage2è®­ç»ƒ...\n"

                    # Import here to avoid circular dependency
                    import time

                    # Call training function and yield intermediate results
                    try:
                        # Run the training in a way that allows yielding
                        # This is a workaround since train_stage2_boost_model is not a generator
                        final_status = ""
                        for update in train_stage2_boost_model_generator(residual_data_key, config, progress):
                            yield update
                            final_status = update
                    except Exception as e:
                        yield f"âŒ è®­ç»ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"

                refresh_residual_btn_stage2.click(
                    fn=lambda: gr.update(choices=get_residual_data_keys()),
                    outputs=[residual_data_selector_stage2]
                )

                train_stage2_btn.click(
                    fn=train_stage2_ui_generator,
                    inputs=[
                        residual_data_selector_stage2,
                        d_model_stage2, nhead_stage2, num_layers_stage2, dropout_stage2,
                        epochs_stage2, batch_size_stage2, lr_stage2,
                        weight_decay_stage2, grad_clip_stage2,
                        scheduler_patience_stage2, scheduler_factor_stage2,
                        test_size_stage2, val_size_stage2
                    ],
                    outputs=[stage2_training_log]
                )

            # Tab 5: ç»¼åˆæ¨ç†æ¨¡å‹ç”Ÿæˆ
            with gr.Tab("ğŸ¯ ç»¼åˆæ¨ç†æ¨¡å‹", elem_id="ensemble_model"):
                gr.Markdown("## ç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹ (Delta RÂ² ç­–ç•¥)")
                gr.Markdown("""
                **ä¼˜åŒ–åçš„ç­–ç•¥**ï¼š
                - ä½¿ç”¨æµ‹è¯•é›†æ•°æ®è¯„ä¼°æ¯ä¸ªä¿¡å·çš„Delta RÂ² = RÂ²_ensemble - RÂ²_stage1
                - åªå¯¹Delta RÂ² > é˜ˆå€¼çš„ä¿¡å·åº”ç”¨Stage2ä¿®æ­£ï¼ˆè¯´æ˜Stage2ç¡®å®èƒ½æå‡æ€§èƒ½ï¼‰
                - å…¶ä½™ä¿¡å·ä»…ä½¿ç”¨Stage1é¢„æµ‹
                - è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰ä¿¡å·çš„åˆ†ææŠ¥å‘Šå’ŒCSVæ±‡æ€»æ–‡ä»¶
                """)

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ”§ æ¨¡å‹é€‰æ‹©")
                        base_model_selector = gr.Dropdown(
                            choices=get_available_models(),
                            label="é€‰æ‹©åŸºç¡€SSTæ¨¡å‹"
                        )
                        stage2_model_selector = gr.Dropdown(
                            choices=get_stage2_model_keys(),
                            label="é€‰æ‹©Stage2æ¨¡å‹"
                        )
                        refresh_ensemble_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### ğŸ“¤ åŠ è½½Stage2æ¨¡å‹ï¼ˆå¯é€‰ï¼‰")
                        gr.Markdown("ä»saved_models/tft_modelsæ–‡ä»¶å¤¹åŠ è½½é¢„è®­ç»ƒçš„Stage2æ¨¡å‹")

                        stage2_inference_config_selector = gr.Dropdown(
                            choices=get_stage2_inference_config_files(),
                            label="é€‰æ‹©Stage2æ¨ç†é…ç½®æ–‡ä»¶",
                            info="é€‰æ‹© *_inference.json æ–‡ä»¶"
                        )
                        with gr.Row():
                            load_stage2_inference_btn = gr.Button("ğŸ“¥ åŠ è½½é…ç½®", size="sm", variant="secondary")
                            refresh_stage2_inference_btn = gr.Button("ğŸ”„ åˆ·æ–°é…ç½®", size="sm")

                        stage2_model_file_selector = gr.Dropdown(
                            choices=get_stage2_model_files(),
                            label="é€‰æ‹©Stage2æ¨¡å‹æ–‡ä»¶",
                            info="é€‰æ‹© *.pth æ–‡ä»¶"
                        )
                        with gr.Row():
                            load_stage2_model_btn = gr.Button("ğŸ“¥ åŠ è½½æ¨¡å‹", size="sm", variant="secondary")
                            refresh_stage2_model_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹", size="sm")

                        stage2_load_status = gr.Textbox(label="Stage2åŠ è½½çŠ¶æ€", lines=5, interactive=False)

                        gr.Markdown("### ğŸšï¸ Delta RÂ²é˜ˆå€¼è®¾ç½®")
                        delta_r2_threshold_slider = gr.Slider(
                            0.0, 0.5, 0.05, 0.01,
                            label="Delta RÂ²é˜ˆå€¼",
                            info="åªå¯¹Delta RÂ² > é˜ˆå€¼çš„ä¿¡å·åº”ç”¨Stage2ä¿®æ­£ï¼ˆ0.05 = 5%æå‡ï¼‰"
                        )

                        generate_ensemble_btn = gr.Button("ğŸ¯ ç”Ÿæˆç»¼åˆæ¨¡å‹", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        ensemble_status = gr.Textbox(
                            label="ç”ŸæˆçŠ¶æ€",
                            lines=30,
                            autoscroll=True,
                            interactive=False
                        )

                # æ·»åŠ å¯è§†åŒ–è¾“å‡º
                with gr.Row():
                    ensemble_visualization = gr.Plot(
                        label="ç»¼åˆæ¨¡å‹åˆ†æå¯è§†åŒ–",
                        show_label=True
                    )

                def generate_ensemble_ui(base_model_name, stage2_model_name, delta_r2_threshold):
                    if not base_model_name or not stage2_model_name:
                        return "âŒ è¯·é€‰æ‹©åŸºç¡€æ¨¡å‹å’ŒStage2æ¨¡å‹ï¼", None

                    status_msg, ensemble_info, fig = compute_signal_r2_and_select_threshold(
                        base_model_name, stage2_model_name, delta_r2_threshold
                    )
                    return status_msg, fig

                def load_stage2_inference_ui(config_path):
                    """UI wrapper for loading Stage2 from inference config"""
                    model_key, status_msg = load_stage2_from_inference_config(config_path)
                    if model_key:
                        return gr.update(choices=get_stage2_model_keys(), value=model_key), status_msg
                    else:
                        return gr.update(), status_msg

                def load_stage2_model_ui(model_path):
                    """UI wrapper for loading Stage2 from model file"""
                    model_key, status_msg = load_stage2_from_model_file(model_path)
                    if model_key:
                        return gr.update(choices=get_stage2_model_keys(), value=model_key), status_msg
                    else:
                        return gr.update(), status_msg

                # Event bindings for Stage2 model loading
                refresh_stage2_inference_btn.click(
                    fn=lambda: gr.update(choices=get_stage2_inference_config_files()),
                    outputs=[stage2_inference_config_selector]
                )

                load_stage2_inference_btn.click(
                    fn=load_stage2_inference_ui,
                    inputs=[stage2_inference_config_selector],
                    outputs=[stage2_model_selector, stage2_load_status]
                )

                refresh_stage2_model_btn.click(
                    fn=lambda: gr.update(choices=get_stage2_model_files()),
                    outputs=[stage2_model_file_selector]
                )

                load_stage2_model_btn.click(
                    fn=load_stage2_model_ui,
                    inputs=[stage2_model_file_selector],
                    outputs=[stage2_model_selector, stage2_load_status]
                )

                refresh_ensemble_btn.click(
                    fn=lambda: (gr.update(choices=get_available_models()),
                                gr.update(choices=get_stage2_model_keys())),
                    outputs=[base_model_selector, stage2_model_selector]
                )

                generate_ensemble_btn.click(
                    fn=generate_ensemble_ui,
                    inputs=[base_model_selector, stage2_model_selector, delta_r2_threshold_slider],
                    outputs=[ensemble_status, ensemble_visualization]
                )

            # Tab 6: äºŒæ¬¡æ¨ç†æ¯”è¾ƒ
            with gr.Tab("ğŸ“Š äºŒæ¬¡æ¨ç†æ¯”è¾ƒ", elem_id="reinference_comparison"):
                gr.Markdown("## äºŒæ¬¡æ¨ç†æ¯”è¾ƒ")
                gr.Markdown("é€‰æ‹©indexèŒƒå›´ï¼Œæ¯”è¾ƒç»¼åˆæ¨¡å‹ä¸çº¯SSTæ¨¡å‹çš„æ€§èƒ½æå‡")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¯ æ¨¡å‹é€‰æ‹©")
                        ensemble_selector_reinf = gr.Dropdown(
                            choices=get_ensemble_model_keys(),
                            label="é€‰æ‹©ç»¼åˆæ¨¡å‹"
                        )
                        refresh_reinf_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### ğŸ“ IndexèŒƒå›´é€‰æ‹©")
                        with gr.Row():
                            reinf_start_idx = gr.Number(value=0, label="èµ·å§‹Index", precision=0)
                            reinf_end_idx = gr.Number(value=1000, label="ç»“æŸIndex", precision=0)

                        compare_reinf_btn = gr.Button("ğŸ“Š æ‰§è¡Œæ¯”è¾ƒ", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        reinf_status = gr.Textbox(
                            label="æ¯”è¾ƒç»“æœ",
                            lines=20,
                            autoscroll=True,
                            interactive=False
                        )
                        reinf_plot = gr.Plot(label="æ€§èƒ½å¯¹æ¯”å¯è§†åŒ–")

                def compare_reinference_ui(ensemble_name, start_idx, end_idx):
                    if not ensemble_name:
                        return "âŒ è¯·é€‰æ‹©ç»¼åˆæ¨¡å‹ï¼", None

                    if ensemble_name not in global_state['ensemble_models']:
                        return "âŒ ç»¼åˆæ¨¡å‹ä¸å­˜åœ¨ï¼", None

                    try:
                        ensemble_info = global_state['ensemble_models'][ensemble_name]

                        # è·å–é¢„æµ‹æ•°æ®
                        y_true = ensemble_info['predictions']['y_true']
                        y_pred_base = ensemble_info['predictions']['y_pred_base']
                        y_pred_ensemble = ensemble_info['predictions']['y_pred_ensemble']

                        # åˆ‡ç‰‡
                        start_idx = max(0, int(start_idx))
                        end_idx = min(len(y_true), int(end_idx))

                        y_true_seg = y_true[start_idx:end_idx]
                        y_pred_base_seg = y_pred_base[start_idx:end_idx]
                        y_pred_ensemble_seg = y_pred_ensemble[start_idx:end_idx]

                        # è®¡ç®—æ€§èƒ½ using safe RÂ² computation
                        mae_base = mean_absolute_error(y_true_seg, y_pred_base_seg)
                        mae_ensemble = mean_absolute_error(y_true_seg, y_pred_ensemble_seg)
                        rmse_base = np.sqrt(mean_squared_error(y_true_seg, y_pred_base_seg))
                        rmse_ensemble = np.sqrt(mean_squared_error(y_true_seg, y_pred_ensemble_seg))
                        r2_base, _ = compute_r2_safe(y_true_seg, y_pred_base_seg, method='per_output_mean')
                        r2_ensemble, _ = compute_r2_safe(y_true_seg, y_pred_ensemble_seg, method='per_output_mean')

                        improvement_mae = (mae_base - mae_ensemble) / mae_base * 100
                        improvement_rmse = (rmse_base - rmse_ensemble) / rmse_base * 100

                        status = f"ğŸ“Š äºŒæ¬¡æ¨ç†æ¯”è¾ƒç»“æœ\n"
                        status += f"=" * 60 + "\n\n"
                        status += f"ğŸ“ IndexèŒƒå›´: [{start_idx}, {end_idx})\n"
                        status += f"ğŸ“ˆ æ ·æœ¬æ•°: {len(y_true_seg):,}\n\n"

                        status += f"æ€§èƒ½å¯¹æ¯”:\n"
                        status += f"{'æŒ‡æ ‡':<15} {'SSTæ¨¡å‹':>15} {'ç»¼åˆæ¨¡å‹':>15} {'æ”¹è¿›':>12}\n"
                        status += "-" * 60 + "\n"
                        status += f"{'MAE':<15} {mae_base:>15.6f} {mae_ensemble:>15.6f} {improvement_mae:>11.2f}%\n"
                        status += f"{'RMSE':<15} {rmse_base:>15.6f} {rmse_ensemble:>15.6f} {improvement_rmse:>11.2f}%\n"
                        status += f"{'RÂ²':<15} {r2_base:>15.4f} {r2_ensemble:>15.4f}\n"

                        # åˆ›å»ºå¯è§†åŒ–
                        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                        fig.suptitle(f'äºŒæ¬¡æ¨ç†æ¯”è¾ƒ - {ensemble_name}', fontsize=16)

                        # é¢„æµ‹å¯¹æ¯”ï¼ˆç¬¬ä¸€ä¸ªä¿¡å·ï¼‰
                        plot_samples = min(500, len(y_true_seg))
                        axes[0, 0].plot(y_true_seg[:plot_samples, 0], label='True', alpha=0.7)
                        axes[0, 0].plot(y_pred_base_seg[:plot_samples, 0], label='SST', alpha=0.7)
                        axes[0, 0].plot(y_pred_ensemble_seg[:plot_samples, 0], label='Ensemble', alpha=0.7)
                        axes[0, 0].set_title('é¢„æµ‹å¯¹æ¯” (ä¿¡å·1)')
                        axes[0, 0].legend()
                        axes[0, 0].set_xlabel('Index')
                        axes[0, 0].set_ylabel('Value')

                        # è¯¯å·®å¯¹æ¯”
                        error_base = np.abs(y_true_seg - y_pred_base_seg).mean(axis=1)
                        error_ensemble = np.abs(y_true_seg - y_pred_ensemble_seg).mean(axis=1)

                        axes[0, 1].plot(error_base[:plot_samples], label='SST Error', alpha=0.7)
                        axes[0, 1].plot(error_ensemble[:plot_samples], label='Ensemble Error', alpha=0.7)
                        axes[0, 1].set_title('å¹³å‡ç»å¯¹è¯¯å·®å¯¹æ¯”')
                        axes[0, 1].legend()
                        axes[0, 1].set_xlabel('Index')
                        axes[0, 1].set_ylabel('MAE')

                        # è¯¯å·®åˆ†å¸ƒ
                        axes[1, 0].hist(error_base, bins=50, alpha=0.5, label='SST', edgecolor='black')
                        axes[1, 0].hist(error_ensemble, bins=50, alpha=0.5, label='Ensemble', edgecolor='black')
                        axes[1, 0].set_title('è¯¯å·®åˆ†å¸ƒ')
                        axes[1, 0].legend()
                        axes[1, 0].set_xlabel('Error')
                        axes[1, 0].set_ylabel('Frequency')

                        # æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾
                        metrics = ['MAE', 'RMSE', 'RÂ²']
                        base_values = [mae_base, rmse_base, r2_base]
                        ensemble_values = [mae_ensemble, rmse_ensemble, r2_ensemble]

                        x = np.arange(len(metrics))
                        width = 0.35

                        axes[1, 1].bar(x - width / 2, base_values, width, label='SST', alpha=0.8)
                        axes[1, 1].bar(x + width / 2, ensemble_values, width, label='Ensemble', alpha=0.8)
                        axes[1, 1].set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
                        axes[1, 1].set_xticks(x)
                        axes[1, 1].set_xticklabels(metrics)
                        axes[1, 1].legend()

                        plt.tight_layout()

                        return status, fig

                    except Exception as e:
                        error_msg = f"âŒ æ¯”è¾ƒå¤±è´¥:\n{str(e)}\n\n{traceback.format_exc()}"
                        return error_msg, None

                refresh_reinf_btn.click(
                    fn=lambda: gr.update(choices=get_ensemble_model_keys()),
                    outputs=[ensemble_selector_reinf]
                )

                compare_reinf_btn.click(
                    fn=compare_reinference_ui,
                    inputs=[ensemble_selector_reinf, reinf_start_idx, reinf_end_idx],
                    outputs=[reinf_status, reinf_plot]
                )

            # Tab 7: Sundialæ—¶åºé¢„æµ‹
            with gr.Tab("ğŸ”® Sundialæ®‹å·®é¢„æµ‹", elem_id="sundial_forecast"):
                gr.Markdown("## Sundialæ—¶åºæ¨¡å‹é¢„æµ‹æœªæ¥æ®‹å·®")
                gr.Markdown("åŸºäºç»¼åˆæ¨¡å‹çš„æœ€ç»ˆæ®‹å·®ï¼Œä½¿ç”¨Sundialé¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¯ æ¨¡å‹é€‰æ‹©")
                        ensemble_selector_sundial = gr.Dropdown(
                            choices=get_ensemble_model_keys(),
                            label="é€‰æ‹©ç»¼åˆæ¨¡å‹"
                        )
                        refresh_sundial_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")

                        gr.Markdown("### âš™ï¸ Sundialé…ç½®")
                        sundial_forecast_horizon = gr.Slider(
                            10, 500, 100, 10,
                            label="é¢„æµ‹æ­¥æ•°",
                            info="é¢„æµ‹æœªæ¥å¤šå°‘ä¸ªæ—¶é—´æ­¥çš„æ®‹å·®"
                        )

                        with gr.Row():
                            sundial_d_model = gr.Slider(32, 256, 64, 32, label="æ¨¡å‹ç»´åº¦")
                            sundial_nhead = gr.Slider(2, 8, 4, 2, label="æ³¨æ„åŠ›å¤´æ•°")

                        train_sundial_btn = gr.Button("ğŸ”® TrainingSundial", variant="primary", size="lg")
                        predict_sundial_btn = gr.Button("ğŸ“ˆ é¢„æµ‹æœªæ¥æ®‹å·®", size="lg")

                    with gr.Column(scale=1):
                        sundial_status = gr.Textbox(
                            label="çŠ¶æ€ä¿¡æ¯",
                            lines=20,
                            autoscroll=True,
                            interactive=False
                        )
                        sundial_plot = gr.Plot(label="æ®‹å·®é¢„æµ‹å¯è§†åŒ–")

                def train_sundial_ui(ensemble_name, forecast_horizon, d_model, nhead):
                    if not ensemble_name:
                        return "âŒ è¯·é€‰æ‹©ç»¼åˆæ¨¡å‹ï¼", None

                    # è¿™é‡Œæ˜¯SundialTrainingçš„å ä½ç¬¦ï¼Œå®é™…éœ€è¦å®ç°Sundialæ¨¡å‹
                    status = f"ğŸ”® Sundialæ¨¡å‹è®­ç»ƒ\n"
                    status += f"=" * 60 + "\n\n"
                    status += f"ğŸ“Š ç»¼åˆæ¨¡å‹: {ensemble_name}\n"
                    status += f"ğŸ“ é¢„æµ‹æ­¥æ•°: {int(forecast_horizon)}\n"
                    status += f"âš™ï¸ æ¨¡å‹é…ç½®: d_model={int(d_model)}, nhead={int(nhead)}\n\n"
                    status += f"âš ï¸ Sundialæ¨¡å‹è®­ç»ƒåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...\n"
                    status += f"ğŸ’¡ Sundialå°†åŸºäºæ—¶é—´åºåˆ—ç‰¹æ€§é¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿\n"

                    return status, None

                def predict_sundial_ui(ensemble_name):
                    if not ensemble_name:
                        return "âŒ è¯·é€‰æ‹©ç»¼åˆæ¨¡å‹ï¼", None

                    status = f"ğŸ“ˆ Sundialæ®‹å·®é¢„æµ‹\n"
                    status += f"=" * 60 + "\n\n"
                    status += f"âš ï¸ Sundialé¢„æµ‹åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...\n"

                    return status, None

                refresh_sundial_btn.click(
                    fn=lambda: gr.update(choices=get_ensemble_model_keys()),
                    outputs=[ensemble_selector_sundial]
                )

                train_sundial_btn.click(
                    fn=train_sundial_ui,
                    inputs=[ensemble_selector_sundial, sundial_forecast_horizon,
                            sundial_d_model, sundial_nhead],
                    outputs=[sundial_status, sundial_plot]
                )

                predict_sundial_btn.click(
                    fn=predict_sundial_ui,
                    inputs=[ensemble_selector_sundial],
                    outputs=[sundial_status, sundial_plot]
                )

        # Footer info
        gr.Markdown("""
        ---
        ## ğŸ“– ä½¿ç”¨æµç¨‹

        ### å®Œæ•´æµç¨‹
        1ï¸âƒ£ **æ•°æ®åŠ è½½** â†’ ä¸Šä¼ CSVæˆ–Create sample data
        2ï¸âƒ£ **SSTæ¨¡å‹Training** â†’ Trainingé™æ€ä¼ æ„Ÿå™¨æ˜ å°„Transformer
        3ï¸âƒ£ **æ®‹å·®æå–** â†’ ä»SSTæ¨¡å‹æå–é¢„æµ‹æ®‹å·®
        4ï¸âƒ£ **Stage2Training** â†’ TrainingStage2 residual model
        5ï¸âƒ£ **ç”Ÿæˆç»¼åˆæ¨¡å‹** â†’ æ™ºèƒ½RÂ²é˜ˆå€¼é€‰æ‹©ï¼Œç”Ÿæˆç»¼åˆæ¨ç†æ¨¡å‹
        6ï¸âƒ£ **äºŒæ¬¡æ¨ç†æ¯”è¾ƒ** â†’ å¯¹æ¯”ç»¼åˆæ¨¡å‹ä¸SSTæ¨¡å‹çš„æ€§èƒ½æå‡
        7ï¸âƒ£ **Sundialé¢„æµ‹** â†’ é¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿

        **ğŸ¯ åˆ›æ–°ç‚¹**: 
        - âœ¨ Stage2 Boostæ¶æ„ï¼šé’ˆå¯¹æ€§æ”¹è¿›ä½RÂ²ä¿¡å·
        - ğŸ¯ æ™ºèƒ½é˜ˆå€¼é€‰æ‹©ï¼šè‡ªåŠ¨å†³å®šå“ªäº›ä¿¡å·éœ€è¦Stage2
        - ğŸ“Š ç»¼åˆæ¨ç†æ¨¡å‹ï¼šæœ€ä¼˜ç»„åˆSSTå’ŒStage2
        - ğŸ”® æ®‹å·®é¢„æµ‹ï¼šSundialé¢„æµ‹æœªæ¥æ®‹å·®è¶‹åŠ¿
        """)

        # Auto refresh dropdowns on page load
        # Initial load: populate dropdowns and check for pre-loaded data
        def initial_load():
            """Load initial state including pre-loaded data from Colab"""
            # Get dropdown choices
            models = get_available_models()
            residual_keys = get_residual_data_keys()
            stage2_keys = get_stage2_model_keys()
            ensemble_keys = get_ensemble_model_keys()

            # Get available CSV files (safe - won't break interface)
            csv_files = get_available_csv_files()

            # Get available JSON config files
            json_configs = get_available_json_configs()

            # Get available inference config files
            inference_configs = get_inference_config_files()

            # Check for pre-loaded data (but don't auto-load)
            status, preview_df, signals = check_preloaded_data()

            # Get column choices if data exists
            if global_state.get('df') is not None:
                cols = list(global_state['df'].columns)
            else:
                cols = []

            return (
                gr.update(choices=models),
                gr.update(choices=residual_keys),
                gr.update(choices=stage2_keys),
                gr.update(choices=ensemble_keys),
                gr.update(choices=csv_files),  # Populate CSV file selector
                gr.update(choices=json_configs),  # Populate JSON config selector
                gr.update(choices=inference_configs),  # Populate inference config selector
                status, signals, preview_df,
                gr.update(choices=cols), gr.update(choices=cols)
            )

        demo.load(
            fn=initial_load,
            outputs=[
                model_selector, residual_data_selector_stage2,
                stage2_model_selector, ensemble_selector_reinf,
                csv_file_selector,  # CSV file selector
                json_config_selector,  # JSON config selector (Tab2)
                inference_config_selector,  # Inference config selector (Tab3)
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # Data loading events
        def load_data_and_update(file_obj):
            status, preview_df, signals = load_data_from_csv(file_obj)
            if preview_df is not None:
                cols = list(global_state['df'].columns)
                return (
                    status, signals, preview_df,
                    gr.update(choices=cols), gr.update(choices=cols)
                )
            return (
                status, signals, None,
                gr.update(choices=[]), gr.update(choices=[])
            )

        def create_sample_and_update():
            status, preview_df, signals = create_sample_data()
            if preview_df is not None:
                cols = list(global_state['df'].columns)
                return (
                    status, signals, preview_df,
                    gr.update(choices=cols), gr.update(choices=cols)
                )
            return (
                status, signals, None,
                gr.update(choices=[]), gr.update(choices=[])
            )

        # CSV file selector event - load from data/ folder
        def load_from_selector_and_update(csv_path):
            status, preview_df, signals = load_csv_from_path(csv_path)
            if preview_df is not None:
                cols = list(global_state['df'].columns)
                return (
                    status, signals, preview_df,
                    gr.update(choices=cols), gr.update(choices=cols)
                )
            return (
                status, signals, None,
                gr.update(choices=[]), gr.update(choices=[])
            )

        select_csv_btn.click(
            fn=load_from_selector_and_update,
            inputs=[csv_file_selector],
            outputs=[
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # Refresh CSV file list
        refresh_csv_btn.click(
            fn=lambda: gr.update(choices=get_available_csv_files()),
            outputs=[csv_file_selector]
        )

        # Upload button event - load from uploaded file
        upload_btn.click(
            fn=load_data_and_update,
            inputs=[data_file],
            outputs=[
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # Sample button event - create sample data
        sample_btn.click(
            fn=create_sample_and_update,
            outputs=[
                data_status, signals_display, data_preview,
                boundary_signals_static, target_signals_static
            ]
        )

        # JSONé…ç½®åŠ è½½äº‹ä»¶
        def load_json_from_selector(json_path):
            """Load JSON config from dropdown selector"""
            if not json_path:
                return gr.update(), gr.update(), "âš ï¸ è¯·é€‰æ‹©JSONé…ç½®æ–‡ä»¶"
            boundary, target, status = load_signals_config_from_json(json_path)
            return gr.update(value=boundary), gr.update(value=target), status

        def load_json_from_upload(json_file):
            """Load JSON config from uploaded file"""
            if not json_file:
                return gr.update(), gr.update(), "âš ï¸ è¯·ä¸Šä¼ JSONé…ç½®æ–‡ä»¶"
            boundary, target, status = load_signals_config_from_json(json_file)
            return gr.update(value=boundary), gr.update(value=target), status

        # Load JSON from selector
        load_json_btn.click(
            fn=load_json_from_selector,
            inputs=[json_config_selector],
            outputs=[boundary_signals_static, target_signals_static, json_status]
        )

        # Load JSON from uploaded file
        json_upload.change(
            fn=load_json_from_upload,
            inputs=[json_upload],
            outputs=[boundary_signals_static, target_signals_static, json_status]
        )

        # Refresh JSON file list
        refresh_json_btn.click(
            fn=lambda: gr.update(choices=get_available_json_configs()),
            outputs=[json_config_selector]
        )

        # TrainingæŒ‰é’®ç»‘å®š
        train_btn_static.click(
            fn=train_base_model_ui,
            inputs=[
                boundary_signals_static, target_signals_static,
                gr.Textbox(value="StaticSensorTransformer", visible=False),
                epochs_static, batch_size_static, lr_static,
                d_model_static, nhead_static, num_layers_static, dropout_static,
                test_size_static, val_size_static,
                weight_decay_static, scheduler_patience_static, scheduler_factor_static, grad_clip_norm_static,
                gr.State(value=None), gr.State(value=False)
            ],
            outputs=[training_log_static]
        )

        # StopæŒ‰é’®ç»‘å®š - Tab2
        def stop_training_tab2():
            global_state['stop_training_tab2'] = True
            return "âš ï¸  å·²å‘é€åœæ­¢ä¿¡å·ï¼Œè®­ç»ƒå°†åœ¨å½“å‰ epoch ç»“æŸååœæ­¢..."

        stop_btn_tab2.click(
            fn=stop_training_tab2,
            outputs=[training_log_static]
        )

        # StopæŒ‰é’®ç»‘å®š - Tab4
        def stop_training_tab4():
            global_state['stop_training_tab4'] = True
            return "âš ï¸  å·²å‘é€åœæ­¢ä¿¡å·ï¼Œè®­ç»ƒå°†åœ¨å½“å‰ epoch ç»“æŸååœæ­¢..."

        stop_btn_tab4.click(
            fn=stop_training_tab4,
            outputs=[stage2_training_log]
        )

    return demo


# ============================================================================
# Launch application

if __name__ == "__main__":
    import sys

    print("å¯åŠ¨å·¥ä¸šæ•°å­—å­ªç”Ÿæ®‹å·®Boostè®­ç»ƒç³»ç»Ÿ...")
    print("="*80)

    # Check if running in Colab
    try:
        import google.colab
        IN_COLAB = True
        print("âœ… æ£€æµ‹åˆ°Colabç¯å¢ƒ")
    except:
        IN_COLAB = False
        print("âœ… æœ¬åœ°ç¯å¢ƒ")

    demo = create_unified_interface()
    print("âœ… ç•Œé¢åˆ›å»ºå®Œæˆ")
    print("="*80)

    if IN_COLAB:
        # Colab environment - use share=True for public URL
        print("\nğŸŒ åœ¨Colabä¸­å¯åŠ¨Gradio...")
        print("ğŸ“ æç¤ºï¼šGradioå°†ç”Ÿæˆä¸€ä¸ªå…¬ç½‘é“¾æ¥")
        demo.launch(
            share=True,
            debug=True,
            show_error=True,
            inline=False  # Use separate window
        )
    else:
        # Local environment - try multiple ports
        print("\nğŸŒ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å¯åŠ¨Gradio...")
        for port in range(7860, 7870):
            try:
                print(f"å°è¯•ç«¯å£ {port}...")
                demo.launch(
                    server_name="127.0.0.1",
                    server_port=port,
                    share=False,
                    debug=True,
                    show_error=True,
                    quiet=False
                )
                print(f"âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼")
                print(f"ğŸ”— è®¿é—®åœ°å€: http://localhost:{port}")
                print("="*80)
                break
            except OSError:
                print(f"âš ï¸  ç«¯å£ {port} è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                continue
        else:
            print("âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ (7860-7869)")
