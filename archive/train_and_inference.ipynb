{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Digital Twin by Transformer - Training & Inference\n",
    "\n",
    "This notebook demonstrates how to train and use the V1 and V4 Transformer models for industrial sensor prediction.\n",
    "\n",
    "## Features\n",
    "- **V1 Model**: Static sensor-to-sensor mapping using Transformer architecture\n",
    "- **V4 Model**: Hybrid temporal + static approach for time-dependent sensor signals\n",
    "\n",
    "## Quick Start\n",
    "1. Upload your CSV dataset to `data/raw/` folder\n",
    "2. Run cells sequentially to train models\n",
    "3. Evaluate and visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/YOUR_USERNAME/Industrial-digital-twin-by-transformer.git\n",
    "    %cd Industrial-digital-twin-by-transformer\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import models and utilities\n",
    "from models.v1_transformer import CompactSensorTransformer\n",
    "from models.v4_hybrid_transformer import HybridTemporalTransformer\n",
    "from models.utils import create_temporal_context_data, apply_ifd_smoothing\n",
    "from src.data_loader import SensorDataLoader\n",
    "from src.trainer import ModelTrainer\n",
    "from src.inference import ModelInference\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Upload your CSV file to `data/raw/` folder or specify the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from file\n",
    "data_path = 'data/raw/your_sensor_data.csv'  # Update this path\n",
    "\n",
    "# Option 2: Use pre-loaded DataFrame (if you have one)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# data_loader = SensorDataLoader(df=df)\n",
    "\n",
    "data_loader = SensorDataLoader(data_path=data_path)\n",
    "\n",
    "# Display data info\n",
    "print(\"Dataset Info:\")\n",
    "print(data_loader.get_data_info())\n",
    "\n",
    "# Get available signals\n",
    "available_signals = data_loader.get_available_signals()\n",
    "print(f\"\\nAvailable signals ({len(available_signals)}):\")\n",
    "for i, signal in enumerate(available_signals[:10], 1):\n",
    "    print(f\"  {i}. {signal}\")\n",
    "if len(available_signals) > 10:\n",
    "    print(f\"  ... and {len(available_signals) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Model and Signals\n",
    "\n",
    "Select which sensors to use as inputs (boundary conditions) and outputs (targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select signals - UPDATE THESE BASED ON YOUR DATA\n",
    "boundary_signals = available_signals[:5]  # First 5 signals as inputs\n",
    "target_signals = available_signals[5:10]   # Next 5 signals as outputs\n",
    "\n",
    "print(\"Boundary signals (inputs):\")\n",
    "for sig in boundary_signals:\n",
    "    print(f\"  - {sig}\")\n",
    "\n",
    "print(\"\\nTarget signals (outputs):\")\n",
    "for sig in target_signals:\n",
    "    print(f\"  - {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_splits = data_loader.prepare_data(\n",
    "    boundary_signals=boundary_signals,\n",
    "    target_signals=target_signals,\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training samples:   {len(data_splits['X_train'])}\")\n",
    "print(f\"  Validation samples: {len(data_splits['X_val'])}\")\n",
    "print(f\"  Test samples:       {len(data_splits['X_test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train V1 Model (Static Transformer)\n",
    "\n",
    "V1 model learns static relationships between sensors without temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 Model Configuration\n",
    "v1_config = {\n",
    "    'd_model': 128,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'grad_clip': 1.0,\n",
    "    'early_stop_patience': 25,\n",
    "    'scheduler_patience': 10,\n",
    "    'scheduler_factor': 0.5\n",
    "}\n",
    "\n",
    "# Create V1 model\n",
    "v1_model = CompactSensorTransformer(\n",
    "    num_boundary_sensors=len(boundary_signals),\n",
    "    num_target_sensors=len(target_signals),\n",
    "    d_model=v1_config['d_model'],\n",
    "    nhead=v1_config['nhead'],\n",
    "    num_layers=v1_config['num_layers'],\n",
    "    dropout=v1_config['dropout']\n",
    ")\n",
    "\n",
    "print(f\"V1 Model created with {sum(p.numel() for p in v1_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader = data_loader.create_dataloaders(\n",
    "    data_splits['X_train'],\n",
    "    data_splits['y_train'],\n",
    "    data_splits['X_val'],\n",
    "    data_splits['y_val'],\n",
    "    batch_size=v1_config['batch_size']\n",
    ")\n",
    "\n",
    "# Train V1 model\n",
    "v1_trainer = ModelTrainer(v1_model, device=str(device), config=v1_config)\n",
    "v1_history = v1_trainer.train(train_loader, val_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(v1_history['train_losses'], label='Training Loss')\n",
    "plt.plot(v1_history['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('V1 Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.log10(v1_history['train_losses']), label='Log Training Loss')\n",
    "plt.plot(np.log10(v1_history['val_losses']), label='Log Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log10(Loss)')\n",
    "plt.title('V1 Model Training History (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate V1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "v1_inference = ModelInference(\n",
    "    model=v1_model,\n",
    "    scaler_X=data_splits['scaler_X'],\n",
    "    scaler_y=data_splits['scaler_y'],\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test_original = data_splits['scaler_X'].inverse_transform(data_splits['X_test'])\n",
    "y_test_original = data_splits['scaler_y'].inverse_transform(data_splits['y_test'])\n",
    "\n",
    "v1_metrics = v1_inference.evaluate(X_test_original, y_test_original, target_signals)\n",
    "v1_inference.print_metrics(v1_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for first 3 target signals\n",
    "fig = v1_inference.plot_predictions(\n",
    "    X_test_original[:1000],\n",
    "    y_test_original[:1000],\n",
    "    signal_indices=[0, 1, 2],\n",
    "    target_signal_names=target_signals\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train V4 Model (Hybrid Temporal + Static)\n",
    "\n",
    "V4 model combines temporal context analysis with static mapping for improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4 Model Configuration\n",
    "v4_config = {\n",
    "    'd_model': 64,\n",
    "    'nhead': 4,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'context_window': 5,\n",
    "    'use_temporal': True,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'grad_clip': 1.0,\n",
    "    'early_stop_patience': 25,\n",
    "    'scheduler_patience': 10,\n",
    "    'scheduler_factor': 0.5\n",
    "}\n",
    "\n",
    "# Create temporal context data\n",
    "X_train_ctx, y_train_ctx, _ = create_temporal_context_data(\n",
    "    data_splits['X_train'],\n",
    "    data_splits['y_train'],\n",
    "    context_window=v4_config['context_window']\n",
    ")\n",
    "\n",
    "X_val_ctx, y_val_ctx, _ = create_temporal_context_data(\n",
    "    data_splits['X_val'],\n",
    "    data_splits['y_val'],\n",
    "    context_window=v4_config['context_window']\n",
    ")\n",
    "\n",
    "print(f\"Temporal context data shapes:\")\n",
    "print(f\"  Training: {X_train_ctx.shape}\")\n",
    "print(f\"  Validation: {X_val_ctx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create V4 model\n",
    "v4_model = HybridTemporalTransformer(\n",
    "    num_boundary_sensors=len(boundary_signals),\n",
    "    num_target_sensors=len(target_signals),\n",
    "    d_model=v4_config['d_model'],\n",
    "    nhead=v4_config['nhead'],\n",
    "    num_layers=v4_config['num_layers'],\n",
    "    dropout=v4_config['dropout'],\n",
    "    use_temporal=v4_config['use_temporal'],\n",
    "    context_window=v4_config['context_window']\n",
    ")\n",
    "\n",
    "print(f\"V4 Model created with {sum(p.numel() for p in v4_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for V4\n",
    "train_loader_v4, val_loader_v4 = data_loader.create_dataloaders(\n",
    "    X_train_ctx,\n",
    "    y_train_ctx,\n",
    "    X_val_ctx,\n",
    "    y_val_ctx,\n",
    "    batch_size=v4_config['batch_size']\n",
    ")\n",
    "\n",
    "# Train V4 model\n",
    "v4_trainer = ModelTrainer(v4_model, device=str(device), config=v4_config)\n",
    "v4_history = v4_trainer.train(train_loader_v4, val_loader_v4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot V4 training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(v4_history['train_losses'], label='Training Loss')\n",
    "plt.plot(v4_history['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('V4 Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.log10(v4_history['train_losses']), label='Log Training Loss')\n",
    "plt.plot(np.log10(v4_history['val_losses']), label='Log Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log10(Loss)')\n",
    "plt.title('V4 Model Training History (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Models\n",
    "\n",
    "Compare V1 and V4 model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare V4 test data with temporal context\n",
    "X_test_ctx, y_test_ctx, _ = create_temporal_context_data(\n",
    "    data_splits['X_test'],\n",
    "    data_splits['y_test'],\n",
    "    context_window=v4_config['context_window']\n",
    ")\n",
    "\n",
    "# Evaluate V4 model\n",
    "v4_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test_ctx).to(device)\n",
    "    y_pred_v4_scaled = v4_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_v4 = data_splits['scaler_y'].inverse_transform(y_pred_v4_scaled)\n",
    "y_test_v4 = data_splits['scaler_y'].inverse_transform(y_test_ctx)\n",
    "\n",
    "# Compute metrics for V4\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, signal in enumerate(target_signals[:5]):  # Compare first 5 signals\n",
    "    v1_r2 = v1_metrics[signal]['R2']\n",
    "    v1_rmse = v1_metrics[signal]['RMSE']\n",
    "    \n",
    "    v4_r2 = r2_score(y_test_v4[:, i], y_pred_v4[:, i])\n",
    "    v4_rmse = np.sqrt(mean_squared_error(y_test_v4[:, i], y_pred_v4[:, i]))\n",
    "    \n",
    "    print(f\"\\n{signal[:50]}:\")\n",
    "    print(f\"  V1 - R²: {v1_r2:.4f}, RMSE: {v1_rmse:.4f}\")\n",
    "    print(f\"  V4 - R²: {v4_r2:.4f}, RMSE: {v4_rmse:.4f}\")\n",
    "    print(f\"  Improvement: R² {(v4_r2-v1_r2)*100:+.2f}%, RMSE {(v4_rmse-v1_rmse)/v1_rmse*100:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models\n",
    "\n",
    "Save trained models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save V1 model\n",
    "v1_trainer.save_model('models/saved/v1_model.pth')\n",
    "print(\"✅ V1 model saved to models/saved/v1_model.pth\")\n",
    "\n",
    "# Save V4 model\n",
    "v4_trainer.save_model('models/saved/v4_model.pth')\n",
    "print(\"✅ V4 model saved to models/saved/v4_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Saved Models (Optional)\n",
    "\n",
    "Load previously saved models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load V1 model\n",
    "# v1_trainer_loaded = ModelTrainer(v1_model, device=str(device))\n",
    "# v1_trainer_loaded.load_model('models/saved/v1_model.pth')\n",
    "# print(\"✅ V1 model loaded\")\n",
    "\n",
    "# Load V4 model\n",
    "# v4_trainer_loaded = ModelTrainer(v4_model, device=str(device))\n",
    "# v4_trainer_loaded.load_model('models/saved/v4_model.pth')\n",
    "# print(\"✅ V4 model loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
