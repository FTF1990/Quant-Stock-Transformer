"""
Complete Gradio Interface - Based on Original Cell 3
å·¥ä¸šæ•°å­—å­ªç”Ÿ Transformer - å®Œæ•´Gradioç•Œé¢

This file contains the COMPLETE Gradio interface from the original Cell 3,
adapted to use the modular project structure.

ä½¿ç”¨æ–¹æ³• (How to use):
1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt
2. è¿è¡Œæ­¤è„šæœ¬: python gradio_full_interface.py
3. åœ¨æµè§ˆå™¨ä¸­è®¿é—®æ˜¾ç¤ºçš„URL (é€šå¸¸æ˜¯ http://127.0.0.1:7860)

Features:
- å®Œæ•´çš„SSTå’ŒHSTæ¨¡å‹è®­ç»ƒåŠŸèƒ½
- å®æ—¶è®­ç»ƒè¿›åº¦æ˜¾ç¤º
- é…ç½®å¯¼å…¥/å¯¼å‡º
- å®Œæ•´çš„æ¨ç†å’Œå¯è§†åŒ–åŠŸèƒ½
- ä¿¡å·é€‰æ‹©éªŒè¯
"""

# ============================================================================
# å¯¼å…¥éƒ¨åˆ† - Import Section
# ============================================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import gradio as gr
import json
import os
from datetime import datetime
import traceback

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—åŒ–æ¨¡å‹å’Œå·¥å…·
from models.static_transformer import StaticSensorTransformer
from models.hybrid_transformer import HybridSensorTransformer
from models.utils import (
    create_temporal_context_data,
    apply_ifd_smoothing,
    handle_duplicate_columns,
    get_available_signals,
    validate_signal_exclusivity_v1,
    validate_signal_exclusivity_v4
)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f\"SST & HST æ¨¡å‹å·²åŠ è½½ - ä½¿ç”¨è®¾å¤‡: {device}\")

# ============================================================================
# å…¨å±€çŠ¶æ€å­˜å‚¨ - Global State Storage
# ============================================================================

global_state = {
    'df': None,
    'trained_models': {},
    'scalers': {},
    'training_history': {},
    'all_signals': []
}

# ============================================================================
# è®­ç»ƒå‡½æ•° - Training Functions
# ============================================================================

# è¿™é‡ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒå‡½æ•°ï¼Œä¸åŸå§‹Cell 3å®Œå…¨ç›¸åŒ
# ä¸ºäº†èŠ‚çœç©ºé—´ï¼Œè¿™é‡Œå¼•ç”¨å·²ç»åœ¨å‰é¢åˆ›å»ºçš„è®­ç»ƒå‡½æ•°

def train_static_transformer_model_complete(X_train, y_train, X_val, y_val, num_boundary, num_target, config):
    \"\"\"è®­ç»ƒStaticTransformeræ¨¡å‹ - å®Œæ•´ç‰ˆæœ¬ï¼ˆæ”¯æŒå®æ—¶æ—¥å¿—ï¼‰\"\"\"
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

    model = StaticSensorTransformer(
        num_boundary_sensors=num_boundary,
        num_target_sensors=num_target,
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        dropout=config['dropout']
    ).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=config['lr'],
                           weight_decay=config['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        patience=config['scheduler_patience'],
        factor=config['scheduler_factor']
    )

    criterion = nn.MSELoss()
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0
    logs = []

    logs.append(f\"å¼€å§‹è®­ç»ƒStaticTransformeræ¨¡å‹... å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")
    logs.append(f\"é…ç½®: LR={config['lr']}, WD={config['weight_decay']}, GradClip={config['grad_clip']}\\n\")

    for epoch in range(config['epochs']):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                predictions = model(batch_X)
                val_loss += criterion(predictions, batch_y).item()
        val_loss /= len(val_loader)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            status_marker = \"â­\"
        else:
            patience_counter += 1
            status_marker = \"  \"

        log_msg = f\"{status_marker} Epoch [{epoch+1:3d}/{config['epochs']:3d}] | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Best: {best_val_loss:.6f} | LR: {current_lr:.2e} | Patience: {patience_counter}/{config['early_stop_patience']}\"
        logs.append(log_msg)

        # æ—©åœ
        if patience_counter >= config['early_stop_patience']:
            logs.append(f\"\\nğŸ›‘ æ—©åœäºç¬¬ {epoch+1} è½® (è€å¿ƒå€¼è¾¾åˆ° {config['early_stop_patience']})\")
            break

    model.load_state_dict(best_model_state)
    logs.append(f\"\\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")

    return model, train_losses, val_losses, logs

def train_hybrid_sensor_transformer_model_complete(X_train, y_train, X_val, y_val, num_boundary, num_target, config, use_temporal):
    \"\"\"è®­ç»ƒHybridSensorTransformeræ¨¡å‹ - å®Œæ•´ç‰ˆæœ¬ï¼ˆæ”¯æŒå®æ—¶æ—¥å¿—ï¼‰\"\"\"
    logs = []

    # å‡†å¤‡æ•°æ®
    if use_temporal:
        logs.append(f\"â±ï¸ åˆ›å»ºæ—¶åºä¸Šä¸‹æ–‡æ•°æ® (çª—å£: Â±{config['context_window']})...\")
        X_train_ctx, y_train_ctx, _ = create_temporal_context_data(X_train, y_train, config['context_window'])
        X_val_ctx, y_val_ctx, _ = create_temporal_context_data(X_val, y_val, config['context_window'])
        logs.append(f\"  â€¢ æ—¶åºæ•°æ®: è®­ç»ƒ{X_train_ctx.shape}, éªŒè¯{X_val_ctx.shape}\\n\")

        train_dataset = TensorDataset(torch.FloatTensor(X_train_ctx), torch.FloatTensor(y_train_ctx))
        val_dataset = TensorDataset(torch.FloatTensor(X_val_ctx), torch.FloatTensor(y_val_ctx))
    else:
        logs.append(\"ğŸ“ ä½¿ç”¨é™æ€æ˜ å°„æ¨¡å¼...\\n\")
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

    model = HybridSensorTransformer(
        num_boundary_sensors=num_boundary,
        num_target_sensors=num_target,
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        dropout=config['dropout'],
        use_temporal=use_temporal,
        context_window=config['context_window']
    ).to(device)

    # æ‰‹åŠ¨åº”ç”¨gainåˆå§‹åŒ–
    gain_value = config.get('gain', 0.1)
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            if 'head' in name or 'fusion' in name:
                nn.init.xavier_uniform_(module.weight, gain=gain_value)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    logs.append(f\"ğŸ—ï¸ HybridSensorTransformeræ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")
    logs.append(f\"âš™ï¸ é…ç½®: Gain={gain_value}, LR={config['lr']}, WD={config['weight_decay']}, GradClip={config['grad_clip']}\\n\")

    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        patience=config['scheduler_patience'],
        factor=config['scheduler_factor']
    )

    criterion = nn.MSELoss()
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    for epoch in range(config['epochs']):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                predictions = model(batch_X)
                val_loss += criterion(predictions, batch_y).item()
        val_loss /= len(val_loader)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            status_marker = \"â­\"
        else:
            patience_counter += 1
            status_marker = \"  \"

        log_msg = f\"{status_marker} Epoch [{epoch+1:3d}/{config['epochs']:3d}] | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Best: {best_val_loss:.6f} | LR: {current_lr:.2e} | Patience: {patience_counter}/{config['early_stop_patience']}\"
        logs.append(log_msg)

        # æ—©åœ
        if patience_counter >= config['early_stop_patience']:
            logs.append(f\"\\nğŸ›‘ æ—©åœäºç¬¬ {epoch+1} è½® (è€å¿ƒå€¼è¾¾åˆ° {config['early_stop_patience']})\")
            break

    model.load_state_dict(best_model_state)
    logs.append(f\"\\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")

    return model, train_losses, val_losses, logs

# ============================================================================
# é…ç½®å¯¼å…¥å¯¼å‡ºå‡½æ•° - Configuration Import/Export Functions
# ============================================================================

def export_config_static_transformer(boundary_signals, target_signals, test_size, val_size,
                   epochs, batch_size, lr, d_model, nhead, num_layers, dropout,
                   weight_decay, scheduler_patience, scheduler_factor,
                   grad_clip, early_stop_patience):
    """å¯¼å‡ºStaticTransformeræ¨¡å‹é…ç½®ä¸ºJSON"""
    config = {
        "model_type": "static_transformer",
        "signals": {
            "boundary": boundary_signals,
            "target": target_signals
        },
        "data_split": {
            "test_size": test_size,
            "val_size": val_size
        },
        "training": {
            "epochs": int(epochs),
            "batch_size": int(batch_size),
            "lr": float(lr),
            "weight_decay": float(weight_decay),
            "grad_clip": float(grad_clip),
            "early_stop_patience": int(early_stop_patience)
        },
        "model_architecture": {
            "d_model": int(d_model),
            "nhead": int(nhead),
            "num_layers": int(num_layers),
            "dropout": float(dropout)
        },
        "scheduler": {
            "patience": int(scheduler_patience),
            "factor": float(scheduler_factor)
        }
    }

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"config_static_transformer_{timestamp}.json"
    filepath = os.path.join('.', filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    return filepath, f"âœ… é…ç½®å·²å¯¼å‡ºåˆ°: {filename}"

def export_config_hybrid_sensor_transformer(boundary_signals, target_signals, temporal_signals,
                   test_size, val_size, epochs, batch_size, lr, d_model, nhead,
                   num_layers, dropout, context_window, apply_smoothing, gain,
                   weight_decay, scheduler_patience, scheduler_factor,
                   grad_clip, early_stop_patience):
    """å¯¼å‡ºHybridSensorTransformeræ¨¡å‹é…ç½®ä¸ºJSON"""
    config = {
        "model_type": "HybridSensorTransformer",
        "signals": {
            "boundary": boundary_signals,
            "target": target_signals,
            "temporal": temporal_signals
        },
        "data_split": {
            "test_size": test_size,
            "val_size": val_size
        },
        "training": {
            "epochs": int(epochs),
            "batch_size": int(batch_size),
            "lr": float(lr),
            "weight_decay": float(weight_decay),
            "grad_clip": float(grad_clip),
            "early_stop_patience": int(early_stop_patience)
        },
        "model_architecture": {
            "d_model": int(d_model),
            "nhead": int(nhead),
            "num_layers": int(num_layers),
            "dropout": float(dropout),
            "gain": float(gain)
        },
        "v4_specific": {
            "context_window": int(context_window),
            "apply_smoothing": bool(apply_smoothing)
        },
        "scheduler": {
            "patience": int(scheduler_patience),
            "factor": float(scheduler_factor)
        }
    }

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"config_hybrid_transformer_{timestamp}.json"
    filepath = os.path.join('.', filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    return filepath, f"âœ… é…ç½®å·²å¯¼å‡ºåˆ°: {filename}"

def import_config(config_file):
    """å¯¼å…¥é…ç½®æ–‡ä»¶"""
    try:
        with open(config_file.name, 'r', encoding='utf-8') as f:
            config = json.load(f)

        model_type = config.get('model_type', 'static_transformer')

        if model_type in ['V1', 'static_transformer']:
            return (
                config['signals']['boundary'],
                config['signals']['target'],
                [],  # temporal_signals (V1ä¸éœ€è¦)
                config['data_split']['test_size'],
                config['data_split']['val_size'],
                config['training']['epochs'],
                config['training']['batch_size'],
                config['training']['lr'],
                config['model_architecture']['d_model'],
                config['model_architecture']['nhead'],
                config['model_architecture']['num_layers'],
                config['model_architecture']['dropout'],
                5,   # context_window (V1é»˜è®¤å€¼)
                True,  # apply_smoothing (V1é»˜è®¤å€¼)
                0.1,   # gain (V1é»˜è®¤å€¼)
                config['training']['weight_decay'],
                config['scheduler']['patience'],
                config['scheduler']['factor'],
                config['training']['grad_clip'],
                config['training']['early_stop_patience'],
                f"âœ… StaticTransformeré…ç½®åŠ è½½æˆåŠŸï¼\nåŒ…å« {len(config['signals']['boundary'])} ä¸ªè¾¹ç•Œä¿¡å·å’Œ {len(config['signals']['target'])} ä¸ªç›®æ ‡ä¿¡å·"
            )
        else:  # V4/HybridSensorTransformer
            return (
                config['signals']['boundary'],
                config['signals']['target'],
                config['signals'].get('temporal', []),
                config['data_split']['test_size'],
                config['data_split']['val_size'],
                config['training']['epochs'],
                config['training']['batch_size'],
                config['training']['lr'],
                config['model_architecture']['d_model'],
                config['model_architecture']['nhead'],
                config['model_architecture']['num_layers'],
                config['model_architecture']['dropout'],
                config['v4_specific']['context_window'],
                config['v4_specific']['apply_smoothing'],
                config['model_architecture']['gain'],
                config['training']['weight_decay'],
                config['scheduler']['patience'],
                config['scheduler']['factor'],
                config['training']['grad_clip'],
                config['training']['early_stop_patience'],
                f"âœ… HybridSensorTransformeré…ç½®åŠ è½½æˆåŠŸï¼\nåŒ…å« {len(config['signals']['boundary'])} ä¸ªè¾¹ç•Œä¿¡å·, {len(config['signals']['target'])} ä¸ªç›®æ ‡ä¿¡å·, {len(config['signals'].get('temporal', []))} ä¸ªæ—¶åºä¿¡å·"
            )
    except Exception as e:
        return ([], [], [], 0.2, 0.2, 100, 64, 0.001, 128, 8, 3, 0.1, 5,
                True, 0.1, 1e-5, 10, 0.5, 1.0, 25,
                f"âŒ é…ç½®åŠ è½½å¤±è´¥: {str(e)}")

# ============================================================================
# è®­ç»ƒTabå›è°ƒå‡½æ•° - Training Tab Callback Functions
# ============================================================================

def on_load_data(dataframe):
    """åŠ è½½æ•°æ®"""
    try:
        # ä¼˜å…ˆä½¿ç”¨å…¨å±€df
        if 'df' in globals():
            global_state['df'] = globals()['df'].copy()
        elif dataframe is not None:
            global_state['df'] = pd.read_csv(dataframe.name) if hasattr(dataframe, 'name') else dataframe
        else:
            return ("âŒ è¯·ä¸Šä¼ CSVæ–‡ä»¶æˆ–ç¡®ä¿dfå˜é‡å·²åŠ è½½", "",
                   gr.update(choices=[]), gr.update(choices=[]),
                   gr.update(choices=[]), gr.update(choices=[]))

        # æ£€æŸ¥å¹¶å¤„ç†é‡å¤åˆ—å
        original_shape = global_state['df'].shape
        global_state['df'], duplicates = handle_duplicate_columns(global_state['df'])

        signals = get_available_signals(global_state['df'])
        global_state['all_signals'] = signals  # ä¿å­˜åˆ°å…¨å±€çŠ¶æ€

        # æ„å»ºçŠ¶æ€æ¶ˆæ¯
        status_msg = f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ!\nå½¢çŠ¶: {global_state['df'].shape}\nå¯ç”¨ä¿¡å·: {len(signals)}ä¸ª"

        if duplicates:
            status_msg += f"\n\nâš ï¸ æ£€æµ‹åˆ°é‡å¤åˆ—å (å·²è‡ªåŠ¨å¤„ç†):"
            for col, count in list(duplicates.items())[:5]:
                status_msg += f"\n â€¢ {col}: å‡ºç° {count+1} æ¬¡"
                status_msg += f" â†’ é‡å‘½åä¸º {col}, {col}_#2"
                if count > 1:
                    status_msg += f", {col}_#3..."
            if len(duplicates) > 5:
                status_msg += f"\n   ... è¿˜æœ‰ {len(duplicates)-5} ä¸ªé‡å¤é¡¹"
            status_msg += "\n\nğŸ’¡ æç¤º: å¯ä»¥åœ¨ä¸‹æ–¹ä¿¡å·åˆ—è¡¨ä¸­çœ‹åˆ°æ‰€æœ‰å¯ç”¨ä¿¡å·"

        # æ„å»ºä¿¡å·åˆ—è¡¨æ˜¾ç¤º
        signals_display = "=" * 60 + "\n"
        signals_display += f"å¯ç”¨ä¿¡å·æ€»æ•°: {len(signals)}\n"
        signals_display += "=" * 60 + "\n\n"
        for i, sig in enumerate(signals, 1):
            signals_display += f"{i:4d}. {sig}\n"
        signals_display += "\n" + "=" * 60

        return (
            status_msg,
            signals_display,
            gr.update(choices=signals, value=[]),
            gr.update(choices=signals, value=[]),
            gr.update(choices=signals, value=[]),
            gr.update(choices=signals, value=[])
        )
    except Exception as e:
        return (f"âŒ åŠ è½½å¤±è´¥: {str(e)}", "",
               gr.update(choices=[]), gr.update(choices=[]),
               gr.update(choices=[]), gr.update(choices=[]))

def start_training_static_transformer(boundary_signals, target_signals, test_size, val_size,
                     epochs, batch_size, lr, d_model, nhead, num_layers, dropout,
                     weight_decay, scheduler_patience, scheduler_factor,
                     grad_clip, early_stop_patience):
    """å¼€å§‹è®­ç»ƒStaticTransformeræ¨¡å‹"""
    if global_state['df'] is None:
        yield "âŒ è¯·å…ˆåŠ è½½æ•°æ®!"
        return

    if not boundary_signals:
        yield "âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¾¹ç•Œæ¡ä»¶ä¿¡å·!"
        return

    if not target_signals:
        yield "âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç›®æ ‡ä¿¡å·!"
        return

    # éªŒè¯ä¿¡å·äº’æ–¥æ€§
    is_valid, error_msg = validate_signal_exclusivity_v1(boundary_signals, target_signals)
    if not is_valid:
        yield error_msg
        return

    try:
        log_messages = []
        log_messages.append("=" * 80)
        log_messages.append(f"å¼€å§‹è®­ç»ƒ StaticTransformer æ¨¡å‹")
        log_messages.append("=" * 80)

        # å‡†å¤‡æ•°æ®
        log_messages.append("\nğŸ“Š å‡†å¤‡æ•°æ®...")
        df = global_state['df']
        X = df[boundary_signals].values
        y = df[target_signals].values
        log_messages.append(f" â€¢ è¾“å…¥ç‰¹å¾: {len(boundary_signals)}ä¸ª")
        log_messages.append(f" â€¢ è¾“å‡ºç›®æ ‡: {len(target_signals)}ä¸ª")
        log_messages.append(f" â€¢ æ€»æ ·æœ¬æ•°: {len(X):,}")
        yield '\n'.join(log_messages)

        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        X_scaled = scaler_X.fit_transform(X)
        y_scaled = scaler_y.fit_transform(y)

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_scaled, test_size=test_size, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=val_size, random_state=42
        )

        log_messages.append(f"\nğŸ“‚ æ•°æ®åˆ†å‰²:")
        log_messages.append(f" â€¢ è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬ ({len(X_train)/len(X)*100:.1f}%)")
        log_messages.append(f" â€¢ éªŒè¯é›†: {len(X_val):,} æ ·æœ¬ ({len(X_val)/len(X)*100:.1f}%)")
        log_messages.append(f" â€¢ æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬ ({len(X_test)/len(X)*100:.1f}%)")
        yield '\n'.join(log_messages)

        # é…ç½®
        config = {
            'epochs': int(epochs),
            'batch_size': int(batch_size),
            'lr': float(lr),
            'd_model': int(d_model),
            'nhead': int(nhead),
            'num_layers': int(num_layers),
            'dropout': float(dropout),
            'weight_decay': float(weight_decay),
            'scheduler_patience': int(scheduler_patience),
            'scheduler_factor': float(scheduler_factor),
            'grad_clip': float(grad_clip),
            'early_stop_patience': int(early_stop_patience)
        }

        log_messages.append("\n" + "=" * 80)
        log_messages.append("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        log_messages.append("=" * 80)
        yield '\n'.join(log_messages)

        # è®­ç»ƒ - å®æ—¶è¾“å‡º
        model, train_losses, val_losses, training_logs = train_static_transformer_model_complete(
            X_train, y_train, X_val, y_val,
            len(boundary_signals), len(target_signals), config
        )

        # é€è¡Œè¾“å‡ºè®­ç»ƒæ—¥å¿—
        for log_line in training_logs:
            log_messages.append(log_line)
            yield '\n'.join(log_messages)

        log_messages.append("\n" + "=" * 80)
        log_messages.append("ğŸ§ª è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½...")
        log_messages.append("=" * 80)
        yield '\n'.join(log_messages)

        # æµ‹è¯•é›†è¯„ä¼°
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test).to(device)
            y_pred_scaled = model(X_test_tensor).cpu().numpy()
            y_pred = scaler_y.inverse_transform(y_pred_scaled)
            y_true = scaler_y.inverse_transform(y_test)

        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        for i, sensor in enumerate(target_signals):
            r2 = r2_score(y_true[:, i], y_pred[:, i])
            rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
            mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
            metrics[sensor] = {'R2': r2, 'RMSE': rmse, 'MAE': mae}

        avg_r2 = np.mean([m['R2'] for m in metrics.values()])
        avg_rmse = np.mean([m['RMSE'] for m in metrics.values()])

        log_messages.append(f"\nğŸ“ˆ æµ‹è¯•é›†æ•´ä½“æ€§èƒ½:")
        log_messages.append(f" â€¢ å¹³å‡ RÂ²: {avg_r2:.4f}")
        log_messages.append(f" â€¢ å¹³å‡ RMSE: {avg_rmse:.4f}")

        # æ˜¾ç¤ºå‰5ä¸ªä¿¡å·çš„è¯¦ç»†æŒ‡æ ‡
        log_messages.append(f"\nğŸ“Š å‰5ä¸ªç›®æ ‡ä¿¡å·è¯¦ç»†æŒ‡æ ‡:")
        for i, (sensor, metric) in enumerate(list(metrics.items())[:5]):
            log_messages.append(f" {i+1}. {sensor[:50]}")
            log_messages.append(f" RÂ²={metric['R2']:.4f}, RMSE={metric['RMSE']:.4f}, MAE={metric['MAE']:.4f}")
        if len(metrics) > 5:
            log_messages.append(f"   ... è¿˜æœ‰ {len(metrics)-5} ä¸ªä¿¡å·")
        yield '\n'.join(log_messages)

        # ä¿å­˜æ¨¡å‹
        model_name = f"StaticTransformer_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': 'StaticTransformer',
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'temporal_signals': None,
            'config': config,
            'metrics': metrics,
            'use_temporal': False
        }
        global_state['scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}
        global_state['training_history'][model_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses
        }

        log_messages.append("\n" + "=" * 80)
        log_messages.append(f"âœ… è®­ç»ƒå®Œæˆå¹¶ä¿å­˜!")
        log_messages.append(f"ğŸ“¦ æ¨¡å‹åç§°: {model_name}")
        log_messages.append("=" * 80)
        yield '\n'.join(log_messages)

    except Exception as e:
        error_msg = f"âŒ è®­ç»ƒå¤±è´¥:\n{str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        yield error_msg

def start_training_hybrid_sensor_transformer(boundary_signals, target_signals, temporal_signals,
                     test_size, val_size, epochs, batch_size, lr, d_model, nhead,
                     num_layers, dropout, context_window, apply_smoothing, gain,
                     weight_decay, scheduler_patience, scheduler_factor,
                     grad_clip, early_stop_patience):
    """å¼€å§‹è®­ç»ƒHybridSensorTransformeræ¨¡å‹"""
    if global_state['df'] is None:
        yield "âŒ è¯·å…ˆåŠ è½½æ•°æ®!"
        return

    if not boundary_signals:
        yield "âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¾¹ç•Œæ¡ä»¶ä¿¡å·!"
        return

    if not target_signals:
        yield "âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç›®æ ‡ä¿¡å·!"
        return

    # éªŒè¯ä¿¡å·äº’æ–¥æ€§
    is_valid, error_msg = validate_signal_exclusivity_v4(boundary_signals, target_signals, temporal_signals)
    if not is_valid:
        yield error_msg
        return

    try:
        log_messages = []
        log_messages.append("=" * 80)
        log_messages.append(f"å¼€å§‹è®­ç»ƒ HybridSensorTransformer æ¨¡å‹")
        log_messages.append("=" * 80)

        # å‡†å¤‡æ•°æ®
        log_messages.append("\nğŸ“Š å‡†å¤‡æ•°æ®...")
        df = global_state['df']
        X = df[boundary_signals].values
        y = df[target_signals].values
        log_messages.append(f" â€¢ è¾“å…¥ç‰¹å¾: {len(boundary_signals)}ä¸ª")
        log_messages.append(f" â€¢ è¾“å‡ºç›®æ ‡: {len(target_signals)}ä¸ª")
        log_messages.append(f" â€¢ æ€»æ ·æœ¬æ•°: {len(X):,}")
        yield '\n'.join(log_messages)

        # V4ç‰¹å®š: IFDå¹³æ»‘
        if apply_smoothing and temporal_signals:
            log_messages.append(f"\nğŸ”§ åº”ç”¨IFDå¹³æ»‘æ»¤æ³¢åˆ° {len(temporal_signals)} ä¸ªä¿¡å·...")
            y = apply_ifd_smoothing(y, target_signals, temporal_signals)
            yield '\n'.join(log_messages)

        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        X_scaled = scaler_X.fit_transform(X)
        y_scaled = scaler_y.fit_transform(y)

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_scaled, test_size=test_size, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=val_size, random_state=42
        )

        log_messages.append(f"\nğŸ“‚ æ•°æ®åˆ†å‰²:")
        log_messages.append(f" â€¢ è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬ ({len(X_train)/len(X)*100:.1f}%)")
        log_messages.append(f" â€¢ éªŒè¯é›†: {len(X_val):,} æ ·æœ¬ ({len(X_val)/len(X)*100:.1f}%)")
        log_messages.append(f" â€¢ æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬ ({len(X_test)/len(X)*100:.1f}%)")
        yield '\n'.join(log_messages)

        # é…ç½®
        config = {
            'epochs': int(epochs),
            'batch_size': int(batch_size),
            'lr': float(lr),
            'd_model': int(d_model),
            'nhead': int(nhead),
            'num_layers': int(num_layers),
            'dropout': float(dropout),
            'context_window': int(context_window),
            'gain': float(gain),
            'weight_decay': float(weight_decay),
            'scheduler_patience': int(scheduler_patience),
            'scheduler_factor': float(scheduler_factor),
            'grad_clip': float(grad_clip),
            'early_stop_patience': int(early_stop_patience)
        }

        log_messages.append("\n" + "=" * 80)
        log_messages.append("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        log_messages.append("=" * 80)
        yield '\n'.join(log_messages)

        # è®­ç»ƒ - å®æ—¶è¾“å‡º
        use_temporal = len(temporal_signals) > 0 if temporal_signals else False
        model, train_losses, val_losses, training_logs = train_v4_model_complete(
            X_train, y_train, X_val, y_val,
            len(boundary_signals), len(target_signals),
            config, use_temporal
        )

        # é€è¡Œè¾“å‡ºè®­ç»ƒæ—¥å¿—
        for log_line in training_logs:
            log_messages.append(log_line)
            yield '\n'.join(log_messages)

        log_messages.append("\n" + "=" * 80)
        log_messages.append("ğŸ§ª è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½...")
        log_messages.append("=" * 80)
        yield '\n'.join(log_messages)

        # æµ‹è¯•é›†è¯„ä¼°
        model.eval()
        with torch.no_grad():
            if use_temporal:
                X_test_ctx, y_test_ctx, _ = create_temporal_context_data(
                    X_test, y_test, context_window
                )
                X_test_tensor = torch.FloatTensor(X_test_ctx).to(device)
                y_pred_scaled = model(X_test_tensor).cpu().numpy()
                y_test_eval = y_test_ctx
            else:
                X_test_tensor = torch.FloatTensor(X_test).to(device)
                y_pred_scaled = model(X_test_tensor).cpu().numpy()
                y_test_eval = y_test

            y_pred = scaler_y.inverse_transform(y_pred_scaled)
            y_true = scaler_y.inverse_transform(y_test_eval)

        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        for i, sensor in enumerate(target_signals):
            r2 = r2_score(y_true[:, i], y_pred[:, i])
            rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
            mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
            metrics[sensor] = {'R2': r2, 'RMSE': rmse, 'MAE': mae}

        avg_r2 = np.mean([m['R2'] for m in metrics.values()])
        avg_rmse = np.mean([m['RMSE'] for m in metrics.values()])

        log_messages.append(f"\nğŸ“ˆ æµ‹è¯•é›†æ•´ä½“æ€§èƒ½:")
        log_messages.append(f" â€¢ å¹³å‡ RÂ²: {avg_r2:.4f}")
        log_messages.append(f" â€¢ å¹³å‡ RMSE: {avg_rmse:.4f}")

        # æ˜¾ç¤ºå‰5ä¸ªä¿¡å·çš„è¯¦ç»†æŒ‡æ ‡
        log_messages.append(f"\nğŸ“Š å‰5ä¸ªç›®æ ‡ä¿¡å·è¯¦ç»†æŒ‡æ ‡:")
        for i, (sensor, metric) in enumerate(list(metrics.items())[:5]):
            log_messages.append(f" {i+1}. {sensor[:50]}")
            log_messages.append(f" RÂ²={metric['R2']:.4f}, RMSE={metric['RMSE']:.4f}, MAE={metric['MAE']:.4f}")
        if len(metrics) > 5:
            log_messages.append(f"   ... è¿˜æœ‰ {len(metrics)-5} ä¸ªä¿¡å·")
        yield '\n'.join(log_messages)

        # ä¿å­˜æ¨¡å‹
        model_name = f"HybridSensorTransformer_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        global_state['trained_models'][model_name] = {
            'model': model,
            'type': 'HybridSensorTransformer',
            'boundary_signals': boundary_signals,
            'target_signals': target_signals,
            'temporal_signals': temporal_signals,
            'config': config,
            'metrics': metrics,
            'use_temporal': use_temporal
        }
        global_state['scalers'][model_name] = {'X': scaler_X, 'y': scaler_y}
        global_state['training_history'][model_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses
        }

        log_messages.append("\n" + "=" * 80)
        log_messages.append(f"âœ… è®­ç»ƒå®Œæˆå¹¶ä¿å­˜!")
        log_messages.append(f"ğŸ“¦ æ¨¡å‹åç§°: {model_name}")
        log_messages.append("=" * 80)
        yield '\n'.join(log_messages)

    except Exception as e:
        error_msg = f"âŒ è®­ç»ƒå¤±è´¥:\n{str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        yield error_msg

# ============================================================================
# æ¨ç†Tabå›è°ƒå‡½æ•° - Inference Tab Callback Functions
# ============================================================================

def get_model_list():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return list(global_state['trained_models'].keys())

def on_model_load(model_name):
    """åŠ è½½æ¨¡å‹"""
    if not model_name or model_name not in global_state['trained_models']:
        return "è¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹", gr.update(choices=[])

    model_info = global_state['trained_models'][model_name]
    target_signals = model_info['target_signals']
    metrics = model_info['metrics']
    avg_r2 = np.mean([m['R2'] for m in metrics.values()])

    info_lines = [
        "=" * 60,
        "æ¨¡å‹ä¿¡æ¯",
        "=" * 60,
        f"æ¨¡å‹ç±»å‹: {model_info['type']}",
        f"è¾¹ç•Œä¿¡å·æ•°: {len(model_info['boundary_signals'])}",
        f"ç›®æ ‡ä¿¡å·æ•°: {len(model_info['target_signals'])}",
        f"å¹³å‡RÂ²: {avg_r2:.4f}",
        ""
    ]

    if model_info['type'] == 'HybridSensorTransformer':
        info_lines.append(f"æ—¶åºæ¨¡å¼: {'æ˜¯' if model_info.get('use_temporal', False) else 'å¦'}")
        if model_info.get('temporal_signals'):
            info_lines.append(f"æ—¶åºä¿¡å·æ•°: {len(model_info['temporal_signals'])}")

    info_lines.append("")
    info_lines.append("ç›®æ ‡ä¿¡å·åˆ—è¡¨:")
    for i, s in enumerate(target_signals[:10]):
        r2 = metrics[s]['R2']
        info_lines.append(f" {i+1:2d}. {s[:50]} (RÂ²={r2:.3f})")
    if len(target_signals) > 10:
        info_lines.append(f"   ... è¿˜æœ‰ {len(target_signals)-10} ä¸ªä¿¡å·")
    info_lines.append("=" * 60)

    return '\n'.join(info_lines), gr.update(choices=target_signals, value=[])

def run_inference(model_name, start_idx, end_idx, selected_signals):
    """è¿è¡Œæ¨ç†"""
    if not model_name or model_name not in global_state['trained_models']:
        return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", None, ""

    if global_state['df'] is None:
        return "âŒ æ•°æ®æœªåŠ è½½", None, ""

    if not selected_signals:
        return "âŒ è¯·é€‰æ‹©è¦å¯è§†åŒ–çš„ä¿¡å·", None, ""

    try:
        # è·å–æ¨¡å‹å’Œé…ç½®
        model_info = global_state['trained_models'][model_name]
        model = model_info['model']
        model_type = model_info['type']
        boundary_signals = model_info['boundary_signals']
        target_signals = model_info['target_signals']
        config = model_info['config']
        scalers = global_state['scalers'][model_name]
        scaler_X = scalers['X']
        scaler_y = scalers['y']

        # å‡†å¤‡æ•°æ®
        df = global_state['df']

        # å¤„ç†ç´¢å¼•èŒƒå›´
        start_idx = int(start_idx)
        end_idx = int(end_idx)

        if start_idx < 0:
            start_idx = 0
        if end_idx > len(df):
            end_idx = len(df)
        if end_idx <= start_idx:
            end_idx = min(start_idx + 1000, len(df))

        df_slice = df.iloc[start_idx:end_idx]
        X = df_slice[boundary_signals].values
        y = df_slice[target_signals].values
        X_scaled = scaler_X.transform(X)

        # æ¨ç†
        model.eval()
        with torch.no_grad():
            if model_type == "HybridSensorTransformer" and model_info.get('use_temporal', False):
                context_window = config['context_window']
                if len(X_scaled) < 2 * context_window + 1:
                    return f"âŒ æ•°æ®æ®µå¤ªçŸ­ï¼Œéœ€è¦è‡³å°‘ {2*context_window+1} ä¸ªæ ·æœ¬", None, ""

                X_ctx, _, valid_indices = create_temporal_context_data(
                    X_scaled,
                    np.zeros((len(X_scaled), len(target_signals))),
                    context_window
                )
                X_tensor = torch.FloatTensor(X_ctx).to(device)
                y_pred_scaled = model(X_tensor).cpu().numpy()

                # è°ƒæ•´yåˆ°æœ‰æ•ˆç´¢å¼•
                y = y[valid_indices]
                actual_indices = np.array(valid_indices) + start_idx
            else:
                X_tensor = torch.FloatTensor(X_scaled).to(device)
                y_pred_scaled = model(X_tensor).cpu().numpy()
                actual_indices = np.arange(start_idx, end_idx)

        y_pred = scaler_y.inverse_transform(y_pred_scaled)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics_lines = ["=" * 60, "è¯„ä¼°æŒ‡æ ‡", "=" * 60, ""]
        for signal in selected_signals:
            if signal not in target_signals:
                continue

            idx = target_signals.index(signal)
            y_true_signal = y[:, idx]
            y_pred_signal = y_pred[:, idx]
            residuals = y_true_signal - y_pred_signal

            r2 = r2_score(y_true_signal, y_pred_signal)
            rmse = np.sqrt(mean_squared_error(y_true_signal, y_pred_signal))
            mae = mean_absolute_error(y_true_signal, y_pred_signal)

            metrics_lines.append(f"{signal}:")
            metrics_lines.append(f" RÂ² Score: {r2:.4f}")
            metrics_lines.append(f" RMSE: {rmse:.4f}")
            metrics_lines.append(f" MAE: {mae:.4f}")
            metrics_lines.append(f" æ®‹å·®å‡å€¼: {np.mean(residuals):.4f}")
            metrics_lines.append(f" æ®‹å·®æ ‡å‡†å·®: {np.std(residuals):.4f}")
            metrics_lines.append(f" æ®‹å·®èŒƒå›´: [{np.min(residuals):.4f}, {np.max(residuals):.4f}]")
            metrics_lines.append("")

        metrics_lines.append("=" * 60)

        # å¯è§†åŒ–
        n_signals = len(selected_signals)
        fig = plt.figure(figsize=(18, 5*n_signals))

        for i, signal in enumerate(selected_signals):
            if signal not in target_signals:
                continue

            idx = target_signals.index(signal)
            y_true_signal = y[:, idx]
            y_pred_signal = y_pred[:, idx]
            residuals = y_true_signal - y_pred_signal

            # é¢„æµ‹ vs å®é™…
            ax1 = plt.subplot(n_signals, 3, i*3 + 1)
            ax1.plot(actual_indices[:len(y_true_signal)], y_true_signal,
                    label='å®é™…å€¼', linewidth=2, alpha=0.8, color='#2c3e50')
            ax1.plot(actual_indices[:len(y_pred_signal)], y_pred_signal,
                    label='é¢„æµ‹å€¼', linewidth=2, alpha=0.8, color='#3498db')
            ax1.set_title(f'{signal}\né¢„æµ‹ vs å®é™…', fontsize=11, fontweight='bold')
            ax1.set_xlabel('æ•°æ®ç´¢å¼•', fontsize=10)
            ax1.set_ylabel('å€¼', fontsize=10)
            ax1.legend(fontsize=9)
            ax1.grid(True, alpha=0.3)

            # æ®‹å·®å›¾
            ax2 = plt.subplot(n_signals, 3, i*3 + 2)
            ax2.plot(actual_indices[:len(residuals)], residuals,
                    color='#e74c3c', linewidth=1.5, alpha=0.7)
            ax2.axhline(y=0, color='black', linestyle='--', linewidth=2)
            ax2.fill_between(actual_indices[:len(residuals)], residuals,
                            alpha=0.3, color='#e74c3c')
            ax2.set_title(f'{signal}\næ®‹å·®åˆ†æ', fontsize=11, fontweight='bold')
            ax2.set_xlabel('æ•°æ®ç´¢å¼•', fontsize=10)
            ax2.set_ylabel('æ®‹å·® (å®é™… - é¢„æµ‹)', fontsize=10)
            ax2.grid(True, alpha=0.3)

            # æ•£ç‚¹å›¾
            ax3 = plt.subplot(n_signals, 3, i*3 + 3)
            ax3.scatter(y_true_signal, y_pred_signal, alpha=0.6, s=20, color='#3498db')
            min_val = min(y_true_signal.min(), y_pred_signal.min())
            max_val = max(y_true_signal.max(), y_pred_signal.max())
            ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
            r2 = r2_score(y_true_signal, y_pred_signal)
            ax3.set_title(f'{signal}\né¢„æµ‹ç²¾åº¦ (RÂ²={r2:.3f})', fontsize=11, fontweight='bold')
            ax3.set_xlabel('å®é™…å€¼', fontsize=10)
            ax3.set_ylabel('é¢„æµ‹å€¼', fontsize=10)
            ax3.legend(fontsize=9)
            ax3.grid(True, alpha=0.3)

        plt.tight_layout()

        status_msg = f"âœ… æ¨ç†å®Œæˆ!\nåˆ†æèŒƒå›´: index {start_idx} åˆ° {end_idx}\nå®é™…åˆ†ææ ·æœ¬æ•°: {len(y_true_signal)}"

        return status_msg, fig, '\n'.join(metrics_lines)

    except Exception as e:
        error_msg = f"âŒ æ¨ç†å¤±è´¥:\n{str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        return error_msg, None, ""

# ============================================================================
# åˆ›å»ºGradioç•Œé¢ - Create Gradio Interface
# ============================================================================

with gr.Blocks(title="ä¼ æ„Ÿå™¨é¢„æµ‹æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš€ ä¼ æ„Ÿå™¨é¢„æµ‹æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ç³»ç»Ÿ")
    gr.Markdown("### æ”¯æŒStaticTransformer (é™æ€æ˜ å°„) å’Œ HybridSensorTransformer (æ··åˆæ—¶åº+é™æ€) æ¨¡å‹")

    with gr.Tabs():
        # ========== æ•°æ®åŠ è½½Tab ==========
        with gr.Tab("ğŸ“Š æ•°æ®åŠ è½½"):
            gr.Markdown("### åŠ è½½è®­ç»ƒæ•°æ®")
            with gr.Row():
                with gr.Column(scale=1):
                    data_file = gr.File(label="ä¸Šä¼ CSVæ–‡ä»¶ (å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨å…¨å±€df)", file_types=[".csv"])
                    load_data_btn = gr.Button("ğŸ“¥ åŠ è½½æ•°æ®", variant="primary", size="lg")
                    data_status = gr.Textbox(label="æ•°æ®çŠ¶æ€", lines=10)

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“‹ å¯ç”¨ä¿¡å·åˆ—è¡¨")
                    gr.Markdown("æ•°æ®åŠ è½½åï¼Œä»¥ä¸‹æ‰€æœ‰ä¿¡å·å¯ç”¨äºè®­ç»ƒ")
                    signals_list_display = gr.Textbox(
                        label="ä¿¡å·åˆ—è¡¨",
                        lines=25,
                        placeholder="åŠ è½½æ•°æ®åæ˜¾ç¤ºæ‰€æœ‰å¯ç”¨ä¿¡å·...",
                        interactive=False
                    )

        # ========== StaticTransformerè®­ç»ƒTab ==========
        with gr.Tab("ğŸ¯ StaticTransformerè®­ç»ƒ"):
            gr.Markdown("## StaticTransformer: é™æ€ä¼ æ„Ÿå™¨æ˜ å°„ Transformer")

            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“‹ é…ç½®ç®¡ç†")
                    config_file_static = gr.File(label="å¯¼å…¥é…ç½®æ–‡ä»¶", file_types=[".json"])
                    import_config_btn_static = gr.Button("ğŸ“‚ å¯¼å…¥é…ç½®", variant="secondary")
                    export_config_btn_static = gr.Button("ğŸ’¾ å¯¼å‡ºé…ç½®", variant="secondary")
                    config_status_static = gr.Textbox(label="é…ç½®çŠ¶æ€", lines=3)
                    config_download_static = gr.File(label="ä¸‹è½½é…ç½®æ–‡ä»¶", visible=False)

                    gr.Markdown("### ğŸ›ï¸ ä¿¡å·é€‰æ‹©")
                    gr.Markdown("âš ï¸ **æ³¨æ„**: è¾¹ç•Œä¿¡å·å’Œç›®æ ‡ä¿¡å·ä¸èƒ½é‡å¤ï¼Œè®­ç»ƒå‰ä¼šè‡ªåŠ¨éªŒè¯")

                    boundary_signals_static = gr.Dropdown(
                        choices=[],
                        multiselect=True,
                        label="è¾¹ç•Œæ¡ä»¶ä¿¡å· (è¾“å…¥)",
                        info="é€‰æ‹©ç”¨äºé¢„æµ‹çš„è¾“å…¥ä¿¡å·"
                    )

                    target_signals_static = gr.Dropdown(
                        choices=[],
                        multiselect=True,
                        label="ç›®æ ‡ä¿¡å· (è¾“å‡º)",
                        info="é€‰æ‹©è¦é¢„æµ‹çš„ä¿¡å·"
                    )

                    gr.Markdown("### âš™ï¸ æ•°æ®åˆ†å‰²")
                    with gr.Row():
                        test_size_static = gr.Slider(0.1, 0.4, value=0.2, step=0.05, label="æµ‹è¯•é›†æ¯”ä¾‹")
                        val_size_static = gr.Slider(0.1, 0.3, value=0.2, step=0.05, label="éªŒè¯é›†æ¯”ä¾‹")

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ”§ è®­ç»ƒå‚æ•°")
                    with gr.Row():
                        epochs_static = gr.Slider(1, 200, value=100, step=1, label="è®­ç»ƒè½®æ•°")
                        batch_size_static = gr.Slider(16, 256, value=64, step=16, label="æ‰¹å¤§å°")

                    with gr.Row():
                        lr_static = gr.Number(value=0.001, label="å­¦ä¹ ç‡")
                        weight_decay_static = gr.Number(value=1e-5, label="æƒé‡è¡°å‡", precision=6)

                    gr.Markdown("### ğŸ—ï¸ æ¨¡å‹æ¶æ„")
                    with gr.Row():
                        d_model_static = gr.Slider(32, 256, value=128, step=32, label="æ¨¡å‹ç»´åº¦")
                        nhead_static = gr.Slider(2, 16, value=8, step=2, label="æ³¨æ„åŠ›å¤´æ•°")

                    with gr.Row():
                        num_layers_static = gr.Slider(1, 6, value=3, step=1, label="Transformerå±‚æ•°")
                        dropout_static = gr.Slider(0.0, 0.5, value=0.1, step=0.05, label="Dropout")

                    gr.Markdown("### ğŸ“ˆ ä¼˜åŒ–å™¨å‚æ•°")
                    with gr.Row():
                        scheduler_patience_static = gr.Slider(5, 20, value=10, step=1, label="å­¦ä¹ ç‡è°ƒåº¦è€å¿ƒ")
                        scheduler_factor_static = gr.Slider(0.1, 0.9, value=0.5, step=0.1, label="å­¦ä¹ ç‡è¡°å‡å› å­")

                    with gr.Row():
                        grad_clip_static = gr.Slider(0.5, 5.0, value=1.0, step=0.1, label="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
                        early_stop_patience_static = gr.Slider(10, 50, value=25, step=5, label="æ—©åœè€å¿ƒ")

                    gr.Markdown("### ğŸš€ å¼€å§‹è®­ç»ƒ")
                    train_btn_static = gr.Button("â–¶ï¸ å¼€å§‹è®­ç»ƒ StaticTransformer æ¨¡å‹", variant="primary", size="lg")
                    training_log_static = gr.Textbox(label="è®­ç»ƒæ—¥å¿—", lines=30, max_lines=50, autoscroll=True)

        # ========== HybridSensorTransformerè®­ç»ƒTab ==========
        with gr.Tab("ğŸ¯ HybridSensorTransformerè®­ç»ƒ"):
            gr.Markdown("## HybridSensorTransformer: æ··åˆæ—¶åº+é™æ€ Transformer")

            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“‹ é…ç½®ç®¡ç†")
                    config_file_hybrid = gr.File(label="å¯¼å…¥é…ç½®æ–‡ä»¶", file_types=[".json"])
                    import_config_btn_hybrid = gr.Button("ğŸ“‚ å¯¼å…¥é…ç½®", variant="secondary")
                    export_config_btn_hybrid = gr.Button("ğŸ’¾ å¯¼å‡ºé…ç½®", variant="secondary")
                    config_status_hybrid = gr.Textbox(label="é…ç½®çŠ¶æ€", lines=3)
                    config_download_hybrid = gr.File(label="ä¸‹è½½é…ç½®æ–‡ä»¶", visible=False)

                    gr.Markdown("### ğŸ›ï¸ ä¿¡å·é€‰æ‹©")
                    gr.Markdown("âš ï¸ **æ³¨æ„**: è¾¹ç•Œä¿¡å·å’Œç›®æ ‡ä¿¡å·ä¸èƒ½é‡å¤ï¼›æ—¶åºä¿¡å·å¿…é¡»æ˜¯ç›®æ ‡ä¿¡å·çš„å­é›†ï¼Œè®­ç»ƒå‰ä¼šè‡ªåŠ¨éªŒè¯")

                    boundary_signals_hybrid = gr.Dropdown(
                        choices=[],
                        multiselect=True,
                        label="è¾¹ç•Œæ¡ä»¶ä¿¡å· (è¾“å…¥)",
                        info="é€‰æ‹©ç”¨äºé¢„æµ‹çš„è¾“å…¥ä¿¡å·"
                    )

                    target_signals_hybrid = gr.Dropdown(
                        choices=[],
                        multiselect=True,
                        label="ç›®æ ‡ä¿¡å· (è¾“å‡º)",
                        info="é€‰æ‹©è¦é¢„æµ‹çš„ä¿¡å·"
                    )

                    gr.Markdown("### â±ï¸ æ—¶åºé€‰é¡¹")
                    gr.Markdown("ğŸ’¡ æ—¶åºä¿¡å·å¿…é¡»ä»ç›®æ ‡ä¿¡å·ä¸­é€‰æ‹©")
                    with gr.Row():
                        temporal_signals_hybrid = gr.Dropdown(
                            choices=[],
                            multiselect=True,
                            label="æ—¶åºæ¨¡å¼ä¿¡å·",
                            info="ä»ç›®æ ‡ä¿¡å·ä¸­é€‰æ‹©éœ€è¦æ—¶åºä¸Šä¸‹æ–‡çš„ä¿¡å· (ç•™ç©ºåˆ™å…¨éƒ¨ä½¿ç”¨é™æ€æ¨¡å¼)"
                        )
                        sync_temporal_btn_hybrid = gr.Button("ğŸ”„ åŒæ­¥ç›®æ ‡ä¿¡å·", size="sm", scale=0)
                        context_window_hybrid = gr.Slider(1, 10, value=5, step=1, label="ä¸Šä¸‹æ–‡çª—å£å¤§å°")
                        apply_smoothing_hybrid = gr.Checkbox(label="åº”ç”¨IFDå¹³æ»‘æ»¤æ³¢", value=True)

                    gr.Markdown("### âš™ï¸ æ•°æ®åˆ†å‰²")
                    with gr.Row():
                        test_size_hybrid = gr.Slider(0.1, 0.4, value=0.2, step=0.05, label="æµ‹è¯•é›†æ¯”ä¾‹")
                        val_size_hybrid = gr.Slider(0.1, 0.3, value=0.2, step=0.05, label="éªŒè¯é›†æ¯”ä¾‹")

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ”§ è®­ç»ƒå‚æ•°")
                    with gr.Row():
                        epochs_hybrid = gr.Slider(1, 200, value=100, step=1, label="è®­ç»ƒè½®æ•°")
                        batch_size_hybrid = gr.Slider(16, 256, value=64, step=16, label="æ‰¹å¤§å°")

                    with gr.Row():
                        lr_hybrid = gr.Number(value=0.001, label="å­¦ä¹ ç‡")
                        weight_decay_hybrid = gr.Number(value=1e-5, label="æƒé‡è¡°å‡", precision=6)

                    gr.Markdown("### ğŸ—ï¸ æ¨¡å‹æ¶æ„")
                    with gr.Row():
                        d_model_hybrid = gr.Slider(32, 256, value=64, step=32, label="æ¨¡å‹ç»´åº¦")
                        nhead_hybrid = gr.Slider(2, 16, value=4, step=2, label="æ³¨æ„åŠ›å¤´æ•°")

                    with gr.Row():
                        num_layers_hybrid = gr.Slider(1, 6, value=2, step=1, label="Transformerå±‚æ•°")
                        dropout_hybrid = gr.Slider(0.0, 0.5, value=0.1, step=0.05, label="Dropout")

                    with gr.Row():
                        gain_hybrid = gr.Slider(0.01, 1.0, value=0.1, step=0.01, label="æƒé‡åˆå§‹åŒ–Gain")

                    gr.Markdown("### ğŸ“ˆ ä¼˜åŒ–å™¨å‚æ•°")
                    with gr.Row():
                        scheduler_patience_hybrid = gr.Slider(5, 20, value=10, step=1, label="å­¦ä¹ ç‡è°ƒåº¦è€å¿ƒ")
                        scheduler_factor_hybrid = gr.Slider(0.1, 0.9, value=0.5, step=0.1, label="å­¦ä¹ ç‡è¡°å‡å› å­")

                    with gr.Row():
                        grad_clip_hybrid = gr.Slider(0.5, 5.0, value=1.0, step=0.1, label="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
                        early_stop_patience_hybrid = gr.Slider(10, 50, value=25, step=5, label="æ—©åœè€å¿ƒ")

                    gr.Markdown("### ğŸš€ å¼€å§‹è®­ç»ƒ")
                    train_btn_hybrid = gr.Button("â–¶ï¸ å¼€å§‹è®­ç»ƒ HybridSensorTransformer æ¨¡å‹", variant="primary", size="lg")
                    training_log_hybrid = gr.Textbox(label="è®­ç»ƒæ—¥å¿—", lines=30, max_lines=50, autoscroll=True)

        # ========== æ¨ç†Tab ==========
        with gr.Tab("ğŸ”® æ¨¡å‹æ¨ç†"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### 1ï¸âƒ£ é€‰æ‹©æ¨¡å‹")
                    model_selector = gr.Dropdown(
                        choices=[],
                        label="å·²è®­ç»ƒæ¨¡å‹",
                        info="é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"
                    )
                    refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨")
                    load_model_btn = gr.Button("ğŸ“‚ åŠ è½½æ¨¡å‹", variant="primary")
                    model_info = gr.Textbox(label="æ¨¡å‹ä¿¡æ¯", lines=15)

                    gr.Markdown("### 2ï¸âƒ£ æ¨ç†è®¾ç½®")
                    with gr.Row():
                        start_idx = gr.Number(value=0, label="èµ·å§‹ç´¢å¼•", precision=0)
                        end_idx = gr.Number(value=1000, label="ç»“æŸç´¢å¼•", precision=0)

                    inference_signals = gr.Dropdown(
                        choices=[],
                        multiselect=True,
                        label="é€‰æ‹©å¯è§†åŒ–ä¿¡å·",
                        info="é€‰æ‹©è¦åˆ†æçš„ç›®æ ‡ä¿¡å·ï¼ˆæœ€å¤š5ä¸ªï¼‰"
                    )

                    inference_btn = gr.Button("â–¶ï¸ è¿è¡Œæ¨ç†", variant="primary", size="lg")
                    inference_status = gr.Textbox(label="æ¨ç†çŠ¶æ€", lines=3)

                with gr.Column(scale=2):
                    gr.Markdown("### 3ï¸âƒ£ æ¨ç†ç»“æœ")
                    inference_plot = gr.Plot(label="å¯è§†åŒ–ç»“æœ")
                    with gr.Row():
                        metrics_output = gr.Textbox(label="è¯„ä¼°æŒ‡æ ‡", lines=20)

            gr.Markdown("""
---
### ğŸ“– ä½¿ç”¨è¯´æ˜

**æ•°æ®åŠ è½½:**
1. åœ¨"æ•°æ®åŠ è½½"Tabä¸Šä¼ CSVæ–‡ä»¶æˆ–ä½¿ç”¨å…¨å±€dfå˜é‡
2. ç‚¹å‡»"åŠ è½½æ•°æ®"æŒ‰é’®
3. åœ¨å³ä¾§å¯ä»¥çœ‹åˆ°æ‰€æœ‰å¯ç”¨ä¿¡å·çš„å®Œæ•´åˆ—è¡¨

**ä¿¡å·é€‰æ‹©è§„åˆ™:**
- **StaticTransformeræ¨¡å‹**: è¾¹ç•Œæ¡ä»¶ä¿¡å·å’Œç›®æ ‡ä¿¡å·ä¸èƒ½é‡å¤
- **HybridSensorTransformeræ¨¡å‹**: è¾¹ç•Œæ¡ä»¶ä¿¡å·å’Œç›®æ ‡ä¿¡å·ä¸èƒ½é‡å¤ï¼›æ—¶åºä¿¡å·å¿…é¡»ä»ç›®æ ‡ä¿¡å·ä¸­é€‰æ‹©
- **éªŒè¯æ—¶æœº**: ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"æ—¶è‡ªåŠ¨éªŒè¯ï¼Œå¦‚æœ‰å†²çªä¼šæ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯

**StaticTransformeræ¨¡å‹è®­ç»ƒæµç¨‹:**
1. (å¯é€‰) å¯¼å…¥ä¹‹å‰ä¿å­˜çš„é…ç½®æ–‡ä»¶
2. é€‰æ‹©è¾¹ç•Œæ¡ä»¶å’Œç›®æ ‡ä¿¡å·
3. è°ƒæ•´è®­ç»ƒå‚æ•°å’Œæ¨¡å‹æ¶æ„
4. ç‚¹å‡»"å¼€å§‹è®­ç»ƒ StaticTransformer æ¨¡å‹" - ç³»ç»Ÿä¼šè‡ªåŠ¨éªŒè¯ä¿¡å·é€‰æ‹©
5. **å®æ—¶æŸ¥çœ‹è®­ç»ƒè¿›åº¦** - æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±ä¼šå®æ—¶æ˜¾ç¤ºï¼ˆâ­è¡¨ç¤ºæœ€ä½³æ¨¡å‹ï¼‰
6. (å¯é€‰) å¯¼å‡ºå½“å‰é…ç½®ä»¥ä¾¿åç»­ä½¿ç”¨

**HybridSensorTransformeræ¨¡å‹è®­ç»ƒæµç¨‹:**
1. (å¯é€‰) å¯¼å…¥ä¹‹å‰ä¿å­˜çš„é…ç½®æ–‡ä»¶
2. é€‰æ‹©è¾¹ç•Œæ¡ä»¶å’Œç›®æ ‡ä¿¡å·
3. ä»ç›®æ ‡ä¿¡å·ä¸­é€‰æ‹©éœ€è¦æ—¶åºæ¨¡å¼çš„ä¿¡å·ï¼ˆå¯ä½¿ç”¨"åŒæ­¥ç›®æ ‡ä¿¡å·"æŒ‰é’®å¿«é€Ÿæ›´æ–°é€‰é¡¹ï¼‰
4. è°ƒæ•´ä¸Šä¸‹æ–‡çª—å£å¤§å°å’Œå…¶ä»–å‚æ•°
5. è°ƒæ•´è®­ç»ƒå‚æ•°å’Œæ¨¡å‹æ¶æ„
6. ç‚¹å‡»"å¼€å§‹è®­ç»ƒ HybridSensorTransformer æ¨¡å‹" - ç³»ç»Ÿä¼šè‡ªåŠ¨éªŒè¯ä¿¡å·é€‰æ‹©
7. **å®æ—¶æŸ¥çœ‹è®­ç»ƒè¿›åº¦** - æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±ä¼šå®æ—¶æ˜¾ç¤ºï¼ˆâ­è¡¨ç¤ºæœ€ä½³æ¨¡å‹ï¼‰
8. (å¯é€‰) å¯¼å‡ºå½“å‰é…ç½®ä»¥ä¾¿åç»­ä½¿ç”¨

**è®­ç»ƒæ—¥å¿—è¯´æ˜:**
- â­ æ ‡è®°è¡¨ç¤ºè¯¥epochè·å¾—äº†æœ€ä½³éªŒè¯æŸå¤±ï¼ˆæ¨¡å‹ä¼šè¢«ä¿å­˜ï¼‰
- Train: è®­ç»ƒæŸå¤±
- Val: éªŒè¯æŸå¤±
- Best: å½“å‰æœ€ä½³éªŒè¯æŸå¤±
- LR: å½“å‰å­¦ä¹ ç‡
- Patience: æ—©åœè®¡æ•°å™¨ï¼ˆè¾¾åˆ°è®¾å®šå€¼ä¼šè‡ªåŠ¨åœæ­¢è®­ç»ƒï¼‰

**æ¨ç†æµç¨‹:**
1. ç‚¹å‡»"åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
2. é€‰æ‹©å·²è®­ç»ƒæ¨¡å‹å¹¶ç‚¹å‡»"åŠ è½½æ¨¡å‹"
3. è®¾ç½®æ¨ç†çš„æ•°æ®èŒƒå›´ (èµ·å§‹å’Œç»“æŸç´¢å¼•)
4. é€‰æ‹©è¦å¯è§†åŒ–å’Œåˆ†æçš„ä¿¡å· (å»ºè®®1-3ä¸ª)
5. ç‚¹å‡»"è¿è¡Œæ¨ç†"æŸ¥çœ‹ç»“æœ

**å¯è§†åŒ–è¯´æ˜:**
- å·¦å›¾: é¢„æµ‹å€¼ä¸å®é™…å€¼çš„æ—¶åºå¯¹æ¯”
- ä¸­å›¾: æ®‹å·®åˆ†æ (å®é™…å€¼ - é¢„æµ‹å€¼)
- å³å›¾: é¢„æµ‹ç²¾åº¦æ•£ç‚¹å›¾ (è¶Šæ¥è¿‘å¯¹è§’çº¿è¶Šå¥½)

**æç¤º:**
- HybridSensorTransformeræ¨¡å‹çš„æ—¶åºæ¨¡å¼éœ€è¦è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ•°æ®
- é…ç½®æ–‡ä»¶å¯ä»¥ä¿å­˜ä½ çš„ä¿¡å·é€‰æ‹©å’Œæ‰€æœ‰è®­ç»ƒå‚æ•°
- å»ºè®®å…ˆç”¨å°epochæ•°ï¼ˆå¦‚1-10ï¼‰æµ‹è¯•ï¼Œç¡®è®¤æ— è¯¯åå†è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒ
- å¦‚æœä¿¡å·é€‰æ‹©æœ‰å†²çªï¼Œè®­ç»ƒæ—¶ä¼šæ˜¾ç¤ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæ ¹æ®æç¤ºä¿®æ”¹å³å¯
- è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥å®æ—¶çœ‹åˆ°æ¯ä¸ªepochçš„è¿›åº¦ï¼Œæ— éœ€ç­‰å¾…è®­ç»ƒç»“æŸ
            """)

    # ========== äº‹ä»¶ç»‘å®š ==========

    # æ•°æ®åŠ è½½ - ç›´æ¥æ›´æ–°æ‰€æœ‰ä¿¡å·é€‰æ‹©æ¡†
    load_data_btn.click(
        fn=on_load_data,
        inputs=[data_file],
        outputs=[
            data_status,
            signals_list_display,
            boundary_signals_static,
            target_signals_static,
            boundary_signals_hybrid,
            target_signals_hybrid
        ]
    )

    # StaticTransformeré…ç½®å¯¼å…¥å¯¼å‡º
    import_config_btn_static.click(
        fn=import_config,
        inputs=[config_file_static],
        outputs=[
            boundary_signals_static, target_signals_static, temporal_signals_hybrid,
            test_size_static, val_size_static, epochs_static, batch_size_static, lr_static,
            d_model_static, nhead_static, num_layers_static, dropout_static,
            context_window_hybrid, apply_smoothing_hybrid, gain_hybrid,
            weight_decay_static, scheduler_patience_static, scheduler_factor_static,
            grad_clip_static, early_stop_patience_static, config_status_static
        ]
    )

    export_config_btn_static.click(
        fn=export_config_static_transformer,
        inputs=[
            boundary_signals_static, target_signals_static,
            test_size_static, val_size_static, epochs_static, batch_size_static, lr_static,
            d_model_static, nhead_static, num_layers_static, dropout_static,
            weight_decay_static, scheduler_patience_static, scheduler_factor_static,
            grad_clip_static, early_stop_patience_static
        ],
        outputs=[config_download_static, config_status_static]
    ).then(
        fn=lambda x: gr.update(visible=True),
        inputs=[config_download_static],
        outputs=[config_download_static]
    )

    # HybridSensorTransformeré…ç½®å¯¼å…¥å¯¼å‡º
    sync_temporal_btn_hybrid.click(
        fn=lambda target_sigs: gr.update(choices=target_sigs if target_sigs else []),
        inputs=[target_signals_hybrid],
        outputs=[temporal_signals_hybrid]
    )

    import_config_btn_hybrid.click(
        fn=import_config,
        inputs=[config_file_hybrid],
        outputs=[
            boundary_signals_hybrid, target_signals_hybrid, temporal_signals_hybrid,
            test_size_hybrid, val_size_hybrid, epochs_hybrid, batch_size_hybrid, lr_hybrid,
            d_model_hybrid, nhead_hybrid, num_layers_hybrid, dropout_hybrid,
            context_window_hybrid, apply_smoothing_hybrid, gain_hybrid,
            weight_decay_hybrid, scheduler_patience_hybrid, scheduler_factor_hybrid,
            grad_clip_hybrid, early_stop_patience_hybrid, config_status_hybrid
        ]
    )

    export_config_btn_hybrid.click(
        fn=export_config_hybrid_sensor_transformer,
        inputs=[
            boundary_signals_hybrid, target_signals_hybrid, temporal_signals_hybrid,
            test_size_hybrid, val_size_hybrid, epochs_hybrid, batch_size_hybrid, lr_hybrid,
            d_model_hybrid, nhead_hybrid, num_layers_hybrid, dropout_hybrid,
            context_window_hybrid, apply_smoothing_hybrid, gain_hybrid,
            weight_decay_hybrid, scheduler_patience_hybrid, scheduler_factor_hybrid,
            grad_clip_hybrid, early_stop_patience_hybrid
        ],
        outputs=[config_download_hybrid, config_status_hybrid]
    ).then(
        fn=lambda x: gr.update(visible=True),
        inputs=[config_download_hybrid],
        outputs=[config_download_hybrid]
    )

    # StaticTransformerè®­ç»ƒ
    train_btn_static.click(
        fn=start_training_static_transformer,
        inputs=[
            boundary_signals_static, target_signals_static,
            test_size_static, val_size_static, epochs_static, batch_size_static, lr_static,
            d_model_static, nhead_static, num_layers_static, dropout_static,
            weight_decay_static, scheduler_patience_static, scheduler_factor_static,
            grad_clip_static, early_stop_patience_static
        ],
        outputs=[training_log_static]
    )

    # HybridSensorTransformerè®­ç»ƒ
    train_btn_hybrid.click(
        fn=start_training_hybrid_sensor_transformer,
        inputs=[
            boundary_signals_hybrid, target_signals_hybrid, temporal_signals_hybrid,
            test_size_hybrid, val_size_hybrid, epochs_hybrid, batch_size_hybrid, lr_hybrid,
            d_model_hybrid, nhead_hybrid, num_layers_hybrid, dropout_hybrid,
            context_window_hybrid, apply_smoothing_hybrid, gain_hybrid,
            weight_decay_hybrid, scheduler_patience_hybrid, scheduler_factor_hybrid,
            grad_clip_hybrid, early_stop_patience_hybrid
        ],
        outputs=[training_log_hybrid]
    )

    # æ¨ç†
    refresh_models_btn.click(
        fn=lambda: gr.update(choices=get_model_list()),
        outputs=[model_selector]
    )

    load_model_btn.click(
        fn=on_model_load,
        inputs=[model_selector],
        outputs=[model_info, inference_signals]
    )

    inference_btn.click(
        fn=run_inference,
        inputs=[model_selector, start_idx, end_idx, inference_signals],
        outputs=[inference_status, inference_plot, metrics_output]
    )

print("="*80)
print("å®Œæ•´Gradioç•Œé¢å·²å‡†å¤‡å°±ç»ªï¼")
print("="*80)
print("\nğŸ¯ æ¨¡å‹åç§°å·²ç»Ÿä¸€æ›´æ–°:")
print("  â€¢ V1 â†’ StaticTransformer")
print("  â€¢ V4 â†’ HybridSensorTransformer")
print("\nğŸ“ åŒ…å«å®Œæ•´çš„é…ç½®å¯¼å…¥å¯¼å‡ºå’Œå›è°ƒå‡½æ•°")
print("\nğŸš€ æ‰§è¡Œ demo.launch() å¯åŠ¨ç•Œé¢")
print("\nğŸ’¡ å¦‚æœåœ¨æœ¬åœ°ï¼Œè®¿é—® http://127.0.0.1:7860")
print("="*80)

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    demo.launch(share=True, debug=True)