"""
å®Œæ•´çš„è‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒæµç¨‹
====================================

å®Œæ•´æµç¨‹ï¼š
1. ä»JSONå¯¼å…¥é€‰è‚¡åˆ—è¡¨
2. å†å²æ•°æ®è·å–ï¼ˆyfinance + akshareï¼‰
3. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
4. Stage 1: SSTæ¨¡å‹è®­ç»ƒï¼ˆåŒè¾“å‡ºï¼šTæ—¥ + T+1æ—¥ï¼‰
5. Stage 2: ç‰¹å¾æå–ï¼ˆAttention + Encoder + æ®‹å·®ï¼‰
6. Stage 3: æ—¶åºæ¨¡å‹è®­ç»ƒï¼ˆLSTM + GRU + TCNï¼‰
7. æ¨¡å‹æ•ˆæœæµ‹è¯•å’Œå¯¹æ¯”

ä½¿ç”¨æ–¹æ³•ï¼š
    python complete_training_pipeline.py --stocks_json data/demo.json --target_market CN --target_stock 600519

ä½œè€…ï¼šQuant-Stock-Transformer Team
ç‰ˆæœ¬ï¼š1.0.0
"""

import json
import pickle
import argparse
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# æ•°æ®è·å–åº“
try:
    import yfinance as yf
except ImportError:
    print("è­¦å‘Š: yfinanceæœªå®‰è£…ï¼Œæ— æ³•è·å–ç¾è‚¡/æ¸¯è‚¡/æ—¥è‚¡æ•°æ®")

try:
    import akshare as ak
except ImportError:
    print("è­¦å‘Š: akshareæœªå®‰è£…ï¼Œæ— æ³•è·å–Aè‚¡æ•°æ®")

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from models.spatial_feature_extractor import SpatialFeatureExtractor
from models.temporal_predictor import (
    LSTMTemporalPredictor,
    GRUTemporalPredictor,
    TCNTemporalPredictor,
    TemporalDataset
)
from models.relationship_extractors import HybridExtractor

warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)


# ============================================================================
# ç¬¬ä¸€æ­¥ï¼šæ•°æ®è·å–æ¨¡å—
# ============================================================================

class StockDataFetcher:
    """å¤šå¸‚åœºè‚¡ç¥¨æ•°æ®æŠ“å–å™¨"""

    def __init__(self):
        self.data_cache = {}

    def fetch_historical_data(
        self,
        stocks_json: Dict,
        start_date: str,
        end_date: str,
        interval: str = "1d",
        include_market_index: bool = True,
        batch_size: int = 5,
        delay_between_batches: float = 2.0,
        delay_between_stocks: float = 0.5
    ) -> Dict:
        """
        æŠ“å–å†å²æ•°æ®ï¼ˆæ™ºèƒ½åˆ†æ‰¹ï¼Œé¿å…APIé™æµï¼‰

        Args:
            stocks_json: è‚¡ç¥¨JSONå­—å…¸
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            interval: æ•°æ®ç²’åº¦ï¼ˆ"1d"æŒ‰å¤©ï¼Œ"1h"æŒ‰å°æ—¶ï¼‰
            include_market_index: æ˜¯å¦åŒ…å«å¤§ç›˜æŒ‡æ•°
            batch_size: æ¯æ‰¹æŠ“å–çš„è‚¡ç¥¨æ•°é‡
            delay_between_batches: æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•°
            delay_between_stocks: åŒæ‰¹è‚¡ç¥¨é—´å»¶è¿Ÿç§’æ•°
        """

        print(f"\n{'='*80}")
        print(f"ğŸ“¥ å¼€å§‹æŠ“å–å†å²æ•°æ®")
        print(f"æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"æ•°æ®ç²’åº¦: {interval}")
        print(f"â±ï¸  åˆ†æ‰¹é…ç½®: æ¯æ‰¹{batch_size}æ”¯ï¼Œæ‰¹æ¬¡é—´å»¶è¿Ÿ{delay_between_batches}ç§’")
        print(f"{'='*80}\n")

        all_data = {}

        for market, stocks in stocks_json.items():
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç†{market}å¸‚åœº ({len(stocks)}åªè‚¡ç¥¨)...")

            market_data = {}

            # æŠ“å–å¤§ç›˜æŒ‡æ•°
            if include_market_index:
                index_data = self._fetch_market_index(
                    market, start_date, end_date, interval
                )
                if index_data is not None:
                    market_data['_INDEX_'] = index_data
                    print(f"  âœ“ å¤§ç›˜æŒ‡æ•°æ•°æ®è·å–æˆåŠŸ ({len(index_data)}æ¡è®°å½•)")
                time.sleep(delay_between_stocks)

            # åˆ†æ‰¹æŠ“å–ä¸ªè‚¡æ•°æ®
            total_stocks = len(stocks)
            num_batches = (total_stocks + batch_size - 1) // batch_size

            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_stocks)
                batch_stocks = stocks[start_idx:end_idx]

                print(f"\n  æ‰¹æ¬¡ [{batch_idx+1}/{num_batches}]: æŠ“å–ç¬¬{start_idx+1}-{end_idx}æ”¯è‚¡ç¥¨")

                for i, stock in enumerate(batch_stocks, start=start_idx+1):
                    symbol = stock['symbol']
                    try:
                        data = self._fetch_stock_data(
                            market, symbol, start_date, end_date, interval
                        )
                        if data is not None and len(data) > 0:
                            market_data[symbol] = data
                            print(f"    âœ“ [{i}/{total_stocks}] {symbol}: {len(data)}æ¡æ•°æ®")
                        else:
                            print(f"    âœ— [{i}/{total_stocks}] {symbol}: æ— æ•°æ®")
                    except Exception as e:
                        print(f"    âœ— [{i}/{total_stocks}] {symbol}: å¤±è´¥ ({str(e)[:50]})")

                    if i < total_stocks:
                        time.sleep(delay_between_stocks)

                if batch_idx < num_batches - 1:
                    print(f"  â¸ï¸  æ‰¹æ¬¡å®Œæˆï¼Œç­‰å¾…{delay_between_batches}ç§’åç»§ç»­...")
                    time.sleep(delay_between_batches)

            all_data[market] = market_data
            print(f"\n  âœ“ {market}å¸‚åœºå®Œæˆï¼šæˆåŠŸ{len(market_data)}æ”¯ï¼ˆå«æŒ‡æ•°ï¼‰")

        self.data_cache = all_data

        print(f"\n{'='*80}")
        print("âœ“ æ‰€æœ‰æ•°æ®æŠ“å–å®Œæˆï¼")
        total_success = sum(len(v) for v in all_data.values())
        total_requested = sum(len(v) for v in stocks_json.values()) + len(stocks_json)
        print(f"æˆåŠŸç‡: {total_success}/{total_requested} ({100*total_success/total_requested:.1f}%)")
        print(f"{'='*80}\n")

        return all_data

    def _fetch_market_index(
        self, market: str, start_date: str, end_date: str, interval: str
    ) -> Optional[pd.DataFrame]:
        """æŠ“å–å¤§ç›˜æŒ‡æ•°"""

        index_symbols = {
            "US": "^GSPC",      # S&P 500
            "CN": "000001",     # ä¸Šè¯æŒ‡æ•°
            "HK": "^HSI",       # æ’ç”ŸæŒ‡æ•°
            "JP": "^N225"       # æ—¥ç»225
        }

        symbol = index_symbols.get(market)
        if not symbol:
            return None

        try:
            if market == "CN":
                # æ–¹æ³•1: å°è¯•ä½¿ç”¨akshare
                try:
                    df = ak.stock_zh_index_daily(symbol=f"sh{symbol}")
                    df['date'] = pd.to_datetime(df['date'])
                    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]
                    df = df.rename(columns={'close': 'Close', 'open': 'Open',
                                           'high': 'High', 'low': 'Low', 'volume': 'Volume'})
                    df = df.set_index('date')
                    return df
                except Exception as ak_error:
                    # æ–¹æ³•2: akshareå¤±è´¥æ—¶ï¼Œä½¿ç”¨yfinanceå¤‡é€‰æ–¹æ¡ˆ
                    print(f"  âš ï¸  akshareå¤±è´¥ï¼Œåˆ‡æ¢åˆ°yfinanceè·å–æŒ‡æ•°...")
                    # ä½¿ç”¨ä¸Šè¯æŒ‡æ•°çš„yfinanceç¬¦å·
                    yahoo_index_symbol = "000001.SS"
                    df = yf.download(yahoo_index_symbol, start=start_date, end=end_date,
                                    interval=interval, progress=False)
                    if len(df) == 0:
                        # å¦‚æœä¸Šè¯æŒ‡æ•°å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨^SSEC
                        df = yf.download("^SSEC", start=start_date, end=end_date,
                                        interval=interval, progress=False)
                    return df
            else:
                df = yf.download(symbol, start=start_date, end=end_date,
                                interval=interval, progress=False)
                return df

        except Exception as e:
            print(f"    è­¦å‘Š: å¤§ç›˜æŒ‡æ•°è·å–å¤±è´¥ ({e})")
            return None

    def _fetch_stock_data(
        self, market: str, symbol: str, start_date: str, end_date: str, interval: str
    ) -> Optional[pd.DataFrame]:
        """æŠ“å–ä¸ªè‚¡æ•°æ®"""

        try:
            if market == "CN":
                # æ–¹æ³•1: å°è¯•ä½¿ç”¨akshareï¼ˆå›½å†…æ•°æ®æ›´å‡†ç¡®ï¼‰
                try:
                    df = ak.stock_zh_a_hist(symbol=symbol, period="daily",
                                           start_date=start_date.replace('-', ''),
                                           end_date=end_date.replace('-', ''))
                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                    df = df.rename(columns={
                        'æ—¥æœŸ': 'Date', 'æ”¶ç›˜': 'Close', 'å¼€ç›˜': 'Open',
                        'æœ€é«˜': 'High', 'æœ€ä½': 'Low', 'æˆäº¤é‡': 'Volume'
                    })
                    df = df.set_index('Date')
                    return df
                except Exception as ak_error:
                    # æ–¹æ³•2: akshareå¤±è´¥æ—¶ï¼Œä½¿ç”¨yfinanceå¤‡é€‰æ–¹æ¡ˆï¼ˆæ·»åŠ äº¤æ˜“æ‰€åç¼€ï¼‰
                    print(f"      âš ï¸  akshareå¤±è´¥ï¼Œåˆ‡æ¢åˆ°yfinance...")

                    # æ·»åŠ äº¤æ˜“æ‰€åç¼€
                    if symbol.startswith('6'):
                        yahoo_symbol = f"{symbol}.SS"  # ä¸Šæµ·äº¤æ˜“æ‰€
                    elif symbol.startswith('0') or symbol.startswith('3'):
                        yahoo_symbol = f"{symbol}.SZ"  # æ·±åœ³äº¤æ˜“æ‰€
                    else:
                        yahoo_symbol = symbol

                    df = yf.download(yahoo_symbol, start=start_date, end=end_date,
                                    interval=interval, progress=False)

                    if len(df) > 0:
                        # yfinanceè¿”å›çš„åˆ—åå·²ç»æ˜¯è‹±æ–‡ï¼Œå¯èƒ½éœ€è¦é‡ç½®ç´¢å¼•
                        if df.index.name != 'Date':
                            df.index.name = 'Date'
                        return df
                    else:
                        raise Exception(f"yfinanceä¹Ÿæœªè¿”å›æ•°æ®")
            else:
                if market == "HK" and not symbol.endswith(".HK"):
                    symbol = symbol.zfill(4) + ".HK"
                elif market == "JP" and not symbol.endswith(".T"):
                    if '.' not in symbol:
                        symbol = symbol + ".T"

                df = yf.download(symbol, start=start_date, end=end_date,
                                interval=interval, progress=False)

            return df
        except Exception as e:
            raise Exception(f"æ•°æ®è·å–å¤±è´¥: {e}")

    def save_data(self, output_path: str = "historical_data.pkl"):
        """ä¿å­˜æ•°æ®åˆ°pickleæ–‡ä»¶"""
        with open(output_path, 'wb') as f:
            pickle.dump(self.data_cache, f)
        print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")

    @staticmethod
    def load_data(input_path: str) -> Dict:
        """ä»pickleæ–‡ä»¶åŠ è½½æ•°æ®"""
        with open(input_path, 'rb') as f:
            data = pickle.load(f)
        print(f"âœ“ æ•°æ®å·²ä» {input_path} åŠ è½½")
        return data


# ============================================================================
# ç¬¬äºŒæ­¥ï¼šæ•°æ®é¢„å¤„ç†æ¨¡å—
# ============================================================================

class StockDataProcessor:
    """è‚¡ç¥¨æ•°æ®é¢„å¤„ç†å™¨"""

    def __init__(self, historical_data: Dict, target_market: str, target_stock: str):
        self.historical_data = historical_data
        self.target_market = target_market
        self.target_stock = target_stock
        self.scaler = StandardScaler()

    def prepare_training_data(
        self,
        use_all_stocks: bool = True
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®

        Returns:
            boundary_conditions: [N, num_boundary_sensors] - è¾¹ç•Œæ¡ä»¶
            targets_T: [N, 1] - Tæ—¥ç›®æ ‡ï¼ˆå½“æ—¥æ”¶ç›Šç‡ï¼‰
            targets_T1: [N, 1] - T+1æ—¥ç›®æ ‡ï¼ˆæ¬¡æ—¥æ”¶ç›Šç‡ï¼‰
            dates: [N] - æ—¥æœŸç´¢å¼•
        """
        print(f"\n{'='*80}")
        print("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        print(f"{'='*80}\n")

        # è·å–å¸‚åœºæ•°æ®
        market_data = self.historical_data[self.target_market]

        # è·å–å¤§ç›˜æŒ‡æ•°
        index_df = market_data.get('_INDEX_')

        # è·å–ç›®æ ‡è‚¡ç¥¨æ•°æ®
        target_df = market_data[self.target_stock].copy()

        # è®¡ç®—æ”¶ç›Šç‡
        target_df['return'] = target_df['Close'].pct_change()
        target_df['return_next'] = target_df['return'].shift(-1)

        # ç§»é™¤NaN
        target_df = target_df.dropna()

        print(f"  âœ“ ç›®æ ‡è‚¡ç¥¨æ•°æ®: {len(target_df)}æ¡")

        # æ„å»ºè¾¹ç•Œæ¡ä»¶
        if use_all_stocks:
            # ä½¿ç”¨æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®ä½œä¸ºè¾¹ç•Œæ¡ä»¶ï¼ˆç®€åŒ–ç‰ˆå®ç°ï¼‰
            print("  ğŸ“Š ä½¿ç”¨ç›®æ ‡è‚¡ç¥¨çš„OHLCVä½œä¸ºè¾¹ç•Œæ¡ä»¶...")
            boundary_features = []

            for i in range(len(target_df) - 1):
                features = [
                    target_df['Open'].iloc[i],
                    target_df['High'].iloc[i],
                    target_df['Low'].iloc[i],
                    target_df['Close'].iloc[i],
                    target_df['Volume'].iloc[i]
                ]

                # æ·»åŠ å¤§ç›˜æŒ‡æ•°
                if index_df is not None and i < len(index_df):
                    features.append(index_df['Close'].iloc[i])

                boundary_features.append(features)
        else:
            boundary_features = [[
                target_df['Open'].iloc[i],
                target_df['High'].iloc[i],
                target_df['Low'].iloc[i],
                target_df['Close'].iloc[i],
                target_df['Volume'].iloc[i]
            ] for i in range(len(target_df) - 1)]

        # æå–ç›®æ ‡
        targets_T = target_df['return'].values[:-1].reshape(-1, 1)
        targets_T1 = target_df['return_next'].values[:-1].reshape(-1, 1)
        dates = target_df.index[:-1].tolist()

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        boundary_features = np.array(boundary_features, dtype=np.float32)
        targets_T = np.array(targets_T, dtype=np.float32)
        targets_T1 = np.array(targets_T1, dtype=np.float32)

        # æ ‡å‡†åŒ–è¾¹ç•Œæ¡ä»¶
        boundary_features = self.scaler.fit_transform(boundary_features)

        print(f"  âœ“ è¾¹ç•Œæ¡ä»¶å½¢çŠ¶: {boundary_features.shape}")
        print(f"  âœ“ Tæ—¥ç›®æ ‡å½¢çŠ¶: {targets_T.shape}")
        print(f"  âœ“ T+1æ—¥ç›®æ ‡å½¢çŠ¶: {targets_T1.shape}")
        print(f"\n{'='*80}")
        print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"{'='*80}\n")

        return boundary_features, targets_T, targets_T1, dates


# ============================================================================
# ç¬¬ä¸‰æ­¥ï¼šåŒè¾“å‡ºSSTæ¨¡å‹
# ============================================================================

class DualOutputSST(SpatialFeatureExtractor):
    """åŒè¾“å‡ºSST - åŒæ—¶é¢„æµ‹Tæ—¥å’ŒT+1æ—¥æ”¶ç›Šç‡"""

    def __init__(self, num_boundary_sensors, num_target_sensors, **kwargs):
        super().__init__(num_boundary_sensors, num_target_sensors, **kwargs)

        # åŒè¾“å‡ºå¤´
        self.output_projection_T = nn.Linear(self.d_model, num_target_sensors)
        self.output_projection_T1 = nn.Linear(self.d_model, num_target_sensors)

        nn.init.xavier_uniform_(self.output_projection_T.weight)
        nn.init.xavier_uniform_(self.output_projection_T1.weight)

    def forward(self, boundary_conditions):
        """å‰å‘ä¼ æ’­"""
        batch_size = boundary_conditions.shape[0]

        x = boundary_conditions.unsqueeze(-1)
        x = self.boundary_embedding(x) + self.boundary_position_encoding.unsqueeze(0)
        x = self.transformer(x)
        x_pooled = x.permute(0, 2, 1)
        x_pooled = self.global_pool(x_pooled).squeeze(-1)

        pred_T = self.output_projection_T(x_pooled)
        pred_T1 = self.output_projection_T1(x_pooled)

        return pred_T, pred_T1

    def forward_with_features(self, boundary_conditions, **kwargs):
        """å‰å‘ä¼ æ’­å¹¶è¿”å›å†…éƒ¨ç‰¹å¾"""
        batch_size = boundary_conditions.shape[0]
        features = {}

        x = boundary_conditions.unsqueeze(-1)
        x = self.boundary_embedding(x) + self.boundary_position_encoding.unsqueeze(0)
        features['embeddings'] = x.clone()

        # è·å–attentionæƒé‡
        if kwargs.get('return_attention', True):
            encoder_output = x
            all_attention_weights = []

            for layer in self.transformer.layers:
                encoder_output, attn_weights = self._extract_attention_from_layer(
                    layer, encoder_output
                )
                all_attention_weights.append(attn_weights)

            features['attention_weights'] = torch.stack(all_attention_weights, dim=1)
            features['encoder_output'] = encoder_output
        else:
            encoder_output = self.transformer(x)
            features['encoder_output'] = encoder_output

        x_pooled = encoder_output.permute(0, 2, 1)
        x_pooled = self.global_pool(x_pooled).squeeze(-1)
        features['pooled_features'] = x_pooled

        pred_T = self.output_projection_T(x_pooled)
        pred_T1 = self.output_projection_T1(x_pooled)

        return (pred_T, pred_T1), features

    def _extract_attention_from_layer(self, layer, x):
        """ä»Transformerå±‚æå–attentionæƒé‡"""
        # ç®€åŒ–å®ç°ï¼šç›´æ¥è°ƒç”¨forward
        attn_output = layer.self_attn(x, x, x, need_weights=True)
        if isinstance(attn_output, tuple):
            output, attn_weights = attn_output
        else:
            output = attn_output
            attn_weights = None

        # æ®‹å·®è¿æ¥å’Œlayer norm
        x = x + layer.dropout1(output)
        x = layer.norm1(x)

        # Feed forward
        ff_output = layer.linear2(layer.dropout(layer.activation(layer.linear1(x))))
        x = x + layer.dropout2(ff_output)
        x = layer.norm2(x)

        return x, attn_weights


# ============================================================================
# ç¬¬å››æ­¥ï¼šè®­ç»ƒå™¨
# ============================================================================

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, device='cpu'):
        self.device = device
        self.history = {}

    def train_sst(
        self,
        model,
        X_train, y_T_train, y_T1_train,
        X_val, y_T_val, y_T1_val,
        epochs=50,
        batch_size=32,
        lr=0.001,
        verbose=True
    ):
        """è®­ç»ƒåŒè¾“å‡ºSSTæ¨¡å‹"""

        print(f"\n{'='*80}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒSSTæ¨¡å‹")
        print(f"{'='*80}\n")

        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        criterion = nn.MSELoss()

        # è½¬æ¢ä¸ºtensor
        X_train_t = torch.FloatTensor(X_train).to(self.device)
        y_T_train_t = torch.FloatTensor(y_T_train).to(self.device)
        y_T1_train_t = torch.FloatTensor(y_T1_train).to(self.device)

        X_val_t = torch.FloatTensor(X_val).to(self.device)
        y_T_val_t = torch.FloatTensor(y_T_val).to(self.device)
        y_T1_val_t = torch.FloatTensor(y_T1_val).to(self.device)

        history = {'train_loss': [], 'val_loss': [], 'train_loss_T': [],
                   'train_loss_T1': [], 'val_loss_T': [], 'val_loss_T1': []}

        best_val_loss = float('inf')

        for epoch in range(epochs):
            model.train()

            # è®­ç»ƒ
            epoch_loss = 0
            epoch_loss_T = 0
            epoch_loss_T1 = 0
            num_batches = (len(X_train) + batch_size - 1) // batch_size

            for i in range(num_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(X_train))

                batch_X = X_train_t[start_idx:end_idx]
                batch_y_T = y_T_train_t[start_idx:end_idx]
                batch_y_T1 = y_T1_train_t[start_idx:end_idx]

                optimizer.zero_grad()

                pred_T, pred_T1 = model(batch_X)

                loss_T = criterion(pred_T, batch_y_T)
                loss_T1 = criterion(pred_T1, batch_y_T1)
                loss = loss_T + loss_T1

                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                epoch_loss_T += loss_T.item()
                epoch_loss_T1 += loss_T1.item()

            epoch_loss /= num_batches
            epoch_loss_T /= num_batches
            epoch_loss_T1 /= num_batches

            # éªŒè¯
            model.eval()
            with torch.no_grad():
                val_pred_T, val_pred_T1 = model(X_val_t)
                val_loss_T = criterion(val_pred_T, y_T_val_t).item()
                val_loss_T1 = criterion(val_pred_T1, y_T1_val_t).item()
                val_loss = val_loss_T + val_loss_T1

            history['train_loss'].append(epoch_loss)
            history['train_loss_T'].append(epoch_loss_T)
            history['train_loss_T1'].append(epoch_loss_T1)
            history['val_loss'].append(val_loss)
            history['val_loss_T'].append(val_loss_T)
            history['val_loss_T1'].append(val_loss_T1)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), 'best_sst_model.pth')

            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}]")
                print(f"  Train Loss: {epoch_loss:.6f} (T: {epoch_loss_T:.6f}, T+1: {epoch_loss_T1:.6f})")
                print(f"  Val Loss: {val_loss:.6f} (T: {val_loss_T:.6f}, T+1: {val_loss_T1:.6f})")

        print(f"\n{'='*80}")
        print(f"âœ“ SSTè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"{'='*80}\n")

        self.history['sst'] = history
        return history

    def train_temporal_model(
        self,
        model,
        train_loader,
        val_loader,
        epochs=100,
        lr=0.001,
        model_name='Temporal',
        verbose=True
    ):
        """è®­ç»ƒæ—¶åºæ¨¡å‹"""

        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ{model_name}æ¨¡å‹")
        print(f"{'='*80}\n")

        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        criterion = nn.MSELoss()

        history = {'train_loss': [], 'val_loss': []}
        best_val_loss = float('inf')

        for epoch in range(epochs):
            # è®­ç»ƒ
            model.train()
            train_loss = 0

            for batch_seq, batch_target in train_loader:
                batch_seq = batch_seq.to(self.device)
                batch_target = batch_target.to(self.device)

                optimizer.zero_grad()
                predictions = model(batch_seq)
                loss = criterion(predictions, batch_target)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            train_loss /= len(train_loader)

            # éªŒè¯
            model.eval()
            val_loss = 0

            with torch.no_grad():
                for batch_seq, batch_target in val_loader:
                    batch_seq = batch_seq.to(self.device)
                    batch_target = batch_target.to(self.device)

                    predictions = model(batch_seq)
                    loss = criterion(predictions, batch_target)
                    val_loss += loss.item()

            val_loss /= len(val_loader)

            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), f'best_{model_name.lower()}_model.pth')

            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

        print(f"\n{'='*80}")
        print(f"âœ“ {model_name}è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"{'='*80}\n")

        self.history[model_name.lower()] = history
        return history


# ============================================================================
# ç¬¬äº”æ­¥ï¼šæ¨¡å‹è¯„ä¼°å™¨
# ============================================================================

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, device='cpu'):
        self.device = device
        self.results = {}

    def evaluate_sst(
        self,
        model,
        X_test,
        y_T_test,
        y_T1_test,
        model_name='SST'
    ) -> Dict:
        """è¯„ä¼°SSTæ¨¡å‹"""

        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯„ä¼°{model_name}æ¨¡å‹")
        print(f"{'='*80}\n")

        model.eval()

        X_test_t = torch.FloatTensor(X_test).to(self.device)

        with torch.no_grad():
            pred_T, pred_T1 = model(X_test_t)
            pred_T = pred_T.cpu().numpy()
            pred_T1 = pred_T1.cpu().numpy()

        # è®¡ç®—æŒ‡æ ‡
        metrics = {}

        # Tæ—¥é¢„æµ‹æŒ‡æ ‡
        metrics['T_MSE'] = mean_squared_error(y_T_test, pred_T)
        metrics['T_MAE'] = mean_absolute_error(y_T_test, pred_T)
        metrics['T_Direction_Acc'] = self._direction_accuracy(y_T_test, pred_T)

        # T+1æ—¥é¢„æµ‹æŒ‡æ ‡
        metrics['T1_MSE'] = mean_squared_error(y_T1_test, pred_T1)
        metrics['T1_MAE'] = mean_absolute_error(y_T1_test, pred_T1)
        metrics['T1_Direction_Acc'] = self._direction_accuracy(y_T1_test, pred_T1)

        # æ‰“å°ç»“æœ
        print(f"Tæ—¥é¢„æµ‹:")
        print(f"  MSE: {metrics['T_MSE']:.6f}")
        print(f"  MAE: {metrics['T_MAE']:.6f}")
        print(f"  æ–¹å‘å‡†ç¡®ç‡: {metrics['T_Direction_Acc']:.2%}")

        print(f"\nT+1æ—¥é¢„æµ‹:")
        print(f"  MSE: {metrics['T1_MSE']:.6f}")
        print(f"  MAE: {metrics['T1_MAE']:.6f}")
        print(f"  æ–¹å‘å‡†ç¡®ç‡: {metrics['T1_Direction_Acc']:.2%}")

        print(f"\n{'='*80}\n")

        self.results[model_name] = {
            'metrics': metrics,
            'predictions': {'T': pred_T, 'T1': pred_T1},
            'actuals': {'T': y_T_test, 'T1': y_T1_test}
        }

        return metrics

    def evaluate_temporal_model(
        self,
        model,
        test_loader,
        model_name='Temporal'
    ) -> Dict:
        """è¯„ä¼°æ—¶åºæ¨¡å‹"""

        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯„ä¼°{model_name}æ¨¡å‹")
        print(f"{'='*80}\n")

        model.eval()

        all_predictions = []
        all_actuals = []

        with torch.no_grad():
            for batch_seq, batch_target in test_loader:
                batch_seq = batch_seq.to(self.device)
                predictions = model(batch_seq)
                all_predictions.append(predictions.cpu().numpy())
                all_actuals.append(batch_target.numpy())

        predictions = np.vstack(all_predictions)
        actuals = np.vstack(all_actuals)

        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        metrics['MSE'] = mean_squared_error(actuals, predictions)
        metrics['MAE'] = mean_absolute_error(actuals, predictions)
        metrics['Direction_Acc'] = self._direction_accuracy(actuals, predictions)
        metrics['Sharpe_Ratio'] = self._sharpe_ratio(actuals, predictions)

        # æ‰“å°ç»“æœ
        print(f"  MSE: {metrics['MSE']:.6f}")
        print(f"  MAE: {metrics['MAE']:.6f}")
        print(f"  æ–¹å‘å‡†ç¡®ç‡: {metrics['Direction_Acc']:.2%}")
        print(f"  Sharpeæ¯”ç‡: {metrics['Sharpe_Ratio']:.4f}")

        print(f"\n{'='*80}\n")

        self.results[model_name] = {
            'metrics': metrics,
            'predictions': predictions,
            'actuals': actuals
        }

        return metrics

    def _direction_accuracy(self, y_true, y_pred):
        """è®¡ç®—æ–¹å‘å‡†ç¡®ç‡"""
        true_direction = np.sign(y_true)
        pred_direction = np.sign(y_pred)
        return np.mean(true_direction == pred_direction)

    def _sharpe_ratio(self, y_true, y_pred, risk_free_rate=0.0):
        """è®¡ç®—Sharpeæ¯”ç‡"""
        returns = y_pred.flatten()
        excess_returns = returns - risk_free_rate
        if len(excess_returns) == 0 or np.std(excess_returns) == 0:
            return 0.0
        return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)

    def compare_models(self):
        """å¯¹æ¯”æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""

        print(f"\n{'='*80}")
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print(f"{'='*80}\n")

        comparison_data = []

        for model_name, result in self.results.items():
            metrics = result['metrics']
            row = {'Model': model_name}
            row.update(metrics)
            comparison_data.append(row)

        df = pd.DataFrame(comparison_data)
        print(df.to_string(index=False))

        print(f"\n{'='*80}\n")

        return df


# ============================================================================
# ç¬¬å…­æ­¥ï¼šå®Œæ•´Pipeline
# ============================================================================

class CompletePipeline:
    """å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""

    def __init__(
        self,
        stocks_json_path: str,
        target_market: str,
        target_stock: str,
        device: str = 'cpu'
    ):
        self.stocks_json_path = stocks_json_path
        self.target_market = target_market
        self.target_stock = target_stock
        self.device = device

        # åŠ è½½è‚¡ç¥¨åˆ—è¡¨
        with open(stocks_json_path, 'r', encoding='utf-8') as f:
            self.stocks_json = json.load(f)

        print(f"\n{'='*80}")
        print("ğŸ¯ Pipelineé…ç½®")
        print(f"{'='*80}")
        print(f"ç›®æ ‡å¸‚åœº: {target_market}")
        print(f"ç›®æ ‡è‚¡ç¥¨: {target_stock}")
        print(f"è®¾å¤‡: {device}")
        print(f"è‚¡ç¥¨åˆ—è¡¨: {sum(len(v) for v in self.stocks_json.values())}åªè‚¡ç¥¨")
        print(f"{'='*80}\n")

    def run(
        self,
        start_date: str = "2020-01-01",
        end_date: str = "2024-12-31",
        fetch_data: bool = True,
        data_path: str = None,
        sst_epochs: int = 50,
        temporal_epochs: int = 100,
        seq_len: int = 60
    ):
        """è¿è¡Œå®Œæ•´æµç¨‹"""

        # Step 1: è·å–å†å²æ•°æ®
        if fetch_data:
            fetcher = StockDataFetcher()
            historical_data = fetcher.fetch_historical_data(
                stocks_json=self.stocks_json,
                start_date=start_date,
                end_date=end_date,
                interval="1d",
                include_market_index=True
            )
            fetcher.save_data("historical_data.pkl")
        else:
            if data_path is None:
                data_path = "historical_data.pkl"
            historical_data = StockDataFetcher.load_data(data_path)

        # Step 2: æ•°æ®é¢„å¤„ç†
        processor = StockDataProcessor(
            historical_data=historical_data,
            target_market=self.target_market,
            target_stock=self.target_stock
        )

        X, y_T, y_T1, dates = processor.prepare_training_data()

        # æ•°æ®é›†åˆ’åˆ†
        train_size = int(0.7 * len(X))
        val_size = int(0.15 * len(X))

        X_train = X[:train_size]
        y_T_train = y_T[:train_size]
        y_T1_train = y_T1[:train_size]

        X_val = X[train_size:train_size+val_size]
        y_T_val = y_T[train_size:train_size+val_size]
        y_T1_val = y_T1[train_size:train_size+val_size]

        X_test = X[train_size+val_size:]
        y_T_test = y_T[train_size+val_size:]
        y_T1_test = y_T1[train_size+val_size:]

        print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\n")

        # Step 3: è®­ç»ƒSSTæ¨¡å‹
        num_features = X.shape[1]
        sst_model = DualOutputSST(
            num_boundary_sensors=num_features,
            num_target_sensors=1,
            d_model=128,
            nhead=8,
            num_layers=3,
            dropout=0.1,
            enable_feature_extraction=True
        ).to(self.device)

        print(f"SSTæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in sst_model.parameters()):,}\n")

        trainer = ModelTrainer(device=self.device)
        sst_history = trainer.train_sst(
            sst_model,
            X_train, y_T_train, y_T1_train,
            X_val, y_T_val, y_T1_val,
            epochs=sst_epochs,
            batch_size=32,
            lr=0.001
        )

        # Step 4: è¯„ä¼°SSTæ¨¡å‹
        evaluator = ModelEvaluator(device=self.device)
        sst_metrics = evaluator.evaluate_sst(
            sst_model,
            X_test,
            y_T_test,
            y_T1_test,
            model_name='SST'
        )

        # Step 5: æå–ç‰¹å¾
        print(f"\n{'='*80}")
        print("ğŸ” æå–SSTå†…éƒ¨ç‰¹å¾")
        print(f"{'='*80}\n")

        sst_model.eval()
        with torch.no_grad():
            X_all_t = torch.FloatTensor(X).to(self.device)
            (pred_T, pred_T1), features = sst_model.forward_with_features(
                X_all_t,
                return_attention=True,
                return_encoder_output=True
            )

            # æå–ç‰¹å¾
            encoder_output = features['encoder_output'].cpu().numpy()
            pooled_features = features['pooled_features'].cpu().numpy()

            # è®¡ç®—æ®‹å·®
            residual_T = y_T - pred_T.cpu().numpy()
            residual_T1 = y_T1 - pred_T1.cpu().numpy()

        print(f"  âœ“ Encoderè¾“å‡ºå½¢çŠ¶: {encoder_output.shape}")
        print(f"  âœ“ æ± åŒ–ç‰¹å¾å½¢çŠ¶: {pooled_features.shape}")
        print(f"  âœ“ æ®‹å·®è®¡ç®—å®Œæˆ")

        # ç»„åˆç‰¹å¾ç”¨äºæ—¶åºæ¨¡å‹
        relationship_features = pooled_features  # ä½¿ç”¨æ± åŒ–ç‰¹å¾ä½œä¸ºå…³ç³»ç‰¹å¾

        # Step 6: å‡†å¤‡æ—¶åºæ•°æ®
        print(f"\n{'='*80}")
        print("ğŸ”„ å‡†å¤‡æ—¶åºæ•°æ®")
        print(f"{'='*80}\n")

        # ä½¿ç”¨åŸå§‹ç‰¹å¾ + å…³ç³»ç‰¹å¾
        target_stock_features = torch.FloatTensor(X)
        relationship_features_t = torch.FloatTensor(relationship_features)
        targets = torch.FloatTensor(y_T1)  # é¢„æµ‹T+1æ—¥æ”¶ç›Š

        # åˆ†å‰²æ•°æ®
        train_dataset = TemporalDataset(
            target_stock_features=target_stock_features[:train_size],
            relationship_features=relationship_features_t[:train_size],
            targets=targets[:train_size],
            seq_len=seq_len
        )

        val_dataset = TemporalDataset(
            target_stock_features=target_stock_features[train_size:train_size+val_size],
            relationship_features=relationship_features_t[train_size:train_size+val_size],
            targets=targets[train_size:train_size+val_size],
            seq_len=seq_len
        )

        test_dataset = TemporalDataset(
            target_stock_features=target_stock_features[train_size+val_size:],
            relationship_features=relationship_features_t[train_size+val_size:],
            targets=targets[train_size+val_size:],
            seq_len=seq_len
        )

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        print(f"  æ—¶åºè®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"  æ—¶åºéªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"  æ—¶åºæµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

        # Step 7: è®­ç»ƒæ—¶åºæ¨¡å‹
        input_dim = num_features + relationship_features.shape[1]

        # LSTMæ¨¡å‹
        lstm_model = LSTMTemporalPredictor(
            input_dim=input_dim,
            hidden_dim=128,
            num_layers=2,
            output_dim=1,
            use_attention=True
        ).to(self.device)

        print(f"\nLSTMæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in lstm_model.parameters()):,}")

        lstm_history = trainer.train_temporal_model(
            lstm_model,
            train_loader,
            val_loader,
            epochs=temporal_epochs,
            lr=0.001,
            model_name='LSTM'
        )

        # GRUæ¨¡å‹
        gru_model = GRUTemporalPredictor(
            input_dim=input_dim,
            hidden_dim=128,
            num_layers=2,
            output_dim=1,
            use_attention=True
        ).to(self.device)

        print(f"GRUæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in gru_model.parameters()):,}")

        gru_history = trainer.train_temporal_model(
            gru_model,
            train_loader,
            val_loader,
            epochs=temporal_epochs,
            lr=0.001,
            model_name='GRU'
        )

        # TCNæ¨¡å‹
        tcn_model = TCNTemporalPredictor(
            input_dim=input_dim,
            num_channels=[64, 128, 128, 64],
            kernel_size=3,
            output_dim=1
        ).to(self.device)

        print(f"TCNæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in tcn_model.parameters()):,}")

        tcn_history = trainer.train_temporal_model(
            tcn_model,
            train_loader,
            val_loader,
            epochs=temporal_epochs,
            lr=0.001,
            model_name='TCN'
        )

        # Step 8: è¯„ä¼°æ—¶åºæ¨¡å‹
        lstm_metrics = evaluator.evaluate_temporal_model(
            lstm_model, test_loader, model_name='LSTM'
        )

        gru_metrics = evaluator.evaluate_temporal_model(
            gru_model, test_loader, model_name='GRU'
        )

        tcn_metrics = evaluator.evaluate_temporal_model(
            tcn_model, test_loader, model_name='TCN'
        )

        # Step 9: å¯¹æ¯”ç»“æœ
        comparison_df = evaluator.compare_models()

        # Step 10: ä¿å­˜ç»“æœ
        print(f"\n{'='*80}")
        print("ğŸ’¾ ä¿å­˜ç»“æœ")
        print(f"{'='*80}\n")

        results = {
            'sst_metrics': sst_metrics,
            'lstm_metrics': lstm_metrics,
            'gru_metrics': gru_metrics,
            'tcn_metrics': tcn_metrics,
            'comparison': comparison_df,
            'histories': trainer.history
        }

        with open('training_results.pkl', 'wb') as f:
            pickle.dump(results, f)

        print("  âœ“ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: training_results.pkl")
        print("  âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
        print(f"\n{'='*80}")
        print("âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆï¼")
        print(f"{'='*80}\n")

        return results


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='å®Œæ•´çš„è‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒæµç¨‹')

    parser.add_argument('--stocks_json', type=str, default='data/demo.json',
                        help='è‚¡ç¥¨åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target_market', type=str, default='CN',
                        help='ç›®æ ‡å¸‚åœº (US/CN/HK/JP)')
    parser.add_argument('--target_stock', type=str, default='600519',
                        help='ç›®æ ‡è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--start_date', type=str, default='2020-01-01',
                        help='æ•°æ®å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end_date', type=str, default='2024-12-31',
                        help='æ•°æ®ç»“æŸæ—¥æœŸ')
    parser.add_argument('--fetch_data', action='store_true',
                        help='æ˜¯å¦é‡æ–°æŠ“å–æ•°æ®')
    parser.add_argument('--data_path', type=str, default=None,
                        help='å·²ä¿å­˜çš„æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sst_epochs', type=int, default=50,
                        help='SSTè®­ç»ƒè½®æ•°')
    parser.add_argument('--temporal_epochs', type=int, default=100,
                        help='æ—¶åºæ¨¡å‹è®­ç»ƒè½®æ•°')
    parser.add_argument('--seq_len', type=int, default=60,
                        help='æ—¶åºçª—å£é•¿åº¦')
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¡ç®—è®¾å¤‡ (cpu/cuda)')

    args = parser.parse_args()

    # åˆ›å»ºå¹¶è¿è¡Œpipeline
    pipeline = CompletePipeline(
        stocks_json_path=args.stocks_json,
        target_market=args.target_market,
        target_stock=args.target_stock,
        device=args.device
    )

    results = pipeline.run(
        start_date=args.start_date,
        end_date=args.end_date,
        fetch_data=args.fetch_data,
        data_path=args.data_path,
        sst_epochs=args.sst_epochs,
        temporal_epochs=args.temporal_epochs,
        seq_len=args.seq_len
    )

    return results


if __name__ == '__main__':
    results = main()
